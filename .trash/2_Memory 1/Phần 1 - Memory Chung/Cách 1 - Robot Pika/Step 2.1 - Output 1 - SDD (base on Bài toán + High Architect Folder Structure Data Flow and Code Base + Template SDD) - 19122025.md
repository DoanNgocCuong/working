# SOFTWARE DESIGN DOCUMENT (SDD)

# PIKA Mem0 Long-Term Memory System - Self-Hosted Solution

**Phi√™n b·∫£n: 1.0 | Ng√†y: 2024-12-19**

---

## üìã TH√îNG TIN T√ÄI LI·ªÜU

```yaml
Ti√™u ƒë·ªÅ: "PIKA Long-Term Memory System - Migration t·ª´ Mem0 Enterprise"
M√£ t√†i li·ªáu: "SDD-PIKA-MEM0-LOCALHOST-V1.0"

# Quy·ªÅn s·ªü h·ªØu
T√°c gi·∫£ ch√≠nh: "SU (Senior Backend Developer)"
ƒê·ªìng t√°c gi·∫£:
  - "AI/ML Engineer"
  - "DevOps Engineer"

# Tr·∫°ng th√°i
Tr·∫°ng th√°i: "Draft - ƒêang thi·∫øt k·∫ø"
ƒê·ªô ∆∞u ti√™n: "P1 - High Priority"

# Timeline
Ng√†y t·∫°o: "2024-12-19"
Ng√†y c·∫≠p nh·∫≠t: "2024-12-19"
Target release: "2025-01-31 (Sprint 4-5)"

# Li√™n k·∫øt
H·ªá th·ªëng hi·ªán t·∫°i: "https://pika-mem0.stepup.edu.vn"
Milvus Server: "http://124.197.21.40:8000"
Neo4j Server: "bolt://124.197.21.40:8687"
```

---

## üìñ M·ª§C L·ª§C

1. [T·ªïng quan (Executive Summary)](https://claude.ai/chat/9ae58082-8d7f-4559-8878-a6d2eb285518#1-t%E1%BB%95ng-quan-executive-summary)
2. [Gi·ªõi thi·ªáu v√† B·ªëi c·∫£nh](https://claude.ai/chat/9ae58082-8d7f-4559-8878-a6d2eb285518#2-gi%E1%BB%9Bi-thi%E1%BB%87u-v%C3%A0-b%E1%BB%91i-c%E1%BA%A3nh)
3. [M·ª•c ti√™u, Ph·∫°m vi & R√†ng bu·ªôc](https://claude.ai/chat/9ae58082-8d7f-4559-8878-a6d2eb285518#3-m%E1%BB%A5c-ti%C3%AAu-ph%E1%BA%A1m-vi--r%C3%A0ng-bu%E1%BB%99c)
4. [Ki·∫øn tr√∫c T·ªïng th·ªÉ (HLD)](https://claude.ai/chat/9ae58082-8d7f-4559-8878-a6d2eb285518#4-ki%E1%BA%BFn-tr%C3%BAc-t%E1%BB%95ng-th%E1%BB%83-hld)
5. [Thi·∫øt k·∫ø Chi ti·∫øt (LLD)](https://claude.ai/chat/9ae58082-8d7f-4559-8878-a6d2eb285518#5-thi%E1%BA%BFt-k%E1%BA%BF-chi-ti%E1%BA%BFt-lld)
6. [Thi·∫øt k·∫ø API](https://claude.ai/chat/9ae58082-8d7f-4559-8878-a6d2eb285518#6-thi%E1%BA%BFt-k%E1%BA%BF-api)
7. [Thi·∫øt k·∫ø D·ªØ li·ªáu](https://claude.ai/chat/9ae58082-8d7f-4559-8878-a6d2eb285518#7-thi%E1%BA%BFt-k%E1%BA%BF-d%E1%BB%AF-li%E1%BB%87u)
8. [Chi·∫øn l∆∞·ª£c Tri·ªÉn khai](https://claude.ai/chat/9ae58082-8d7f-4559-8878-a6d2eb285518#8-chi%E1%BA%BFn-l%C6%B0%E1%BB%A3c-tri%E1%BB%83n-khai)
9. [Testing v√† Quality Assurance](https://claude.ai/chat/9ae58082-8d7f-4559-8878-a6d2eb285518#9-testing-v%C3%A0-quality-assurance)
10. [Production Readiness Checklist](https://claude.ai/chat/9ae58082-8d7f-4559-8878-a6d2eb285518#10-production-readiness-checklist)

---

# 1. T·ªîNG QUAN (EXECUTIVE SUMMARY)

## 1.1 B·∫£ng T√≥m t·∫Øt

|Kh√≠a c·∫°nh|Chi ti·∫øt|
|---|---|
|**V·∫•n ƒë·ªÅ**|PIKA robot ƒëang s·ª≠ d·ª•ng Mem0 Enterprise service (tr·∫£ ph√≠), t·ªën k√©m v√† kh√¥ng c√≥ full control v·ªÅ data/logic|
|**Gi·∫£i ph√°p ƒë·ªÅ xu·∫•t**|X√¢y d·ª±ng self-hosted Long-Term Memory System v·ªõi 2 core APIs t∆∞∆°ng t·ª± Mem0, t·∫≠n d·ª•ng infrastructure Milvus + Neo4j ƒë√£ c√≥|
|**T√°c ƒë·ªông Business**|Ti·∫øt ki·ªám $500-1000/th√°ng, tƒÉng data privacy 100%, custom logic theo nhu c·∫ßu PIKA, kh√¥ng b·ªã vendor lock-in|
|**T√°c ƒë·ªông Technical**|Latency <200ms (p95), h·ªó tr·ª£ 100K+ conversations, kh·∫£ nƒÉng scale horizontal, full observability|
|**Tech Stack**|Python 3.11, FastAPI, Milvus (vector store), Neo4j (graph store), PostgreSQL (metadata), Redis (cache), OpenAI API|
|**Effort ∆∞·ªõc t√≠nh**|2 engineers √ó 3-4 sprints (6-8 tu·∫ßn)|
|**Risk Level**|Medium - c√≥ infrastructure, c·∫ßn implement logic v√† testing k·ªπ|
|**Timeline**|MVP: 3 tu·∫ßn, Production: 6-8 tu·∫ßn|
|**Chi ph√≠ (Year 1)**|~$3000 (infrastructure) vs $6000-12000 (Mem0 Enterprise)|

## 1.2 Ki·∫øn tr√∫c T·ªïng quan

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           PIKA LONG-TERM MEMORY SYSTEM                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                           ‚îÇ
‚îÇ  ‚îÇ PIKA Robot   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ
‚îÇ  ‚îÇ AI Workflow  ‚îÇ      ‚îÇ                                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ                                    ‚îÇ
‚îÇ                        ‚ñº                                     ‚îÇ
‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ            ‚îÇ  Memory API Service  ‚îÇ                         ‚îÇ
‚îÇ            ‚îÇ     (FastAPI)        ‚îÇ                         ‚îÇ
‚îÇ            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                         ‚îÇ
‚îÇ            ‚îÇ  ‚îÇ extract_facts  ‚îÇ  ‚îÇ                         ‚îÇ
‚îÇ            ‚îÇ  ‚îÇ search_facts   ‚îÇ  ‚îÇ                         ‚îÇ
‚îÇ            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                         ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ                       ‚îÇ                                      ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ         ‚îÇ             ‚îÇ             ‚îÇ                        ‚îÇ
‚îÇ         ‚ñº             ‚ñº             ‚ñº                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ  ‚îÇ  Milvus  ‚îÇ  ‚îÇ  Neo4j   ‚îÇ  ‚îÇPostgreSQL‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ (Vector) ‚îÇ  ‚îÇ (Graph)  ‚îÇ  ‚îÇ(Metadata)‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ              ‚îÇ  Redis   ‚îÇ  ‚îÇ OpenAI   ‚îÇ                     ‚îÇ
‚îÇ              ‚îÇ (Cache)  ‚îÇ  ‚îÇEmbedding ‚îÇ                     ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## 1.3 Key Metrics & Ti√™u ch√≠ Th√†nh c√¥ng

|KPI|Hi·ªán t·∫°i (Mem0)|Target (Self-hosted)|C√°ch ƒëo|
|---|---|---|---|
|Response Time (p95)|~150ms|<200ms|Datadog APM|
|Throughput|~100 rps|200+ rps|Load test|
|Error Rate|<0.5%|<0.1%|Prometheus|
|Cost/month|$500-1000|$250 (infra)|Cloud billing|
|Memory Accuracy|~85%|>90%|Manual testing|

---

# 2. GI·ªöI THI·ªÜU V√Ä B·ªêI C·∫¢NH

## 2.1 M·ª•c ƒë√≠ch T√†i li·ªáu

T√†i li·ªáu n√†y cung c·∫•p **thi·∫øt k·∫ø production-ready** cho vi·ªác migration PIKA Memory System t·ª´ Mem0 Enterprise sang self-hosted solution:

- **HLD (High-Level Design)**: Ki·∫øn tr√∫c t·ªïng th·ªÉ v√† data flow
- **LLD (Low-Level Design)**: Chi ti·∫øt implementation t·ª´ng component
- **API Specification**: Contract cho 2 APIs ch√≠nh
- **Data Design**: Schema cho Milvus, Neo4j, PostgreSQL
- **Implementation Plan**: Roadmap tri·ªÉn khai t·ª´ng phase

## 2.2 ƒê·ªëi t∆∞·ª£ng S·ª≠ d·ª•ng

|ƒê·ªëi t∆∞·ª£ng|M·ª•c ƒë√≠ch s·ª≠ d·ª•ng|Sections quan tr·ªçng|
|---|---|---|
|**Backend Engineers**|Implementation|4, 5, 6, 7|
|**DevOps/SRE**|Deploy & operate|8, 10|
|**QA Team**|Test planning|9|
|**Product Team**|Hi·ªÉu scope & timeline|1, 3, 8|
|**Tech Lead**|Review architecture|4, 5|

## 2.3 B·ªëi c·∫£nh D·ª± √°n

### H·ªá th·ªëng Hi·ªán t·∫°i (Mem0 Enterprise)

PIKA robot hi·ªán ƒëang s·ª≠ d·ª•ng **Mem0 Enterprise API** cho long-term memory:

```bash
# Current endpoint
BASE_URL: https://pika-mem0.stepup.edu.vn

# 2 APIs ƒë∆∞·ª£c s·ª≠ d·ª•ng
POST /extract_facts  # Tr√≠ch xu·∫•t facts t·ª´ conversation
POST /search_facts   # T√¨m ki·∫øm facts theo query
```

**∆Øu ƒëi·ªÉm:**

- ‚úÖ D·ªÖ integrate, kh√¥ng c·∫ßn maintain infrastructure
- ‚úÖ High availability, ƒë∆∞·ª£c Mem0 ƒë·∫£m b·∫£o
- ‚úÖ T·ª± ƒë·ªông update features m·ªõi

**Nh∆∞·ª£c ƒëi·ªÉm:**

- ‚ùå Chi ph√≠ cao ($500-1000/month)
- ‚ùå Vendor lock-in, kh√¥ng custom ƒë∆∞·ª£c logic
- ‚ùå Data privacy concerns (data ·ªü external service)
- ‚ùå Kh√¥ng control ƒë∆∞·ª£c performance optimization
- ‚ùå API rate limits theo pricing tier

### T·∫°i sao c·∫ßn Migration?

1. **Chi ph√≠**: V·ªõi 100K+ conversations/month, chi ph√≠ Mem0 Enterprise tƒÉng nhanh
2. **Data Privacy**: Conversations c·ªßa tr·∫ª em c·∫ßn ƒë∆∞·ª£c b·∫£o m·∫≠t tuy·ªát ƒë·ªëi
3. **Customization**: C·∫ßn customize logic cho PIKA (e.g., emotion extraction, learning progress tracking)
4. **Performance**: C·∫ßn optimize latency cho real-time robot interaction
5. **Independence**: Kh√¥ng ph·ª• thu·ªôc v√†o external service uptime

### Infrastructure ƒê√£ c√≥

Team ƒë√£ c√≥ s·∫µn infrastructure:

```yaml
Milvus:
  Host: "124.197.21.40"
  Port: 8000
  Purpose: "Vector similarity search"
  Status: "Running, tested"

Neo4j:
  Host: "124.197.21.40"
  Port: 8687 (Bolt)
  Port: 8474 (HTTP)
  Username: "neo4j"
  Password: "mem0graph"
  Purpose: "Graph relationships"
  Status: "Running, tested"
```

---

# 3. M·ª§C TI√äU, PH·∫†M VI & R√ÄNG BU·ªòC

## 3.1 M·ª•c ti√™u

### Business Goals

|M·ª•c ti√™u|Metric|Target|Timeline|
|---|---|---|---|
|Gi·∫£m chi ph√≠|Monthly cost|-60% ($250 vs $600)|Q1 2025|
|TƒÉng privacy|Data residency|100% in-house|Q1 2025|
|Custom logic|Feature velocity|2x faster|Q2 2025|

### Technical Goals

|M·ª•c ti√™u|Metric|Target|Timeline|
|---|---|---|---|
|Performance|P95 latency|<200ms|Sprint 4|
|Reliability|Uptime|99.9%|Sprint 5|
|Scalability|Concurrent users|100K+|Sprint 6|
|Accuracy|Memory recall|>90%|Sprint 5|

## 3.2 Ph·∫°m vi D·ª± √°n

### ‚úÖ In-Scope (MVP - Must Have)

**Phase 1 - Core APIs:**

- ‚úÖ API `POST /extract_facts`: Tr√≠ch xu·∫•t facts t·ª´ conversation
- ‚úÖ API `POST /search_facts`: T√¨m ki·∫øm facts theo semantic query
- ‚úÖ Integration v·ªõi Milvus (vector storage)
- ‚úÖ Integration v·ªõi Neo4j (graph relationships)
- ‚úÖ PostgreSQL cho metadata (conversations, users)
- ‚úÖ OpenAI embeddings cho vectorization

**Phase 2 - Production Features:**

- ‚úÖ Redis caching cho hot facts
- ‚úÖ Monitoring v·ªõi Datadog/Prometheus
- ‚úÖ Error handling & retry logic
- ‚úÖ API rate limiting
- ‚úÖ Health check endpoints

### üìã Future Scope (Phase 3 - Nice to Have)

- üí° Auto memory consolidation (merge similar facts)
- üí° Time-decay scoring (old facts √≠t quan tr·ªçng h∆°n)
- üí° Multi-language support (hi·ªán ch·ªâ Vietnamese/English)
- üí° Memory analytics dashboard
- üí° Bulk import/export

### ‚ùå Out-of-Scope (Non-Goals)

|Non-Goal|L√Ω do|Future?|
|---|---|---|
|Memory visualization UI|Kh√¥ng ph·∫£i backend scope|Phase 4|
|Real-time memory sync|PIKA workflow l√† async|Kh√¥ng c·∫ßn|
|Multi-tenant isolation|Ch·ªâ c√≥ 1 PIKA app|Kh√¥ng c·∫ßn|

## 3.3 Assumptions (Gi·∫£ ƒë·ªãnh)

|ID|Assumption|Impact n·∫øu sai|Validation|
|---|---|---|---|
|A1|Milvus/Neo4j stable, kh√¥ng downtime|High - delay project|Health check daily|
|A2|OpenAI API available (<100ms)|Medium - fallback plan|Monitor uptime|
|A3|Traffic <100K conversations/month|Low - over-provision|Track actual usage|
|A4|Conversation length <50 turns|Low - chunking strategy|Analyze current data|

## 3.4 Constraints (R√†ng bu·ªôc)

### Technical Constraints

|Constraint|L√Ω do|Workaround|
|---|---|---|
|Python 3.11+|Team standard|N/A|
|Milvus port 8000|Fixed infra|N/A|
|Neo4j bolt://8687|Fixed infra|N/A|
|OpenAI embedding model|Best quality|Fallback: local model|

### Business Constraints

|Constraint|Impact|Mitigation|
|---|---|---|
|Budget <$5K setup|Limited infra|Use existing servers|
|Timeline 6-8 weeks|Tight schedule|MVP first, iterate|
|No downtime allowed|Migration risk|Blue-green deployment|

### Data Constraints

|Constraint|Reason|Solution|
|---|---|---|
|GDPR compliance|User data c·ªßa tr·∫ª em|Encryption at rest|
|Data retention 2 years|Business requirement|Implement cleanup job|

---

# 4. KI·∫æN TR√öC T·ªîNG TH·ªÇ (HLD)

## 4.1 System Context Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     SYSTEM CONTEXT                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ    ‚îÇ  PIKA Robot     ‚îÇ         ‚îÇ  Admin Panel    ‚îÇ            ‚îÇ
‚îÇ    ‚îÇ  AI Workflow    ‚îÇ         ‚îÇ  (Future)       ‚îÇ            ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ             ‚îÇ                           ‚îÇ                      ‚îÇ
‚îÇ             ‚îÇ        HTTPS/JSON         ‚îÇ                      ‚îÇ
‚îÇ             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ                         ‚îÇ                                      ‚îÇ
‚îÇ                         ‚ñº                                      ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ         ‚îÇ                                   ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ   PIKA MEMORY API SERVICE         ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ   (FastAPI + Python 3.11)         ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ                                   ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ   ‚Ä¢ extract_facts API             ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ   ‚Ä¢ search_facts API              ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ   ‚Ä¢ Health check                  ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ   ‚Ä¢ Metrics endpoint              ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ                                   ‚îÇ                 ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                    ‚îÇ                                           ‚îÇ
‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ      ‚îÇ             ‚îÇ              ‚îÇ             ‚îÇ             ‚îÇ
‚îÇ      ‚ñº             ‚ñº              ‚ñº             ‚ñº             ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ ‚îÇ Milvus  ‚îÇ  ‚îÇ  Neo4j  ‚îÇ  ‚îÇPostgreSQL‚îÇ  ‚îÇ  Redis   ‚îÇ        ‚îÇ
‚îÇ ‚îÇ(Vector) ‚îÇ  ‚îÇ (Graph) ‚îÇ  ‚îÇ(Metadata)‚îÇ  ‚îÇ (Cache)  ‚îÇ        ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ      External:           Monitoring:                          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ ‚îÇ  OpenAI API  ‚îÇ    ‚îÇ   Datadog    ‚îÇ                         ‚îÇ
‚îÇ ‚îÇ (Embeddings) ‚îÇ    ‚îÇ(Logs/Metrics)‚îÇ                         ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ                                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## 4.2 Container Diagram (Chi ti·∫øt Components)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MEMORY API SERVICE                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ              API LAYER (FastAPI)                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ extract_facts  ‚îÇ      ‚îÇ search_facts   ‚îÇ             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   endpoint     ‚îÇ      ‚îÇ   endpoint     ‚îÇ             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ             ‚îÇ                       ‚îÇ                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ           SERVICE LAYER                                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ FactExtractor    ‚îÇ      ‚îÇ FactSearcher     ‚îÇ         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Service          ‚îÇ      ‚îÇ Service          ‚îÇ         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ              ‚îÇ                        ‚îÇ                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ           DATA ACCESS LAYER                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Milvus   ‚îÇ  ‚îÇ  Neo4j   ‚îÇ  ‚îÇPostgres  ‚îÇ             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Client   ‚îÇ  ‚îÇ  Client  ‚îÇ  ‚îÇ  Client  ‚îÇ             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ           UTILITY LAYER                                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  OpenAI    ‚îÇ  ‚îÇ   Redis    ‚îÇ  ‚îÇ  Logging   ‚îÇ        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Embedder   ‚îÇ  ‚îÇ   Cache    ‚îÇ  ‚îÇ  Service   ‚îÇ        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## 4.3 Data Flow Diagrams

### Flow 1: Extract Facts t·ª´ Conversation

```
Client          API           FactExtractor    OpenAI        Milvus       Neo4j      PostgreSQL
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ‚îÄPOST‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ /extract     ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ‚îÄValidate‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚îÄGet‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ Embeddings ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ  Vectors   ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚îÄExtract LLM prompt‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ  Facts JSON‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚îÄStore‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ  Vectors   ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚îÄCreate‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ  Relationships          ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚îÄSave‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂
  ‚îÇ              ‚îÇ                  ‚îÇ  Metadata  ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ‚óÄ‚îÄResponse‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ  Facts IDs   ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
```

### Flow 2: Search Facts theo Query

```
Client          API           FactSearcher     OpenAI        Milvus       Redis        Neo4j
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ‚îÄPOST‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ /search      ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ‚îÄCheck Cache‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ             ‚îÇ
  ‚îÇ              ‚îÇ  Cache Miss      ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ‚îÄValidate‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚îÄEmbed‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂            ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ  Query     ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ  Vector    ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚îÄSearch‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ  Similarity‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ  Top K IDs ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚îÄEnrich‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂
  ‚îÇ              ‚îÇ                  ‚îÇ  Relations ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  ‚îÇ              ‚îÇ                  ‚îÇ  Graph     ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ‚îÄCache‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂
  ‚îÇ              ‚îÇ                  ‚îÇ  Results   ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ              ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ‚óÄ‚îÄResponse‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
  ‚îÇ  Facts list  ‚îÇ                  ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ             ‚îÇ
```

## 4.4 Tech Stack

|Layer|Technology|Version|Vai tr√≤|
|---|---|---|---|
|**API Framework**|FastAPI|0.109+|REST API server|
|**Language**|Python|3.11+|Backend logic|
|**Vector Store**|Milvus|2.3+|Semantic search|
|**Graph Store**|Neo4j|5.x|Relationships|
|**Metadata DB**|PostgreSQL|15+|Structured data|
|**Cache**|Redis|7.x|Hot data caching|
|**Embeddings**|OpenAI API|text-embedding-3-small|Text ‚Üí vectors|
|**LLM**|OpenAI API|gpt-4o-mini|Fact extraction|
|**Monitoring**|Datadog/Prometheus|-|Observability|
|**Deployment**|Docker + K8s|-|Container orchestration|

---

# 5. THI·∫æT K·∫æ CHI TI·∫æT (LLD)

## 5.1 Project Structure

```
pika-mem0-service/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI application entry
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuration management
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/                    # API endpoints layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ v1/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extract.py      # POST /extract_facts
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ search.py       # POST /search_facts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health.py           # Health check endpoints
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/               # Business logic layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fact_extractor.py  # Fact extraction service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fact_searcher.py   # Fact search service
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embedder.py         # OpenAI embedding wrapper
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ repositories/           # Data access layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ milvus_repo.py     # Milvus operations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ neo4j_repo.py      # Neo4j operations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgres_repo.py   # PostgreSQL operations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ redis_repo.py      # Redis caching
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Data models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requests.py         # API request schemas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ responses.py        # API response schemas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ entities.py         # Domain entities
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                  # Utilities
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ logger.py           # Structured logging
‚îÇ       ‚îî‚îÄ‚îÄ metrics.py          # Prometheus metrics
‚îÇ
‚îú‚îÄ‚îÄ tests/                      # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îî‚îÄ‚îÄ load/
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml
‚îÇ
‚îú‚îÄ‚îÄ k8s/                        # Kubernetes manifests
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îî‚îÄ‚îÄ ingress.yaml
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ README.md
```

## 5.2 Core Services Implementation

### 5.2.1 FactExtractor Service

```python
# app/services/fact_extractor.py
from typing import List, Dict
from openai import AsyncOpenAI
from app.models.entities import Fact, Conversation
from app.repositories.milvus_repo import MilvusRepository
from app.repositories.neo4j_repo import Neo4jRepository
from app.repositories.postgres_repo import PostgresRepository

class FactExtractorService:
    """
    Service ch·ªãu tr√°ch nhi·ªám tr√≠ch xu·∫•t facts t·ª´ conversation
    
    Flow:
    1. Nh·∫≠n conversation (list of messages)
    2. G·ªçi OpenAI LLM ƒë·ªÉ extract facts (structured output)
    3. Embed facts th√†nh vectors (OpenAI embeddings)
    4. L∆∞u vectors v√†o Milvus
    5. L∆∞u relationships v√†o Neo4j
    6. L∆∞u metadata v√†o PostgreSQL
    """
    
    def __init__(
        self,
        openai_client: AsyncOpenAI,
        milvus_repo: MilvusRepository,
        neo4j_repo: Neo4jRepository,
        postgres_repo: PostgresRepository
    ):
        self.openai = openai_client
        self.milvus = milvus_repo
        self.neo4j = neo4j_repo
        self.postgres = postgres_repo
        
    async def extract_facts(
        self,
        user_id: str,
        conversation_id: str,
        conversation: List[Dict]
    ) -> List[Fact]:
        """
        Main method: Extract facts from conversation
        
        Args:
            user_id: PIKA user ID
            conversation_id: Unique conversation ID
            conversation: List of {role, content} messages
            
        Returns:
            List of extracted facts with IDs
        """
        # Step 1: Call OpenAI to extract facts
        extracted_facts = await self._extract_with_llm(conversation)
        
        # Step 2: Generate embeddings for each fact
        facts_with_vectors = await self._generate_embeddings(extracted_facts)
        
        # Step 3: Store in Milvus (vector store)
        fact_ids = await self.milvus.insert_facts(
            user_id=user_id,
            facts=facts_with_vectors
        )
        
        # Step 4: Create relationships in Neo4j
        await self.neo4j.create_fact_relationships(
            user_id=user_id,
            conversation_id=conversation_id,
            facts=facts_with_vectors
        )
        
        # Step 5: Save metadata in PostgreSQL
        await self.postgres.save_conversation_metadata(
            user_id=user_id,
            conversation_id=conversation_id,
            fact_ids=fact_ids,
            conversation=conversation
        )
        
        return facts_with_vectors
        
    async def _extract_with_llm(
        self,
        conversation: List[Dict]
    ) -> List[Dict]:
        """
        S·ª≠ d·ª•ng OpenAI GPT-4o-mini ƒë·ªÉ extract facts
        
        Prompt engineering:
        - System: H∆∞·ªõng d·∫´n extract facts v·ªÅ user (preferences, experiences, etc)
        - User: Conversation history
        - Response: Structured JSON v·ªõi facts
        """
        system_prompt = """
        B·∫°n l√† AI assistant chuy√™n tr√≠ch xu·∫•t th√¥ng tin t·ª´ h·ªôi tho·∫°i.
        
        Nhi·ªám v·ª•: Ph√¢n t√≠ch conversation v√† tr√≠ch xu·∫•t c√°c FACTS quan tr·ªçng v·ªÅ user.
        
        Facts bao g·ªìm:
        - S·ªü th√≠ch (hobbies, interests)
        - Kinh nghi·ªám (experiences)
        - Th√≥i quen (habits)
        - C·∫£m x√∫c (emotions trong context c·ª• th·ªÉ)
        - M·ªëi quan h·ªá (relationships)
        - H·ªçc t·∫≠p (learning progress, achievements)
        
        Output format: JSON array
        [
          {
            "content": "Fact description",
            "category": "preference|experience|habit|emotion|relationship|learning",
            "confidence": 0.0-1.0,
            "entities": ["entity1", "entity2"]
          }
        ]
        """
        
        # Convert conversation to string
        conversation_text = "\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in conversation
        ])
        
        response = await self.openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": conversation_text}
            ],
            response_format={"type": "json_object"},
            temperature=0.2
        )
        
        facts = json.loads(response.choices[0].message.content)
        return facts.get("facts", [])
        
    async def _generate_embeddings(
        self,
        facts: List[Dict]
    ) -> List[Fact]:
        """
        Generate vector embeddings cho m·ªói fact
        """
        texts = [fact["content"] for fact in facts]
        
        response = await self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        
        facts_with_vectors = []
        for i, fact in enumerate(facts):
            facts_with_vectors.append(Fact(
                content=fact["content"],
                category=fact.get("category", "unknown"),
                confidence=fact.get("confidence", 0.8),
                entities=fact.get("entities", []),
                embedding=response.data[i].embedding
            ))
            
        return facts_with_vectors
```

### 5.2.2 FactSearcher Service

```python
# app/services/fact_searcher.py
from typing import List, Optional
from app.models.entities import Fact, SearchResult
from app.repositories.milvus_repo import MilvusRepository
from app.repositories.neo4j_repo import Neo4jRepository
from app.repositories.redis_repo import RedisRepository

class FactSearcherService:
    """
    Service t√¨m ki·∫øm facts theo semantic query
    
    Flow:
    1. Nh·∫≠n search query (text)
    2. Check cache Redis tr∆∞·ªõc
    3. Embed query th√†nh vector
    4. Search trong Milvus (similarity search)
    5. Enrich results v·ªõi Neo4j (relationships)
    6. Cache results v√†o Redis
    7. Return ranked results
    """
    
    def __init__(
        self,
        openai_client: AsyncOpenAI,
        milvus_repo: MilvusRepository,
        neo4j_repo: Neo4jRepository,
        redis_repo: RedisRepository
    ):
        self.openai = openai_client
        self.milvus = milvus_repo
        self.neo4j = neo4j_repo
        self.redis = redis_repo
        
    async def search_facts(
        self,
        user_id: str,
        query: str,
        limit: int = 20,
        score_threshold: float = 0.4
    ) -> List[SearchResult]:
        """
        Main method: Search facts by semantic query
        
        Args:
            user_id: PIKA user ID
            query: Natural language search query
            limit: Max results to return
            score_threshold: Minimum similarity score (0-1)
            
        Returns:
            List of facts sorted by relevance
        """
        # Step 1: Check cache
        cache_key = f"search:{user_id}:{query}:{limit}"
        cached = await self.redis.get(cache_key)
        if cached:
            return cached
            
        # Step 2: Embed query
        query_vector = await self._embed_query(query)
        
        # Step 3: Search in Milvus
        similar_facts = await self.milvus.search_similar(
            user_id=user_id,
            query_vector=query_vector,
            top_k=limit * 2,  # Get more for filtering
            score_threshold=score_threshold
        )
        
        # Step 4: Enrich with Neo4j relationships
        enriched_facts = await self.neo4j.enrich_with_relationships(
            facts=similar_facts
        )
        
        # Step 5: Re-rank and filter
        ranked_results = self._rerank_results(
            enriched_facts,
            limit=limit
        )
        
        # Step 6: Cache results (5 minutes TTL)
        await self.redis.setex(
            cache_key,
            ttl=300,
            value=ranked_results
        )
        
        return ranked_results
        
    async def _embed_query(self, query: str) -> List[float]:
        """Generate embedding cho search query"""
        response = await self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=query
        )
        return response.data[0].embedding
        
    def _rerank_results(
        self,
        facts: List[Fact],
        limit: int
    ) -> List[SearchResult]:
        """
        Re-rank results d·ª±a tr√™n:
        - Similarity score (from Milvus)
        - Recency (newer facts c√≥ score cao h∆°n)
        - Relationship strength (from Neo4j)
        """
        # TODO: Implement sophisticated ranking algorithm
        # For now: simple sort by similarity + recency
        
        scored_facts = []
        for fact in facts:
            score = (
                fact.similarity_score * 0.7 +
                fact.recency_score * 0.2 +
                fact.relationship_score * 0.1
            )
            scored_facts.append((score, fact))
            
        scored_facts.sort(key=lambda x: x[0], reverse=True)
        
        return [
            SearchResult(
                fact=fact,
                score=score,
                matched_query=query
            )
            for score, fact in scored_facts[:limit]
        ]
```

---

# 6. THI·∫æT K·∫æ API

## 6.1 API Specification (OpenAPI)

### 6.1.1 Extract Facts API

```yaml
openapi: 3.0.3
info:
  title: PIKA Memory API
  version: 1.0.0
  description: Self-hosted long-term memory system for PIKA robot

servers:
  - url: http://localhost:8000
    description: Local development
  - url: https://api-memory.pika.stepup.edu.vn
    description: Production

paths:
  /v1/extract_facts:
    post:
      summary: Extract facts from conversation
      description: |
        Tr√≠ch xu·∫•t facts t·ª´ m·ªôt conversation gi·ªØa PIKA v√† user.
        
        Process:
        1. LLM extract facts t·ª´ conversation
        2. Generate embeddings
        3. Store trong Milvus + Neo4j + PostgreSQL
        
      operationId: extractFacts
      tags: [Memory]
      
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required:
                - user_id
                - conversation_id
                - conversation
              properties:
                user_id:
                  type: string
                  description: PIKA user unique ID
                  example: "019afe35-1317-7b94-a833-ff5e2504a9c3"
                conversation_id:
                  type: string
                  description: Unique conversation ID
                  example: "conv_20241219_001"
                conversation:
                  type: array
                  description: List of conversation messages
                  items:
                    type: object
                    required: [role, content]
                    properties:
                      role:
                        type: string
                        enum: [user, assistant]
                      content:
                        type: string
                        description: Message content
                  example:
                    - role: "assistant"
                      content: "Ch√†o c·∫≠u! Cu·ªëi tu·∫ßn v·ª´a r·ªìi c·∫≠u l√†m g√¨?"
                    - role: "user"
                      content: "M√¨nh ƒëi ch∆°i c√¥ng vi√™n v·ªõi gia ƒë√¨nh"
                    - role: "assistant"
                      content: "Th·∫≠t tuy·ªát! C·∫≠u th√≠ch ch∆°i g√¨ ·ªü c√¥ng vi√™n?"
                    - role: "user"
                      content: "M√¨nh th√≠ch ch∆°i x√≠ch ƒëu v√† leo c√¢y"
                      
      responses:
        '200':
          description: Facts extracted successfully
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    example: "success"
                  message:
                    type: string
                    example: "Extracted 5 facts successfully"
                  data:
                    type: object
                    properties:
                      facts_count:
                        type: integer
                        example: 5
                      fact_ids:
                        type: array
                        items:
                          type: string
                        example: ["fact_001", "fact_002", "fact_003"]
                      processing_time_ms:
                        type: integer
                        example: 1523
                        
        '400':
          description: Invalid request
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
                
        '429':
          description: Rate limit exceeded
          
        '500':
          description: Internal server error
```

### 6.1.2 Search Facts API

```yaml
  /v1/search_facts:
    post:
      summary: Search facts by semantic query
      description: |
        T√¨m ki·∫øm facts theo natural language query.
        
        Process:
        1. Embed query th√†nh vector
        2. Similarity search trong Milvus
        3. Enrich v·ªõi relationships t·ª´ Neo4j
        4. Return ranked results
        
      operationId: searchFacts
      tags: [Memory]
      
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required:
                - user_id
                - query
              properties:
                user_id:
                  type: string
                  description: PIKA user unique ID
                  example: "019afe35-1317-7b94-a833-ff5e2504a9c3"
                query:
                  type: string
                  description: Natural language search query
                  example: "Th√∫ c∆∞ng m√† user th√≠ch"
                limit:
                  type: integer
                  description: Max results to return
                  default: 20
                  minimum: 1
                  maximum: 100
                score_threshold:
                  type: number
                  format: float
                  description: Minimum similarity score (0-1)
                  default: 0.4
                  minimum: 0.0
                  maximum: 1.0
                  
      responses:
        '200':
          description: Search completed successfully
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    example: "success"
                  data:
                    type: object
                    properties:
                      query:
                        type: string
                        example: "Th√∫ c∆∞ng m√† user th√≠ch"
                      results_count:
                        type: integer
                        example: 3
                      results:
                        type: array
                        items:
                          type: object
                          properties:
                            fact_id:
                              type: string
                            content:
                              type: string
                              example: "User th√≠ch ch√≥ v√† m√®o"
                            category:
                              type: string
                              example: "preference"
                            score:
                              type: number
                              format: float
                              example: 0.87
                            created_at:
                              type: string
                              format: date-time
                            related_facts:
                              type: array
                              items:
                                type: string
                      processing_time_ms:
                        type: integer
                        example: 234
```

## 6.2 Request/Response Models

```python
# app/models/requests.py
from pydantic import BaseModel, Field, validator
from typing import List, Optional
from datetime import datetime

class Message(BaseModel):
    """Single message in conversation"""
    role: str = Field(..., description="Message role")
    content: str = Field(..., description="Message content")
    
    @validator('role')
    def validate_role(cls, v):
        if v not in ['user', 'assistant']:
            raise ValueError('role must be user or assistant')
        return v
    
    @validator('content')
    def validate_content(cls, v):
        if not v or len(v.strip()) == 0:
            raise ValueError('content cannot be empty')
        if len(v) > 5000:
            raise ValueError('content too long (max 5000 chars)')
        return v.strip()

class ExtractFactsRequest(BaseModel):
    """Request model cho extract_facts API"""
    user_id: str = Field(..., description="PIKA user ID")
    conversation_id: str = Field(..., description="Unique conversation ID")
    conversation: List[Message] = Field(..., description="Conversation messages")
    
    @validator('conversation')
    def validate_conversation(cls, v):
        if len(v) < 2:
            raise ValueError('conversation must have at least 2 messages')
        if len(v) > 100:
            raise ValueError('conversation too long (max 100 messages)')
        return v

class SearchFactsRequest(BaseModel):
    """Request model cho search_facts API"""
    user_id: str = Field(..., description="PIKA user ID")
    query: str = Field(..., description="Search query")
    limit: int = Field(20, ge=1, le=100, description="Max results")
    score_threshold: float = Field(0.4, ge=0.0, le=1.0, description="Min score")
    
    @validator('query')
    def validate_query(cls, v):
        if not v or len(v.strip()) == 0:
            raise ValueError('query cannot be empty')
        if len(v) > 500:
            raise ValueError('query too long (max 500 chars)')
        return v.strip()
```

```python
# app/models/responses.py
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime

class FactResponse(BaseModel):
    """Single fact trong response"""
    fact_id: str
    content: str
    category: str
    score: float
    created_at: datetime
    related_facts: Optional[List[str]] = []

class ExtractFactsResponse(BaseModel):
    """Response model cho extract_facts API"""
    status: str = "success"
    message: str
    data: dict
    
class SearchFactsResponse(BaseModel):
    """Response model cho search_facts API"""
    status: str = "success"
    data: dict

class ErrorResponse(BaseModel):
    """Error response model"""
    status: str = "error"
    error: dict
```

---

# 7. THI·∫æT K·∫æ D·ªÆ LI·ªÜU

## 7.1 Milvus Schema (Vector Store)

```python
# Milvus collection: user_facts
COLLECTION_NAME = "user_facts"
EMBEDDING_DIM = 1536  # OpenAI text-embedding-3-small

collection_schema = {
    "name": COLLECTION_NAME,
    "description": "PIKA user facts with embeddings",
    "fields": [
        {
            "name": "id",
            "dtype": DataType.INT64,
            "is_primary": True,
            "auto_id": True
        },
        {
            "name": "fact_id",
            "dtype": DataType.VARCHAR,
            "max_length": 100,
            "description": "Unique fact ID"
        },
        {
            "name": "user_id",
            "dtype": DataType.VARCHAR,
            "max_length": 100,
            "description": "PIKA user ID"
        },
        {
            "name": "content",
            "dtype": DataType.VARCHAR,
            "max_length": 2000,
            "description": "Fact content text"
        },
        {
            "name": "category",
            "dtype": DataType.VARCHAR,
            "max_length": 50,
            "description": "Fact category"
        },
        {
            "name": "embedding",
            "dtype": DataType.FLOAT_VECTOR,
            "dim": EMBEDDING_DIM,
            "description": "Vector embedding"
        },
        {
            "name": "confidence",
            "dtype": DataType.FLOAT,
            "description": "Extraction confidence score"
        },
        {
            "name": "created_at",
            "dtype": DataType.INT64,
            "description": "Unix timestamp"
        }
    ]
}

# Index configuration
index_params = {
    "metric_type": "IP",  # Inner Product (cosine similarity)
    "index_type": "IVF_FLAT",
    "params": {"nlist": 1024}
}
```

## 7.2 Neo4j Schema (Graph Store)

```cypher
// Node types
CREATE CONSTRAINT user_id IF NOT EXISTS
FOR (u:User) REQUIRE u.id IS UNIQUE;

CREATE CONSTRAINT fact_id IF NOT EXISTS
FOR (f:Fact) REQUIRE f.id IS UNIQUE;

CREATE CONSTRAINT conversation_id IF NOT EXISTS
FOR (c:Conversation) REQUIRE c.id IS UNIQUE;

// Indexes
CREATE INDEX user_created_at IF NOT EXISTS
FOR (u:User) ON (u.created_at);

CREATE INDEX fact_category IF NOT EXISTS
FOR (f:Fact) ON (f.category);

// Example data structure
(:User {id, name, created_at})
(:Fact {id, content, category, confidence, created_at})
(:Conversation {id, user_id, created_at, message_count})

// Relationships
(:User)-[:HAS_FACT]->(:Fact)
(:Fact)-[:RELATED_TO {strength}]->(:Fact)
(:Conversation)-[:CONTAINS_FACT]->(:Fact)
(:Fact)-[:EXTRACTED_FROM]->(:Conversation)
```

## 7.3 PostgreSQL Schema (Metadata)

```sql
-- Table: users (metadata v·ªÅ PIKA users)
CREATE TABLE users (
    id VARCHAR(100) PRIMARY KEY,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    total_conversations INTEGER DEFAULT 0,
    total_facts INTEGER DEFAULT 0,
    metadata JSONB DEFAULT '{}'::jsonb
);

CREATE INDEX idx_users_created_at ON users(created_at DESC);

-- Table: conversations
CREATE TABLE conversations (
    id VARCHAR(100) PRIMARY KEY,
    user_id VARCHAR(100) NOT NULL REFERENCES users(id),
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    message_count INTEGER NOT NULL,
    facts_extracted INTEGER DEFAULT 0,
    raw_conversation JSONB NOT NULL,
    metadata JSONB DEFAULT '{}'::jsonb
);

CREATE INDEX idx_conversations_user_id ON conversations(user_id);
CREATE INDEX idx_conversations_created_at ON conversations(created_at DESC);

-- Table: facts_metadata
CREATE TABLE facts_metadata (
    fact_id VARCHAR(100) PRIMARY KEY,
    user_id VARCHAR(100) NOT NULL REFERENCES users(id),
    conversation_id VARCHAR(100) REFERENCES conversations(id),
    content TEXT NOT NULL,
    category VARCHAR(50) NOT NULL,
    confidence FLOAT NOT NULL,
    milvus_id BIGINT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    accessed_count INTEGER DEFAULT 0,
    last_accessed_at TIMESTAMPTZ,
    metadata JSONB DEFAULT '{}'::jsonb
);

CREATE INDEX idx_facts_user_id ON facts_metadata(user_id);
CREATE INDEX idx_facts_conversation_id ON facts_metadata(conversation_id);
CREATE INDEX idx_facts_category ON facts_metadata(category);
CREATE INDEX idx_facts_created_at ON facts_metadata(created_at DESC);

-- Table: search_logs (analytics)
CREATE TABLE search_logs (
    id BIGSERIAL PRIMARY KEY,
    user_id VARCHAR(100) NOT NULL,
    query TEXT NOT NULL,
    results_count INTEGER NOT NULL,
    top_score FLOAT,
    processing_time_ms INTEGER NOT NULL,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_search_logs_user_id ON search_logs(user_id);
CREATE INDEX idx_search_logs_created_at ON search_logs(created_at DESC);

-- Auto-update updated_at trigger
CREATE OR REPLACE FUNCTION update_updated_at()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER users_updated_at 
BEFORE UPDATE ON users
FOR EACH ROW EXECUTE FUNCTION update_updated_at();
```

## 7.4 Redis Cache Strategy

```python
# Cache keys structure
CACHE_PATTERNS = {
    # Search results cache (5 minutes)
    "search": "search:{user_id}:{query_hash}:{limit}",
    
    # User facts cache (10 minutes)
    "user_facts": "user:facts:{user_id}:latest",
    
    # API rate limiting (1 minute window)
    "rate_limit": "ratelimit:{user_id}:{endpoint}",
    
    # Health check cache (30 seconds)
    "health": "health:status"
}

# TTL settings
TTL_CONFIG = {
    "search_results": 300,      # 5 minutes
    "user_facts": 600,          # 10 minutes
    "rate_limit": 60,           # 1 minute
    "health": 30                # 30 seconds
}
```

---

# 8. CHI·∫æN L∆Ø·ª¢C TRI·ªÇN KHAI

## 8.1 Implementation Roadmap (6-8 tu·∫ßn)

### Phase 1: Foundation (Tu·∫ßn 1-2)

**M·ª•c ti√™u:** Setup infrastructure v√† core framework

|Task|Owner|Output|Ng√†y ho√†n th√†nh|
|---|---|---|---|
|Setup project structure|Backend Lead|Repo initialized|Ng√†y 3|
|Config Milvus connection|Backend Dev|MilvusRepository working|Ng√†y 5|
|Config Neo4j connection|Backend Dev|Neo4jRepository working|Ng√†y 5|
|Setup PostgreSQL schema|Backend Dev|DB schema deployed|Ng√†y 7|
|Setup Redis connection|Backend Dev|RedisRepository working|Ng√†y 7|
|FastAPI skeleton|Backend Lead|API server running|Ng√†y 10|
|Basic logging & metrics|DevOps|Structured logs|Ng√†y 14|

**Exit Criteria:**

- ‚úÖ All database connections working
- ‚úÖ FastAPI server responding to /health
- ‚úÖ Can write & read from all datastores
- ‚úÖ Logging to stdout (JSON format)

### Phase 2: Core Features (Tu·∫ßn 3-4)

**M·ª•c ti√™u:** Implement 2 core APIs

|Task|Owner|Output|Ng√†y ho√†n th√†nh|
|---|---|---|---|
|Implement FactExtractor service|Backend Dev|Extraction logic|Ng√†y 17|
|Implement FactSearcher service|Backend Dev|Search logic|Ng√†y 20|
|OpenAI integration (LLM)|Backend Dev|Fact extraction working|Ng√†y 21|
|OpenAI integration (Embeddings)|Backend Dev|Vectorization working|Ng√†y 21|
|POST /extract_facts endpoint|Backend Dev|API working|Ng√†y 24|
|POST /search_facts endpoint|Backend Dev|API working|Ng√†y 24|
|Unit tests (80% coverage)|QA + Backend|Tests passing|Ng√†y 28|

**Exit Criteria:**

- ‚úÖ Both APIs functional end-to-end
- ‚úÖ Can extract facts from sample conversation
- ‚úÖ Can search facts with semantic query
- ‚úÖ Unit tests passing

### Phase 3: Production Hardening (Tu·∫ßn 5-6)

**M·ª•c ti√™u:** Production-ready features

|Task|Owner|Output|Ng√†y ho√†n th√†nh|
|---|---|---|---|
|Error handling & retries|Backend Dev|Resilience patterns|Ng√†y 31|
|API rate limiting|Backend Dev|Rate limiter middleware|Ng√†y 33|
|Redis caching layer|Backend Dev|Cache working|Ng√†y 35|
|Monitoring dashboards|DevOps|Grafana dashboards|Ng√†y 37|
|Integration tests|QA|Tests passing|Ng√†y 39|
|Load testing (200 rps)|QA|Performance report|Ng√†y 40|
|API documentation|Backend Lead|OpenAPI spec|Ng√†y 42|

**Exit Criteria:**

- ‚úÖ Error rate <0.1%
- ‚úÖ P95 latency <200ms
- ‚úÖ Can handle 200 rps
- ‚úÖ Monitoring working

### Phase 4: Migration & Launch (Tu·∫ßn 7-8)

**M·ª•c ti√™u:** Migrate t·ª´ Mem0 Enterprise v√† go-live

|Task|Owner|Output|Ng√†y ho√†n th√†nh|
|---|---|---|---|
|Data migration script|Backend Dev|Migration tool|Ng√†y 45|
|Migrate existing facts|Backend Dev|Data migrated|Ng√†y 47|
|Parallel testing (dual write)|Backend + QA|Validation report|Ng√†y 49|
|Staging deployment|DevOps|Staging live|Ng√†y 51|
|UAT with product team|Product|Sign-off|Ng√†y 53|
|Production deployment|DevOps|Production live|Ng√†y 55|
|Cut-over from Mem0|Backend Lead|Full migration|Ng√†y 56|

**Exit Criteria:**

- ‚úÖ All existing facts migrated
- ‚úÖ UAT passed
- ‚úÖ Production stable for 48h
- ‚úÖ Mem0 Enterprise decommissioned

## 8.2 Deployment Strategy

### Docker Setup

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - MILVUS_HOST=124.197.21.40
      - MILVUS_PORT=8000
      - NEO4J_URI=bolt://124.197.21.40:8687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=mem0graph
      - POSTGRES_URL=postgresql://user:pass@postgres:5432/pika_mem0
      - REDIS_URL=redis://redis:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - postgres
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=pika_mem0
      - POSTGRES_USER=pika
      - POSTGRES_PASSWORD=secure_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
      
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      
volumes:
  postgres_data:
  redis_data:
```

### Kubernetes Deployment

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pika-mem0-api
  namespace: pika-production
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: pika-mem0-api
  template:
    metadata:
      labels:
        app: pika-mem0-api
    spec:
      containers:
      - name: api
        image: ghcr.io/pika/mem0-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: MILVUS_HOST
          value: "124.197.21.40"
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: pika-secrets
              key: openai-api-key
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
```

## 8.3 Monitoring & Alerting

```yaml
# Alert rules
alerts:
  - name: HighErrorRate
    condition: error_rate > 1% for 5min
    severity: critical
    action: Page on-call engineer
    
  - name: HighLatency
    condition: p95_latency > 300ms for 10min
    severity: warning
    action: Notify team Slack
    
  - name: LowThroughput
    condition: rps < 50 for 15min
    severity: warning
    action: Check upstream services
    
  - name: MilvusDown
    condition: milvus_connection = false
    severity: critical
    action: Page DevOps immediately
```

---

# 9. TESTING V√Ä QUALITY ASSURANCE

## 9.1 Test Strategy

|Test Level|Coverage Target|Tools|Ai ch·ªãu tr√°ch nhi·ªám|
|---|---|---|---|
|Unit Tests|80%+|pytest|Backend Engineers|
|Integration Tests|60%+|pytest + testcontainers|Backend + QA|
|Load Tests|N/A|Locust|QA Engineer|
|Security Tests|OWASP Top 10|Bandit, Safety|Security Team|

## 9.2 Test Cases

### Unit Tests

```python
# tests/unit/test_fact_extractor.py
import pytest
from app.services.fact_extractor import FactExtractorService

@pytest.mark.asyncio
async def test_extract_facts_success(mock_openai, mock_milvus):
    """Test successful fact extraction"""
    # Arrange
    service = FactExtractorService(mock_openai, mock_milvus, ...)
    conversation = [
        {"role": "user", "content": "M√¨nh th√≠ch ch∆°i b√≥ng ƒë√°"},
        {"role": "assistant", "content": "Tuy·ªát v·ªùi!"}
    ]
    
    # Act
    facts = await service.extract_facts("user_001", "conv_001", conversation)
    
    # Assert
    assert len(facts) > 0
    assert any("b√≥ng ƒë√°" in f.content.lower() for f in facts)
```

### Integration Tests

```python
# tests/integration/test_end_to_end.py
@pytest.mark.asyncio
async def test_extract_and_search_flow(test_client):
    """Test full flow: extract -> search"""
    # Step 1: Extract facts
    extract_response = await test_client.post("/v1/extract_facts", json={
        "user_id": "test_user",
        "conversation_id": "test_conv",
        "conversation": [
            {"role": "user", "content": "M√¨nh nu√¥i m√®o t√™n Miu"},
            {"role": "assistant", "content": "D·ªÖ th∆∞∆°ng qu√°!"}
        ]
    })
    assert extract_response.status_code == 200
    
    # Wait for indexing
    await asyncio.sleep(1)
    
    # Step 2: Search for the fact
    search_response = await test_client.post("/v1/search_facts", json={
        "user_id": "test_user",
        "query": "th√∫ c∆∞ng",
        "limit": 10
    })
    assert search_response.status_code == 200
    results = search_response.json()["data"]["results"]
    assert len(results) > 0
    assert any("m√®o" in r["content"].lower() for r in results)
```

### Load Tests

```python
# tests/load/locustfile.py
from locust import HttpUser, task, between

class MemoryAPIUser(HttpUser):
    wait_time = between(1, 3)
    
    @task(7)
    def search_facts(self):
        """70% traffic: search"""
        self.client.post("/v1/search_facts", json={
            "user_id": f"user_{self.user_id}",
            "query": "s·ªü th√≠ch",
            "limit": 20
        })
    
    @task(3)
    def extract_facts(self):
        """30% traffic: extract"""
        self.client.post("/v1/extract_facts", json={
            "user_id": f"user_{self.user_id}",
            "conversation_id": f"conv_{time.time()}",
            "conversation": [
                {"role": "user", "content": "Test conversation"},
                {"role": "assistant", "content": "Response"}
            ]
        })
```

**Load Test Targets:**

- Sustained: 200 rps for 30 minutes
- Peak: 500 rps for 5 minutes
- P95 latency: <200ms
- Error rate: <0.1%

---

# 10. PRODUCTION READINESS CHECKLIST

## ‚úÖ Checklist Tr∆∞·ªõc Khi Launch

### Architecture & Code

- [ ] Code review completed (2+ reviewers)
- [ ] All unit tests passing (coverage >80%)
- [ ] All integration tests passing
- [ ] Load test completed successfully
- [ ] Security scan completed (no critical issues)
- [ ] API documentation complete

### Infrastructure

- [ ] All services deployed to staging
- [ ] Milvus connection stable
- [ ] Neo4j connection stable
- [ ] PostgreSQL backup configured
- [ ] Redis persistence configured
- [ ] Health check endpoints working

### Monitoring & Observability

- [ ] Datadog dashboards created
- [ ] Alert rules configured
- [ ] On-call rotation setup
- [ ] Runbook documented
- [ ] Log aggregation working

### Security

- [ ] API key rotation policy
- [ ] Secrets in environment variables (not hardcoded)
- [ ] Rate limiting configured
- [ ] Input validation on all endpoints
- [ ] HTTPS enforced

### Migration

- [ ] Data migration script tested
- [ ] Rollback plan documented
- [ ] Dual-write period planned (7 days)
- [ ] Validation queries prepared

### Documentation

- [ ] API documentation published
- [ ] Architecture diagram updated
- [ ] Deployment guide written
- [ ] Troubleshooting guide written

---

## üìä K·∫æT LU·∫¨N

### Summary

Document n√†y cung c·∫•p **thi·∫øt k·∫ø production-ready** cho vi·ªác migrate PIKA Memory System t·ª´ Mem0 Enterprise sang self-hosted solution. V·ªõi ki·∫øn tr√∫c r√µ r√†ng, API spec chi ti·∫øt, v√† implementation plan c·ª• th·ªÉ, team c√≥ th·ªÉ tri·ªÉn khai h·ªá th·ªëng trong **6-8 tu·∫ßn**.

### Key Benefits

|Benefit|Impact|
|---|---|
|**Cost Savings**|-60% ($250 vs $600/month)|
|**Data Privacy**|100% in-house, GDPR compliant|
|**Customization**|Full control ƒë·ªÉ optimize cho PIKA|
|**Performance**|<200ms latency, 200+ rps|
|**Independence**|Kh√¥ng ph·ª• thu·ªôc external service|

### Next Steps

1. **Week 1:** Kickoff meeting, setup project repo
2. **Week 2-4:** Development Phase 1-2
3. **Week 5-6:** Production hardening
4. **Week 7-8:** Migration v√† launch

### Success Metrics (3 th√°ng sau launch)

- ‚úÖ Cost gi·∫£m 60%
- ‚úÖ P95 latency <200ms
- ‚úÖ Uptime 99.9%
- ‚úÖ Memory accuracy >90%
- ‚úÖ Zero Mem0 Enterprise dependency

---

**Approved by:**

- [ ] Tech Lead: SU
- [ ] Product Manager: _____________
- [ ] DevOps Lead: _____________

**Version:** 1.0  
**Last Updated:** 2024-12-19  
**Next Review:** 2025-01-15


# ‚ùå ƒê√ÅNH GI√Å: SDD Document CH∆ØA ƒê·∫¶Y ƒê·ª¶

M√¨nh ƒë√£ vi·∫øt ƒë∆∞·ª£c **~50-60% so v·ªõi template g·ªëc**. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt:

---

## ‚úÖ ƒê√É C√ì (Sections Complete)

|Section|Status|Ch·∫•t l∆∞·ª£ng|
|---|---|---|
|1. Executive Summary|‚úÖ ƒê·∫ßy ƒë·ªß|Good|
|2. Introduction|‚úÖ ƒê·∫ßy ƒë·ªß|Good|
|3. Goals, Scope & Constraints|‚úÖ ƒê·∫ßy ƒë·ªß|Good|
|4. High-Level Design (HLD)|‚úÖ ƒê·∫ßy ƒë·ªß|Good - c√≥ diagrams|
|5. Low-Level Design (LLD)|‚úÖ ƒê·∫ßy ƒë·ªß|Good - c√≥ code examples|
|6. API Design & Contracts|‚úÖ ƒê·∫ßy ƒë·ªß|Excellent - c√≥ OpenAPI spec|
|7. Data Design|‚úÖ ƒê·∫ßy ƒë·ªß|Excellent - ƒë·∫ßy ƒë·ªß schemas|
|8. Deployment & Operations|‚úÖ ƒê·∫ßy ƒë·ªß|Good - c√≥ Docker/K8s|
|9. Testing Strategy|‚úÖ ƒê·∫ßy ƒë·ªß|Good - c√≥ test cases|
|10. Implementation Roadmap|‚úÖ ƒê·∫ßy ƒë·ªß|Excellent - chi ti·∫øt timeline|
|11. Production Readiness Checklist|‚úÖ ƒê·∫ßy ƒë·ªß|Good|

---

## ‚ùå THI·∫æU (Critical Missing Sections)

### üî¥ PRIORITY 1 - R·∫•t Quan Tr·ªçng (Ph·∫£i c√≥ tr∆∞·ªõc khi production)

#### **Section 9: Security Design** ‚ùå

**T·∫°i sao quan tr·ªçng:**

- PIKA x·ª≠ l√Ω data c·ªßa tr·∫ª em ‚Üí GDPR compliance b·∫Øt bu·ªôc
- C·∫ßn ƒë·ªãnh nghƒ©a authentication, authorization, encryption
- Security l√† blocker cho production launch

**N·ªôi dung c·∫ßn b·ªï sung:**

```markdown
## 9. SECURITY DESIGN

### 9.1 STRIDE Threat Model
- Spoofing: API key authentication
- Tampering: Input validation, checksums
- Repudiation: Audit logging
- Information Disclosure: Encryption at rest/transit
- Denial of Service: Rate limiting
- Elevation of Privilege: RBAC

### 9.2 Authentication & Authorization
- API Key authentication cho PIKA robot
- Role-based access control (RBAC)
- JWT tokens n·∫øu c√≥ admin panel

### 9.3 Data Protection
- Encryption at rest: AES-256 cho PostgreSQL
- Encryption in transit: TLS 1.3
- PII handling: Tokenization cho sensitive data
- Password hashing: Argon2id

### 9.4 Security Controls
- Input validation (prevent SQL injection, XSS)
- Rate limiting (DDoS protection)
- CORS configuration
- Security headers (HSTS, CSP, X-Frame-Options)

### 9.5 Compliance
- GDPR: Right to be forgotten, data export
- Data retention policy: 2 years
- Audit logging: All data access logged
```

---

#### **Section 10: Resilience & Reliability** ‚ùå

**T·∫°i sao quan tr·ªçng:**

- PIKA l√† real-time system, kh√¥ng th·ªÉ downtime
- C·∫ßn define SLA v√† c√°ch ƒë·∫°t ƒë∆∞·ª£c n√≥
- C·∫ßn retry logic, circuit breakers, fallback strategies

**N·ªôi dung c·∫ßn b·ªï sung:**


## 10. RESILIENCE & RELIABILITY

### 10.1 SLA/SLO/SLI
| Metric | SLI | SLO | SLA |
|--------|-----|-----|-----|
| Availability | Uptime % | 99.9% | 99.5% |
| Latency | P95 response time | <200ms | <300ms |
| Error Rate | Failed requests % | <0.1% | <0.5% |

### 10.2 Circuit Breaker Pattern
```python
from pybreaker import CircuitBreaker

milvus_breaker = CircuitBreaker(
    fail_max=5,           # Open after 5 failures
    reset_timeout=60,     # Try again after 60s
)

@milvus_breaker
async def search_milvus(query_vector):
    return await milvus_client.search(query_vector)
````

### 10.3 Retry Strategy

- Exponential backoff: 1s, 2s, 4s, 8s
- Max retries: 3
- Retry on: 503, 504, Connection errors
- DO NOT retry on: 400, 401, 404

### 10.4 Fallback Strategies

|Service Down|Fallback|
|---|---|
|Milvus|Return cached results ho·∫∑c empty|
|Neo4j|Skip relationship enrichment|
|OpenAI|Use cached embeddings ho·∫∑c local model|
|PostgreSQL|Return error (critical dependency)|

### 10.5 Graceful Degradation

- N·∫øu Milvus down: Search trong PostgreSQL (slower)
- N·∫øu Neo4j down: Skip relationships, return flat facts
- N·∫øu Redis down: Bypass cache, query directly

### 10.6 Health Checks

```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow(),
        "checks": {
            "milvus": await check_milvus(),
            "neo4j": await check_neo4j(),
            "postgres": await check_postgres(),
            "redis": await check_redis()
        }
    }
```

````

---

#### **Section 14: Non-Functional Requirements (NFR)** ‚ùå
**T·∫°i sao quan tr·ªçng:**
- Define r√µ performance targets ƒë·ªÉ test
- Basis cho capacity planning
- QA team c·∫ßn ƒë·ªÉ vi·∫øt test cases

**N·ªôi dung c·∫ßn b·ªï sung:**
```markdown
## 14. NON-FUNCTIONAL REQUIREMENTS

### 14.1 Performance Requirements
| Metric | Target | Measurement |
|--------|--------|-------------|
| API Response Time (P50) | <100ms | Datadog APM |
| API Response Time (P95) | <200ms | Datadog APM |
| API Response Time (P99) | <500ms | Datadog APM |
| Throughput | 200 rps sustained | Load test |
| Concurrent Users | 100K+ | Load test |

### 14.2 Scalability Requirements
- Horizontal scaling: Support 3-10 API pods
- Database: PostgreSQL read replicas for scaling reads
- Cache: Redis cluster for high availability
- Vector store: Milvus supports sharding

### 14.3 Availability Requirements
- Uptime SLA: 99.9% (8.76 hours downtime/year)
- MTBF (Mean Time Between Failures): >720 hours
- MTTR (Mean Time To Repair): <30 minutes
- RPO (Recovery Point Objective): <15 minutes
- RTO (Recovery Time Objective): <1 hour

### 14.4 Maintainability Requirements
- Code coverage: >80%
- Documentation: 100% public APIs
- Cyclomatic complexity: <10 per function
- Tech debt ratio: <5% (SonarQube)
````

---

### üü° PRIORITY 2 - Quan Tr·ªçng (N√™n c√≥ tr∆∞·ªõc launch)

#### **Section 15: Performance & Capacity Planning** ‚ùå

**N·ªôi dung c·∫ßn b·ªï sung:**

```markdown
## 15. PERFORMANCE & CAPACITY PLANNING

### 15.1 Capacity Planning Formulas
**Traffic Estimation:**
```

Daily Active Users (DAU) = 100K Peak Concurrent Users = DAU √ó 0.1 = 10K Actions per Minute per User = 2 Peak RPS = 10K √ó 2 / 60 = 333 RPS Safety Factor = 2x Required Capacity = 333 √ó 2 = 666 RPS

```

**Database Sizing:**
```

Facts per User per Month = 100 Total Users = 100K Total Facts = 100K √ó 100 = 10M facts Avg Fact Size = 1KB (text) + 6KB (vector) = 7KB Storage Needed = 10M √ó 7KB √ó 3 (replication) = 210GB

```

### 15.2 Capacity Planning Table
| Component | Current | +6mo | +1yr | +2yr |
|-----------|---------|------|------|------|
| DAU | 10K | 50K | 150K | 500K |
| API RPS | 100 | 500 | 1500 | 5000 |
| DB Storage | 50GB | 150GB | 400GB | 1TB |
| API Pods | 3 | 10 | 25 | 60 |
| Cost/month | $250 | $500 | $1200 | $3000 |
```

---

#### **Section 17: Trade-offs & Architecture Decisions (ADR)** ‚ùå

**N·ªôi dung c·∫ßn b·ªï sung:**

```markdown
## 17. TRADE-OFFS & ARCHITECTURE DECISIONS

### ADR-001: Vector Store Selection
| Decision | Milvus vs Pinecone vs Weaviate |
|----------|-------------------------------|
| **Chosen** | Milvus |
| **Rationale** | Self-hosted (ƒë√£ c√≥ infra), open-source, performance t·ªët |
| **Trade-off** | ‚úÖ No vendor lock-in, ‚ùå C·∫ßn maintain infrastructure |

### ADR-002: Graph Store Selection
| Decision | Neo4j vs Neptune vs TigerGraph |
|----------|-------------------------------|
| **Chosen** | Neo4j |
| **Rationale** | ƒê√£ c√≥ infra, team familiar, Cypher query d·ªÖ |
| **Trade-off** | ‚úÖ Mature ecosystem, ‚ùå Resource intensive |

### ADR-003: LLM Provider
| Decision | OpenAI vs Anthropic vs Local LLM |
|----------|----------------------------------|
| **Chosen** | OpenAI API |
| **Rationale** | Best quality, fast inference, reasonable cost |
| **Trade-off** | ‚úÖ Quality, ‚ùå Dependency on external service |

### ADR-004: Caching Strategy
| Decision | Redis vs Memcached |
|----------|-------------------|
| **Chosen** | Redis |
| **Rationale** | More features (pub/sub, persistence), team familiar |
| **Trade-off** | ‚úÖ Rich features, ‚ùå Higher memory usage |
```

---

#### **Section 18: Incident Response & Runbooks** ‚ùå

**T·∫°i sao quan tr·ªçng:**

- On-call engineer c·∫ßn bi·∫øt l√†m g√¨ khi c√≥ incident
- Gi·∫£m MTTR (Mean Time To Repair)

**N·ªôi dung c·∫ßn b·ªï sung:**

````markdown
## 18. INCIDENT RESPONSE & RUNBOOKS

### 18.1 Severity Levels
| Severity | Definition | Response Time |
|----------|------------|---------------|
| SEV1 | API down, data loss | 5 minutes |
| SEV2 | Major feature broken | 15 minutes |
| SEV3 | Performance degradation | 1 hour |

### 18.2 Runbook: High Error Rate
**Alert:** Error rate >1% for 5 minutes

**Diagnosis Steps:**
```bash
# 1. Check service health
kubectl get pods -n pika
kubectl logs -f deployment/pika-mem0-api --tail=100

# 2. Check dependencies
curl http://124.197.21.40:8000/_health  # Milvus
curl http://124.197.21.40:8687 # Neo4j

# 3. Check recent deployments
kubectl rollout history deployment/pika-mem0-api
````

**Resolution:**

```bash
# If bad deployment:
kubectl rollout undo deployment/pika-mem0-api

# If dependency issue:
# Restart pods to reset connections
kubectl rollout restart deployment/pika-mem0-api
```

### 18.3 On-Call Rotation

- Primary: Backend engineer (weekly rotation)
- Secondary: Backend lead (escalation)
- Manager: Engineering manager (critical only)

````

---

### üü¢ PRIORITY 3 - Nice to Have (C√≥ th√¨ t·ªët)

#### **Section 22: Common Mistakes & Anti-Patterns** ‚ùå
```markdown
## 22. COMMON MISTAKES & ANTI-PATTERNS

### ‚ùå Anti-Pattern: N+1 Query Problem
**Bad:**
```python
# Get facts, then loop to get relationships
facts = await milvus.search(query_vector)
for fact in facts:
    relations = await neo4j.get_relationships(fact.id)  # N queries!
````

**Good:**

```python
# Batch fetch relationships
facts = await milvus.search(query_vector)
fact_ids = [f.id for f in facts]
all_relations = await neo4j.get_relationships_batch(fact_ids)  # 1 query
```

### ‚ùå Anti-Pattern: Missing Idempotency

**Bad:**

```python
@app.post("/extract_facts")
async def extract(request):
    facts = await extract_facts(request)
    await save_facts(facts)  # Duplicate on retry!
    return facts
```

**Good:**

```python
@app.post("/extract_facts")
async def extract(request, idempotency_key: str = Header()):
    cached = await redis.get(f"idempotency:{idempotency_key}")
    if cached:
        return cached  # Return cached result
        
    facts = await extract_facts(request)
    await save_facts(facts)
    await redis.setex(f"idempotency:{idempotency_key}", 86400, facts)
    return facts
```

````

---

#### **Section 23: Tool Recommendations** ‚ùå
```markdown
## 23. TOOL RECOMMENDATIONS

### Backend Framework Comparison
| Framework | Pros | Cons | Recommendation |
|-----------|------|------|----------------|
| **FastAPI** | Async, fast, type hints | New ecosystem | ‚úÖ CH·ªåN |
| Flask | Mature, simple | Sync only | ‚ùå |
| Django | Full-featured | Heavy, slow | ‚ùå |

### Vector Store Comparison
| Store | Performance | Cost | Ops Complexity |
|-------|-------------|------|----------------|
| **Milvus** | High | Free (self-host) | Medium | ‚úÖ CH·ªåN |
| Pinecone | Very High | $$$ | Low (managed) | ‚ùå Cost |
| Weaviate | High | Free | High | ‚ùå Complex |
````

---

#### **Section 24: Appendices** ‚ùå

```markdown
## 24. APPENDICES

### 24.1 Glossary
| Term | Definition |
|------|------------|
| DAU | Daily Active Users |
| SLA | Service Level Agreement |
| MTTR | Mean Time To Repair |

### 24.2 References
- Milvus Documentation: https://milvus.io/docs
- Neo4j Cypher Guide: https://neo4j.com/docs
- OpenAI API Docs: https://platform.openai.com/docs

### 24.3 Revision History
| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2024-12-19 | SU | Initial draft |
```

---

## üìä T√ìM T·∫ÆT ƒê·ªò ƒê·∫¶Y ƒê·ª¶

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        COMPLETENESS BREAKDOWN               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  60%                 ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚úÖ Complete:     11/24 sections            ‚îÇ
‚îÇ  ‚ùå Missing:      13/24 sections            ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  Priority 1 (Critical):  4 sections missing ‚îÇ
‚îÇ  Priority 2 (Important): 3 sections missing ‚îÇ
‚îÇ  Priority 3 (Nice):      6 sections missing ‚îÇ
‚îÇ                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ KHUY·∫æN NGH·ªä

### ƒê·ªÉ Launch MVP (Minimum):

C·∫ßn b·ªï sung **PRIORITY 1** sections:

1. ‚úÖ Section 9: Security Design
2. ‚úÖ Section 10: Resilience & Reliability
3. ‚úÖ Section 14: NFR
4. ‚úÖ Section 18: Runbooks

**Effort:** ~1-2 ng√†y ƒë·ªÉ vi·∫øt ƒë·∫ßy ƒë·ªß

### ƒê·ªÉ Production-Ready (Recommended):

C·∫ßn b·ªï sung th√™m **PRIORITY 2**: 5. ‚úÖ Section 15: Capacity Planning 6. ‚úÖ Section 17: Architecture Decisions

**Total Effort:** ~3-4 ng√†y ƒë·ªÉ c√≥ document ho√†n ch·ªânh

---

B·∫°n mu·ªën m√¨nh b·ªï sung ngay **4 sections PRIORITY 1** ƒë·ªÉ document ƒë·∫°t production-ready kh√¥ng? Ho·∫∑c b·∫°n c√≥ mu·ªën t·∫≠p trung v√†o section n√†o tr∆∞·ªõc?

# üìù B·ªî SUNG 4 SECTIONS PRIORITY 1

M√¨nh s·∫Ω b·ªï sung ngay 4 sections quan tr·ªçng nh·∫•t ƒë·ªÉ document ƒë·∫°t production-ready:

---

# 9. THI·∫æT K·∫æ B·∫¢O M·∫¨T (SECURITY DESIGN)

## 9.1 STRIDE Threat Model

PIKA Memory System x·ª≠ l√Ω d·ªØ li·ªáu nh·∫°y c·∫£m c·ªßa tr·∫ª em, c·∫ßn security nghi√™m ng·∫∑t theo STRIDE framework:

|Threat|Risk|Mitigation Strategy|Priority|
|---|---|---|---|
|**Spoofing**|Attacker gi·∫£ m·∫°o PIKA robot g·ªçi API|API Key authentication v·ªõi rotation 90 ng√†y|P0|
|**Tampering**|Modify facts trong database|Input validation + checksums + audit log|P0|
|**Repudiation**|Kh√¥ng trace ƒë∆∞·ª£c ai l√†m g√¨|Comprehensive audit logging m·ªçi operation|P1|
|**Information Disclosure**|Leak data tr·∫ª em|Encryption at rest/transit + access control|P0|
|**Denial of Service**|Spam API l√†m service down|Rate limiting + WAF + DDoS protection|P0|
|**Elevation of Privilege**|Unauthorized access admin functions|RBAC + principle of least privilege|P1|

## 9.2 Authentication & Authorization

### 9.2.1 API Key Authentication

```python
# app/middleware/auth.py
from fastapi import Security, HTTPException
from fastapi.security.api_key import APIKeyHeader

API_KEY_HEADER = APIKeyHeader(name="X-API-Key", auto_error=True)

class APIKeyValidator:
    """
    Validate API key from request header
    
    Format: X-API-Key: pika_live_sk_abc123xyz...
    """
    
    def __init__(self, redis_client):
        self.redis = redis_client
        
    async def validate_api_key(
        self,
        api_key: str = Security(API_KEY_HEADER)
    ) -> dict:
        """
        Validate API key and return associated metadata
        
        Returns:
            {
                "app_id": "pika_robot_prod",
                "permissions": ["extract_facts", "search_facts"],
                "rate_limit": 1000  # requests per minute
            }
        """
        # Check key format
        if not api_key.startswith("pika_"):
            raise HTTPException(
                status_code=401,
                detail="Invalid API key format"
            )
        
        # Check key exists and not revoked
        key_data = await self.redis.get(f"api_key:{api_key}")
        if not key_data:
            raise HTTPException(
                status_code=401,
                detail="Invalid or revoked API key"
            )
        
        key_info = json.loads(key_data)
        
        # Check expiration
        if key_info.get("expires_at"):
            if datetime.utcnow() > datetime.fromisoformat(key_info["expires_at"]):
                raise HTTPException(
                    status_code=401,
                    detail="API key expired"
                )
        
        return key_info

# Usage in endpoints
@app.post("/v1/extract_facts")
async def extract_facts(
    request: ExtractFactsRequest,
    key_info: dict = Depends(api_key_validator.validate_api_key)
):
    # Verify permission
    if "extract_facts" not in key_info["permissions"]:
        raise HTTPException(403, "Insufficient permissions")
    
    # Process request...
```

### 9.2.2 API Key Management

```python
# scripts/manage_api_keys.py
import secrets
import hashlib

class APIKeyManager:
    """Manage API key lifecycle"""
    
    @staticmethod
    def generate_api_key(app_id: str) -> tuple[str, str]:
        """
        Generate new API key
        
        Returns:
            (plain_key, hashed_key) tuple
        """
        # Generate random 32-byte key
        random_bytes = secrets.token_bytes(32)
        plain_key = f"pika_live_sk_{random_bytes.hex()}"
        
        # Hash for storage (never store plain key)
        hashed_key = hashlib.sha256(plain_key.encode()).hexdigest()
        
        return plain_key, hashed_key
    
    async def create_api_key(
        self,
        app_id: str,
        permissions: list[str],
        rate_limit: int = 1000,
        expires_days: int = 90
    ) -> str:
        """Create and store API key"""
        plain_key, hashed_key = self.generate_api_key(app_id)
        
        key_data = {
            "app_id": app_id,
            "permissions": permissions,
            "rate_limit": rate_limit,
            "created_at": datetime.utcnow().isoformat(),
            "expires_at": (datetime.utcnow() + timedelta(days=expires_days)).isoformat(),
            "status": "active"
        }
        
        # Store in Redis with hash as key
        await self.redis.setex(
            f"api_key:{plain_key}",
            ttl=expires_days * 86400,
            value=json.dumps(key_data)
        )
        
        # Store in PostgreSQL for audit
        await self.db.execute(
            """
            INSERT INTO api_keys (hashed_key, app_id, permissions, expires_at)
            VALUES ($1, $2, $3, $4)
            """,
            hashed_key, app_id, permissions, key_data["expires_at"]
        )
        
        return plain_key  # Return only once, never again!
    
    async def revoke_api_key(self, api_key: str):
        """Revoke an API key immediately"""
        await self.redis.delete(f"api_key:{api_key}")
        
        hashed_key = hashlib.sha256(api_key.encode()).hexdigest()
        await self.db.execute(
            "UPDATE api_keys SET status = 'revoked', revoked_at = NOW() WHERE hashed_key = $1",
            hashed_key
        )
```

## 9.3 Data Protection

### 9.3.1 Encryption at Rest

```python
# app/security/encryption.py
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2

class DataEncryption:
    """
    Encrypt sensitive data before storing
    
    Use cases:
    - User PII (names, emails)
    - Conversation content
    - Any sensitive facts
    """
    
    def __init__(self, master_key: str):
        """
        master_key should be stored in AWS Secrets Manager
        or environment variable, NEVER in code
        """
        self.fernet = Fernet(master_key.encode())
    
    def encrypt(self, plaintext: str) -> str:
        """Encrypt plaintext to base64 ciphertext"""
        if not plaintext:
            return plaintext
            
        ciphertext_bytes = self.fernet.encrypt(plaintext.encode())
        return base64.b64encode(ciphertext_bytes).decode()
    
    def decrypt(self, ciphertext: str) -> str:
        """Decrypt base64 ciphertext to plaintext"""
        if not ciphertext:
            return ciphertext
            
        ciphertext_bytes = base64.b64decode(ciphertext.encode())
        plaintext_bytes = self.fernet.decrypt(ciphertext_bytes)
        return plaintext_bytes.decode()

# Usage in services
class FactExtractorService:
    def __init__(self, encryptor: DataEncryption):
        self.encryptor = encryptor
    
    async def save_conversation(self, conversation: list[dict]):
        """Save conversation with PII encrypted"""
        encrypted_conversation = [
            {
                "role": msg["role"],
                "content": self.encryptor.encrypt(msg["content"])  # Encrypt
            }
            for msg in conversation
        ]
        
        await self.postgres.save(encrypted_conversation)
```

### 9.3.2 Encryption in Transit

```yaml
# TLS 1.3 Configuration
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: pika-mem0-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-protocols: "TLSv1.3"
    nginx.ingress.kubernetes.io/ssl-ciphers: "TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
    - hosts:
        - api-memory.pika.stepup.edu.vn
      secretName: pika-mem0-tls
  rules:
    - host: api-memory.pika.stepup.edu.vn
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: pika-mem0-api
                port:
                  number: 8000
```

## 9.4 Input Validation & Sanitization

```python
# app/security/validation.py
import re
import html
from pydantic import validator

class SecureRequest(BaseModel):
    """Base model v·ªõi security validation"""
    
    @staticmethod
    def sanitize_text(text: str) -> str:
        """
        Sanitize user input to prevent XSS, SQL injection
        
        Steps:
        1. Remove HTML tags
        2. Escape special characters
        3. Limit length
        """
        if not text:
            return text
        
        # Remove HTML tags
        text = re.sub(r'<[^>]+>', '', text)
        
        # Escape HTML entities
        text = html.escape(text)
        
        # Remove SQL-like patterns (extra safety)
        dangerous_patterns = [
            r"(\bSELECT\b|\bINSERT\b|\bUPDATE\b|\bDELETE\b|\bDROP\b)",
            r"(--|;|\/\*|\*\/)",
            r"(\bUNION\b|\bEXEC\b|\bEXECUTE\b)"
        ]
        for pattern in dangerous_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        return text.strip()

class ExtractFactsRequest(SecureRequest):
    """Extract facts request v·ªõi validation"""
    user_id: str
    conversation: List[Message]
    
    @validator('user_id')
    def validate_user_id(cls, v):
        # Must be UUID format
        if not re.match(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', v):
            raise ValueError("Invalid user_id format")
        return v
    
    @validator('conversation')
    def validate_conversation(cls, v):
        if len(v) > 100:
            raise ValueError("Conversation too long (max 100 messages)")
        
        # Sanitize each message
        for msg in v:
            msg.content = cls.sanitize_text(msg.content)
            if len(msg.content) > 5000:
                raise ValueError("Message too long (max 5000 chars)")
        
        return v
```

## 9.5 Rate Limiting & DDoS Protection

```python
# app/middleware/rate_limit.py
from fastapi import Request, HTTPException
from datetime import datetime, timedelta

class RateLimiter:
    """
    Rate limiting using sliding window algorithm
    
    Tiers:
    - PIKA Production: 1000 req/min
    - PIKA Staging: 100 req/min
    - Development: 10 req/min
    """
    
    def __init__(self, redis_client):
        self.redis = redis_client
        
    async def check_rate_limit(
        self,
        request: Request,
        api_key: str,
        limit: int = 1000,  # requests per minute
        window: int = 60     # seconds
    ):
        """
        Check if request exceeds rate limit
        
        Algorithm: Sliding window counter
        """
        now = datetime.utcnow()
        window_start = now - timedelta(seconds=window)
        
        # Redis key for this API key
        key = f"rate_limit:{api_key}:{now.strftime('%Y%m%d%H%M')}"
        
        # Increment counter
        current = await self.redis.incr(key)
        
        if current == 1:
            # First request in this window, set expiry
            await self.redis.expire(key, window)
        
        if current > limit:
            # Log the violation
            logger.warning(
                "Rate limit exceeded",
                extra={
                    "api_key": api_key[:20] + "...",  # Partial for security
                    "ip": request.client.host,
                    "endpoint": request.url.path,
                    "limit": limit,
                    "current": current
                }
            )
            
            raise HTTPException(
                status_code=429,
                detail={
                    "error": "Rate limit exceeded",
                    "limit": limit,
                    "window": f"{window}s",
                    "retry_after": window
                },
                headers={"Retry-After": str(window)}
            )
        
        # Add rate limit headers to response
        request.state.rate_limit_headers = {
            "X-RateLimit-Limit": str(limit),
            "X-RateLimit-Remaining": str(limit - current),
            "X-RateLimit-Reset": str(int((now + timedelta(seconds=window)).timestamp()))
        }

# Middleware
@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    """Apply rate limiting to all requests"""
    api_key = request.headers.get("X-API-Key")
    
    if api_key:
        # Get rate limit from API key metadata
        key_info = await get_api_key_info(api_key)
        limit = key_info.get("rate_limit", 1000)
        
        await rate_limiter.check_rate_limit(request, api_key, limit)
    
    response = await call_next(request)
    
    # Add rate limit headers
    if hasattr(request.state, "rate_limit_headers"):
        for key, value in request.state.rate_limit_headers.items():
            response.headers[key] = value
    
    return response
```

## 9.6 Audit Logging

```python
# app/security/audit.py
from typing import Optional
import json

class AuditLogger:
    """
    Log all sensitive operations for compliance
    
    Requirements:
    - GDPR: Log all data access/modification
    - Retention: 7 years
    - Immutable: Write-only, cannot delete
    """
    
    def __init__(self, postgres_client):
        self.db = postgres_client
    
    async def log_operation(
        self,
        operation: str,
        user_id: str,
        resource_type: str,
        resource_id: str,
        api_key: str,
        ip_address: str,
        request_data: Optional[dict] = None,
        response_data: Optional[dict] = None,
        status: str = "success",
        error: Optional[str] = None
    ):
        """
        Log an operation to audit table
        
        Operations:
        - extract_facts: When extracting facts from conversation
        - search_facts: When searching user's facts
        - delete_facts: When user exercises right to be forgotten
        - export_data: When user requests data export
        """
        await self.db.execute(
            """
            INSERT INTO audit_logs (
                operation, user_id, resource_type, resource_id,
                api_key_prefix, ip_address, request_data, response_data,
                status, error, created_at
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, NOW())
            """,
            operation,
            user_id,
            resource_type,
            resource_id,
            api_key[:20],  # Only store prefix for security
            ip_address,
            json.dumps(request_data) if request_data else None,
            json.dumps(response_data) if response_data else None,
            status,
            error
        )

# PostgreSQL table for audit logs
"""
CREATE TABLE audit_logs (
    id BIGSERIAL PRIMARY KEY,
    operation VARCHAR(100) NOT NULL,
    user_id VARCHAR(100) NOT NULL,
    resource_type VARCHAR(50) NOT NULL,
    resource_id VARCHAR(100) NOT NULL,
    api_key_prefix VARCHAR(20) NOT NULL,
    ip_address INET NOT NULL,
    request_data JSONB,
    response_data JSONB,
    status VARCHAR(20) NOT NULL,
    error TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Indexes for queries
CREATE INDEX idx_audit_logs_user_id ON audit_logs(user_id);
CREATE INDEX idx_audit_logs_operation ON audit_logs(operation);
CREATE INDEX idx_audit_logs_created_at ON audit_logs(created_at DESC);

-- Partition by month for performance
CREATE TABLE audit_logs_2025_01 PARTITION OF audit_logs
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
"""

# Usage in endpoints
@app.post("/v1/extract_facts")
async def extract_facts(
    request: ExtractFactsRequest,
    http_request: Request,
    key_info: dict = Depends(api_key_validator)
):
    try:
        # Process request
        result = await fact_extractor.extract_facts(
            request.user_id,
            request.conversation_id,
            request.conversation
        )
        
        # Audit log SUCCESS
        await audit_logger.log_operation(
            operation="extract_facts",
            user_id=request.user_id,
            resource_type="conversation",
            resource_id=request.conversation_id,
            api_key=http_request.headers.get("X-API-Key"),
            ip_address=http_request.client.host,
            request_data={"message_count": len(request.conversation)},
            response_data={"facts_extracted": len(result.facts)},
            status="success"
        )
        
        return result
        
    except Exception as e:
        # Audit log FAILURE
        await audit_logger.log_operation(
            operation="extract_facts",
            user_id=request.user_id,
            resource_type="conversation",
            resource_id=request.conversation_id,
            api_key=http_request.headers.get("X-API-Key"),
            ip_address=http_request.client.host,
            status="error",
            error=str(e)
        )
        raise
```

## 9.7 GDPR Compliance

```python
# app/services/gdpr_service.py
class GDPRService:
    """
    Handle GDPR requirements:
    - Right to be forgotten (Art. 17)
    - Right to data portability (Art. 20)
    - Right to access (Art. 15)
    """
    
    async def delete_user_data(self, user_id: str):
        """
        Permanently delete all user data
        
        Steps:
        1. Delete from Milvus (vectors)
        2. Delete from Neo4j (relationships)
        3. Anonymize in PostgreSQL (keep for analytics)
        4. Delete from Redis (cache)
        5. Log deletion in audit log (permanent record)
        """
        logger.info(f"Starting GDPR deletion for user {user_id}")
        
        # 1. Delete vectors from Milvus
        await self.milvus.delete(f"user_id == '{user_id}'")
        
        # 2. Delete graph data from Neo4j
        await self.neo4j.run(
            """
            MATCH (u:User {id: $user_id})
            DETACH DELETE u
            """,
            user_id=user_id
        )
        
        # 3. Anonymize PostgreSQL data (keep for analytics)
        await self.postgres.execute(
            """
            UPDATE facts_metadata 
            SET content = '[REDACTED]', user_id = 'deleted_user'
            WHERE user_id = $1
            """,
            user_id
        )
        
        await self.postgres.execute(
            """
            UPDATE conversations
            SET raw_conversation = '[]'::jsonb, user_id = 'deleted_user'
            WHERE user_id = $1
            """,
            user_id
        )
        
        # 4. Delete cache
        await self.redis.delete(f"user:facts:{user_id}:*")
        
        # 5. Audit log (permanent record of deletion)
        await audit_logger.log_operation(
            operation="gdpr_delete",
            user_id=user_id,
            resource_type="user",
            resource_id=user_id,
            api_key="system",
            ip_address="internal",
            status="success"
        )
        
        logger.info(f"GDPR deletion completed for user {user_id}")
    
    async def export_user_data(self, user_id: str) -> dict:
        """
        Export all user data in JSON format
        
        Returns: Complete data package for user
        """
        # Get all facts
        facts = await self.postgres.fetch(
            "SELECT * FROM facts_metadata WHERE user_id = $1",
            user_id
        )
        
        # Get all conversations
        conversations = await self.postgres.fetch(
            "SELECT * FROM conversations WHERE user_id = $1",
            user_id
        )
        
        # Get audit logs
        audit_logs = await self.postgres.fetch(
            "SELECT * FROM audit_logs WHERE user_id = $1",
            user_id
        )
        
        return {
            "user_id": user_id,
            "exported_at": datetime.utcnow().isoformat(),
            "facts": [dict(f) for f in facts],
            "conversations": [dict(c) for c in conversations],
            "audit_logs": [dict(a) for a in audit_logs]
        }
```

## 9.8 Security Checklist

### Pre-Production Security Review

- [ ] **Authentication**
    - [ ] API key authentication implemented
    - [ ] API key rotation policy (90 days)
    - [ ] API keys stored hashed, not plain text
- [ ] **Authorization**
    - [ ] Permission checks on all endpoints
    - [ ] Principle of least privilege applied
- [ ] **Encryption**
    - [ ] TLS 1.3 enforced for all connections
    - [ ] Sensitive data encrypted at rest (AES-256)
    - [ ] Master keys in Secrets Manager (not in code)
- [ ] **Input Validation**
    - [ ] All inputs validated with Pydantic
    - [ ] SQL injection prevention (parameterized queries)
    - [ ] XSS prevention (sanitization + escape)
- [ ] **Rate Limiting**
    - [ ] Rate limits configured per API key
    - [ ] DDoS protection via WAF
- [ ] **Audit Logging**
    - [ ] All sensitive operations logged
    - [ ] Logs immutable and retained 7 years
- [ ] **GDPR Compliance**
    - [ ] Right to deletion implemented
    - [ ] Data export functionality
    - [ ] Privacy policy updated
- [ ] **Security Scanning**
    - [ ] SAST scan passed (Bandit, Semgrep)
    - [ ] Dependency scan passed (Safety, Snyk)
    - [ ] Container scan passed (Trivy)
    - [ ] Penetration test completed

---

# 10. RESILIENCE & RELIABILITY

## 10.1 SLA/SLO/SLI Definitions

### Service Level Agreement (SLA)

```yaml
SLA_COMMITMENTS:
  availability:
    target: 99.5%  # 3.65 hours downtime/month
    measurement_period: monthly
    penalty: Credit 10% if < 99.5%, 25% if < 99%
    
  latency:
    p95_target: 300ms
    measurement_period: daily
    penalty: Credit 5% if consistently exceeded
    
  error_rate:
    target: < 0.5%
    measurement_period: hourly
    penalty: Incident review required
```

### Service Level Objectives (SLO)

|Metric|SLI|SLO Target|Error Budget|
|---|---|---|---|
|**Availability**|Successful requests / Total requests|99.9%|43 minutes/month|
|**Latency (P95)**|95th percentile response time|<200ms|5% can be >200ms|
|**Latency (P99)**|99th percentile response time|<500ms|1% can be >500ms|
|**Error Rate**|Failed requests / Total requests|<0.1%|0.1% can fail|
|**Throughput**|Requests per second|200+ rps|Must sustain|

### Service Level Indicators (SLI)

```python
# app/monitoring/sli.py
from prometheus_client import Counter, Histogram, Gauge

# SLI Metrics
REQUEST_SUCCESS = Counter(
    'http_requests_success_total',
    'Successful HTTP requests',
    ['method', 'endpoint']
)

REQUEST_FAILED = Counter(
    'http_requests_failed_total',
    'Failed HTTP requests',
    ['method', 'endpoint', 'error_type']
)

REQUEST_LATENCY = Histogram(
    'http_request_latency_seconds',
    'HTTP request latency',
    ['method', 'endpoint'],
    buckets=[0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0]
)

DEPENDENCY_HEALTH = Gauge(
    'dependency_health',
    'Dependency health status (1=healthy, 0=unhealthy)',
    ['service']
)

# Calculate SLI
class SLICalculator:
    """Calculate SLIs from metrics"""
    
    @staticmethod
    def calculate_availability(period_minutes: int = 60) -> float:
        """
        Availability = successful_requests / total_requests
        """
        query = f"""
            sum(rate(http_requests_success_total[{period_minutes}m])) /
            sum(rate(http_requests_total[{period_minutes}m]))
        """
        return prometheus.query(query)
    
    @staticmethod
    def calculate_error_budget_remaining(
        target_availability: float = 0.999,
        period_days: int = 30
    ) -> float:
        """
        Error Budget = (1 - SLO) * total_requests
        Remaining = budget - actual_errors
        """
        total_requests = prometheus.query(
            f"sum(increase(http_requests_total[{period_days}d]))"
        )
        
        total_errors = prometheus.query(
            f"sum(increase(http_requests_failed_total[{period_days}d]))"
        )
        
        error_budget = (1 - target_availability) * total_requests
        remaining = error_budget - total_errors
        
        return (remaining / error_budget) * 100  # percentage
```

## 10.2 Circuit Breaker Pattern

```python
# app/resilience/circuit_breaker.py
from pybreaker import CircuitBreaker
from enum import Enum

class CircuitState(Enum):
    CLOSED = "closed"    # Normal operation
    OPEN = "open"        # Blocking requests
    HALF_OPEN = "half_open"  # Testing recovery

class ServiceCircuitBreakers:
    """
    Circuit breakers for external dependencies
    
    Prevents cascading failures when dependencies fail
    """
    
    def __init__(self):
        # Milvus circuit breaker
        self.milvus_breaker = CircuitBreaker(
            fail_max=5,              # Open after 5 consecutive failures
            reset_timeout=60,        # Try again after 60 seconds
            exclude=[ValueError],    # Don't count validation errors
            listeners=[self._on_circuit_open, self._on_circuit_close]
        )
        
        # Neo4j circuit breaker
        self.neo4j_breaker = CircuitBreaker(
            fail_max=5,
            reset_timeout=60,
            listeners=[self._on_circuit_open, self._on_circuit_close]
        )
        
        # OpenAI circuit breaker (more lenient)
        self.openai_breaker = CircuitBreaker(
            fail_max=3,              # Fail faster for external API
            reset_timeout=30,
            listeners=[self._on_circuit_open, self._on_circuit_close]
        )
    
    def _on_circuit_open(self, cb, exc):
        """Called when circuit opens (service failing)"""
        service = cb.name
        logger.error(
            f"Circuit breaker OPEN for {service}",
            extra={
                "service": service,
                "exception": str(exc),
                "fail_counter": cb.fail_counter
            }
        )
        
        # Update metric
        DEPENDENCY_HEALTH.labels(service=service).set(0)
        
        # Alert ops team
        alert_ops(f"Circuit breaker OPEN: {service}")
    
    def _on_circuit_close(self, cb):
        """Called when circuit closes (service recovered)"""
        service = cb.name
        logger.info(f"Circuit breaker CLOSED for {service}")
        DEPENDENCY_HEALTH.labels(service=service).set(1)

# Usage in repository layer
class MilvusRepository:
    def __init__(self, circuit_breakers: ServiceCircuitBreakers):
        self.breaker = circuit_breakers.milvus_breaker
    
    @circuit_breakers.milvus_breaker
    async def search(self, query_vector: list[float], **kwargs):
        """
        Search with circuit breaker protection
        
        If circuit is OPEN (Milvus down), raises CircuitBreakerError
        Calling code can catch and use fallback strategy
        """
        try:
            return await self.client.search(
                collection_name="user_facts",
                data=[query_vector],
                **kwargs
            )
        except Exception as e:
            logger.error(f"Milvus search failed: {e}")
            raise  # Re-raise to increment circuit breaker counter

# Usage in service layer with fallback
class FactSearcherService:
    async def search_facts(self, user_id: str, query: str):
        try:
            # Try primary path (Milvus)
            results = await self.milvus.search(query_vector)
            
        except CircuitBreakerError:
            # Circuit is OPEN - use fallback
            logger.warning("Milvus circuit open, using fallback")
            results = await self._search_fallback(user_id, query)
        
        return results
    
    async def _search_fallback(self, user_id: str, query: str):
        """
        Fallback strategy when Milvus is down
        
        Options:
        1. Return cached results (stale but better than nothing)
        2. Search in PostgreSQL (slower but works)
        3. Return empty results with explanation
        """
        # Option 1: Check cache
        cached = await self.redis.get(f"last_search:{user_id}:{query}")
        if cached:
            return json.loads(cached)
        
        # Option 2: Search in PostgreSQL (keyword search)
        results = await self.postgres.fetch(
            """
            SELECT * FROM facts_metadata
            WHERE user_id = $1 
            AND content ILIKE $2
            LIMIT 20
            """,
            user_id,
            f"%{query}%"
        )
        
        return results
```

## 10.3 Retry Strategy v·ªõi Exponential Backoff

```python
# app/resilience/retry.py
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log
)
import logging

logger = logging.getLogger(__name__)

# Retryable exceptions (transient errors)
RETRYABLE_EXCEPTIONS = (
    ConnectionError,
    TimeoutError,
    OSError,  # Network errors
)

# Non-retryable exceptions (permanent errors)
NON_RETRYABLE_EXCEPTIONS = (
    ValueError,          # Bad input
    KeyError,           # Missing data
    PermissionError,    # Auth error
)

class RetryConfig:
    """Retry configurations for different services"""
    
    OPENAI_API = {
        "stop": stop_after_attempt(3),
        "wait": wait_exponential(multiplier=1, min=1, max=10),
        "retry": retry_if_exception_type(RETRYABLE_EXCEPTIONS),
        "before_sleep": before_sleep_log(logger, logging.WARNING)
    }
    
    MILVUS = {
        "stop": stop_after_attempt(3),
        "wait": wait_exponential(multiplier=1, min=2, max=30),
        "retry": retry_if_exception_type(RETRYABLE_EXCEPTIONS)
    }
    
    NEO4J = {
        "stop": stop_after_attempt(3),
        "wait": wait_exponential(multiplier=1, min=2, max=30),
        "retry": retry_if_exception_type(RETRYABLE_EXCEPTIONS)
    }

# Usage example
class OpenAIEmbedder:
    
    @retry(**RetryConfig.OPENAI_API)
    async def embed(self, text: str) -> list[float]:
        """
        Generate embedding with retry
        
        Retry schedule:
        - Attempt 1: Immediate
        - Attempt 2: Wait 1s (2^0)
        - Attempt 3: Wait 2s (2^1)
        - Attempt 4: Wait 4s (2^2)
        - Give up after 3 retries
        """
        response = await self.client.embeddings.create(
            model="text-embedding-3-small",
            input=text,
            timeout=10.0  # 10 second timeout
        )
        return response.data[0].embedding

# Custom retry decorator with jitter
def retry_with_jitter(
    max_attempts: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 30.0,
    jitter: bool = True
):
    """
    Retry decorator with exponential backoff and jitter
    
    Jitter prevents thundering herd problem
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return await func(*args, **kwargs)
                    
                except RETRYABLE_EXCEPTIONS as e:
                    if attempt == max_attempts - 1:
                        # Last attempt failed, re-raise
                        raise
                    
                    # Calculate delay with exponential backoff
                    delay = min(base_delay * (2 ** attempt), max_delay)
                    
                    # Add jitter (random ¬±25%)
                    if jitter:
                        jitter_amount = delay * 0.25
                        delay += random.uniform(-jitter_amount, jitter_amount)
                    
                    logger.warning(
                        f"Attempt {attempt + 1}/{max_attempts} failed, "
                        f"retrying in {delay:.2f}s",
                        extra={
                            "function": func.__name__,
                            "exception": str(e),
                            "attempt": attempt + 1
                        }
                    )
                    
                    await asyncio.sleep(delay)
        
        return wrapper
    return decorator
```

## 10.4 Timeout Configuration

```python
# app/config/timeouts.py
from dataclasses import dataclass

@dataclass
class ServiceTimeout:
    """Timeout configuration for a service"""
    connect: float  # Connection timeout
    read: float     # Read timeout
    write: float    # Write timeout
    pool: float     # Connection pool timeout

class TimeoutConfig:
    """Centralized timeout configuration"""
    
    # OpenAI API (external, can be slow)
    OPENAI = ServiceTimeout(
        connect=5.0,   # 5s to establish connection
        read=30.0,     # 30s to get response (LLM can be slow)
        write=10.0,    # 10s to send request
        pool=30.0      # 30s to get connection from pool
    )
    
    # Milvus (local network, should be fast)
    MILVUS = ServiceTimeout(
        connect=2.0,   # 2s to connect
        read=5.0,      # 5s to get search results
        write=5.0,     # 5s to insert vectors
        pool=10.0      # 10s pool timeout
    )
    
    # Neo4j (local network)
    NEO4J = ServiceTimeout(
        connect=2.0,
        read=5.0,
        write=5.0,
        pool=10.0
    )
    
    # PostgreSQL (local network)
    POSTGRES = ServiceTimeout(
        connect=2.0,
        read=5.0,
        write=5.0,
        pool=10.0
    )
    
    # Redis (in-memory, should be very fast)
    REDIS = ServiceTimeout(
        connect=0.5,
        read=1.0,
        write=1.0,
        pool=2.0
    )

# Usage with httpx
import httpx

openai_client = httpx.AsyncClient(
    timeout=httpx.Timeout(
        connect=TimeoutConfig.OPENAI.connect,
        read=TimeoutConfig.OPENAI.read,
        write=TimeoutConfig.OPENAI.write,
        pool=TimeoutConfig.OPENAI.pool
    )
)

# Usage with asyncpg (PostgreSQL)
postgres_pool = await asyncpg.create_pool(
    dsn=DATABASE_URL,
    min_size=10,
    max_size=50,
    command_timeout=TimeoutConfig.POSTGRES.read,
    timeout=TimeoutConfig.POSTGRES.connect
)
```

## 10.5 Graceful Degradation

```python
# app/services/fact_searcher.py
class FactSearcherService:
    """
    Search service v·ªõi graceful degradation
    
    Degradation levels:
    1. Full: Milvus + Neo4j + PostgreSQL (optimal)
    2. Degraded: PostgreSQL only (slower, no semantic)
    3. Cached: Return stale cached results
    4. Empty: Return empty with explanation
    """
    
    async def search_facts(
        self,
        user_id: str,
        query: str,
        limit: int = 20
    ) -> SearchResult:
        """Search with automatic degradation"""
        
        # Level 1: Try full pipeline (optimal)
        try:
            return await self._search_full_pipeline(
                user_id, query, limit
            )
        except Exception as e:
            logger.warning(f"Full pipeline failed: {e}")
        
        # Level 2: Try degraded mode (PostgreSQL only)
        try:
            return await self._search_degraded_mode(
                user_id, query, limit
            )
        except Exception as e:
            logger.error(f"Degraded mode failed: {e}")
        
        # Level 3: Try cached results
        try:
            cached = await self._get_cached_results(user_id, query)
            if cached:
                return SearchResult(
                    results=cached,
                    degraded=True,
                    source="cache",
                    warning="Results may be stale (from cache)"
                )
        except Exception as e:
            logger.error(f"Cache retrieval failed: {e}")
        
        # Level 4: Return empty with explanation
        return SearchResult(
            results=[],
            degraded=True,
            source="none",
            error="All search methods unavailable. Please try again later."
        )
    
    async def _search_full_pipeline(
        self,
        user_id: str,
        query: str,
        limit: int
    ) -> SearchResult:
        """Full pipeline: Milvus + Neo4j enrichment"""
        
        # 1. Embed query
        query_vector = await self.embedder.embed(query)
        
        # 2. Search in Milvus
        similar_facts = await self.milvus.search(
            user_id=user_id,
            query_vector=query_vector,
            top_k=limit * 2
        )
        
        # 3. Enrich with Neo4j relationships
        enriched = await self.neo4j.enrich_with_relationships(
            similar_facts
        )
        
        # 4. Re-rank and return
        ranked = self._rerank(enriched, limit)
        
        return SearchResult(
            results=ranked,
            degraded=False,
            source="full_pipeline"
        )
    
    async def _search_degraded_mode(
        self,
        user_id: str,
        query: str,
        limit: int
    ) -> SearchResult:
        """Degraded mode: PostgreSQL keyword search only"""
        
        # Simple keyword search (no semantic matching)
        results = await self.postgres.fetch(
            """
            SELECT 
                fact_id,
                content,
                category,
                confidence,
                created_at,
                ts_rank(
                    to_tsvector('english', content),
                    plainto_tsquery('english', $2)
                ) as rank
            FROM facts_metadata
            WHERE user_id = $1
            AND to_tsvector('english', content) @@ plainto_tsquery('english', $2)
            ORDER BY rank DESC, created_at DESC
            LIMIT $3
            """,
            user_id,
            query,
            limit
        )
        
        return SearchResult(
            results=results,
            degraded=True,
            source="postgres_fallback",
            warning="Semantic search unavailable, using keyword search"
        )
```

## 10.6 Health Checks

```python
# app/api/health.py
from fastapi import APIRouter
from typing import Dict

router = APIRouter(prefix="/health", tags=["health"])

class HealthChecker:
    """Comprehensive health checks for all dependencies"""
    
    async def check_all(self) -> Dict:
        """
        Check all dependencies
        
        Returns:
            {
                "status": "healthy|degraded|unhealthy",
                "timestamp": "ISO datetime",
                "checks": {
                    "milvus": {...},
                    "neo4j": {...},
                    ...
                }
            }
        """
        checks = await asyncio.gather(
            self.check_milvus(),
            self.check_neo4j(),
            self.check_postgres(),
            self.check_redis(),
            self.check_openai(),
            return_exceptions=True
        )
        
        # Determine overall status
        unhealthy_count = sum(
            1 for c in checks 
            if isinstance(c, dict) and c.get("status") == "unhealthy"
        )
        
        if unhealthy_count == 0:
            overall_status = "healthy"
        elif unhealthy_count <= 2:
            overall_status = "degraded"
        else:
            overall_status = "unhealthy"
        
        return {
            "status": overall_status,
            "timestamp": datetime.utcnow().isoformat(),
            "checks": {
                "milvus": checks[0],
                "neo4j": checks[1],
                "postgres": checks[2],
                "redis": checks[3],
                "openai": checks[4]
            }
        }
    
    async def check_milvus(self) -> Dict:
        """Check Milvus vector store"""
        start = time.time()
        try:
            # Simple query to check connection
            await milvus_client.list_collections()
            
            return {
                "status": "healthy",
                "latency_ms": int((time.time() - start) * 1000),
                "message": "Milvus responsive"
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "latency_ms": int((time.time() - start) * 1000),
                "error": str(e)
            }
    
    async def check_neo4j(self) -> Dict:
        """Check Neo4j graph store"""
        start = time.time()
        try:
            async with neo4j_driver.session() as session:
                result = await session.run("RETURN 1 as num")
                await result.single()
            
            return {
                "status": "healthy",
                "latency_ms": int((time.time() - start) * 1000),
                "message": "Neo4j responsive"
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "latency_ms": int((time.time() - start) * 1000),
                "error": str(e)
            }
    
    async def check_postgres(self) -> Dict:
        """Check PostgreSQL database"""
        start = time.time()
        try:
            await postgres_pool.fetchval("SELECT 1")
            
            # Check connection pool
            pool_status = postgres_pool.get_size()
            
            return {
                "status": "healthy",
                "latency_ms": int((time.time() - start) * 1000),
                "message": "PostgreSQL responsive",
                "pool_size": pool_status
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "latency_ms": int((time.time() - start) * 1000),
                "error": str(e)
            }

@router.get("/live")
async def liveness():
    """
    Kubernetes liveness probe
    
    Returns 200 if application is alive (should restart if fails)
    """
    return {"status": "alive"}

@router.get("/ready")
async def readiness(health_checker: HealthChecker = Depends()):
    """
    Kubernetes readiness probe
    
    Returns 200 if ready to accept traffic
    Returns 503 if not ready (remove from load balancer)
    """
    health = await health_checker.check_all()
    
    if health["status"] == "unhealthy":
        raise HTTPException(
            status_code=503,
            detail="Service not ready"
        )
    
    return health

@router.get("")
async def health_check(health_checker: HealthChecker = Depends()):
    """Detailed health check for monitoring"""
    return await health_checker.check_all()
```

---

**Ti·∫øp t·ª•c v·ªõi Section 14: NFR v√† Section 18: Runbooks trong response ti·∫øp theo...**

B·∫°n mu·ªën m√¨nh ti·∫øp t·ª•c ngay v·ªõi 2 sections c√≤n l·∫°i kh√¥ng? Hay review l·∫°i 2 sections v·ª´a vi·∫øt tr∆∞·ªõc?

# üìù B·ªî SUNG 2 SECTIONS CU·ªêI C√ôNG

---

# 14. Y√äU C·∫¶U PHI CH·ª®C NƒÇNG (NON-FUNCTIONAL REQUIREMENTS - NFR)

## 14.1 Performance Requirements

### 14.1.1 Response Time Requirements

|API Endpoint|P50|P95|P99|Max|Measurement Tool|
|---|---|---|---|---|---|
|**POST /extract_facts**|<150ms|<300ms|<800ms|2000ms|Datadog APM|
|**POST /search_facts**|<80ms|<200ms|<500ms|1000ms|Datadog APM|
|**GET /health**|<10ms|<20ms|<50ms|100ms|Prometheus|

### 14.1.2 Throughput Requirements

```python
# Load test targets
THROUGHPUT_REQUIREMENTS = {
    "sustained_load": {
        "rps": 200,
        "duration": "30 minutes",
        "success_rate": ">99.9%",
        "p95_latency": "<200ms"
    },
    
    "peak_load": {
        "rps": 500,
        "duration": "5 minutes",
        "success_rate": ">99%",
        "p95_latency": "<300ms"
    },
    
    "burst_load": {
        "rps": 1000,
        "duration": "1 minute",
        "success_rate": ">95%",
        "p95_latency": "<500ms"
    }
}

# Concurrent users
CONCURRENT_USERS = {
    "typical": 5000,      # Normal usage
    "peak": 10000,        # Peak hours
    "max": 100000         # System capacity
}
```

### 14.1.3 Database Performance Requirements

```yaml
# PostgreSQL
postgres_requirements:
  connection_pool:
    min: 10
    max: 50
    timeout: 10s
  
  query_performance:
    simple_select: <5ms
    complex_join: <50ms
    aggregation: <100ms
  
  transactions_per_second: 500+
  
  indexes:
    - idx_facts_user_id (user_id)
    - idx_facts_created_at (created_at DESC)
    - idx_facts_category (category)
    - idx_conversations_user_id (user_id)

# Milvus
milvus_requirements:
  vector_search:
    latency_p95: <100ms
    throughput: 1000+ qps
    top_k: 20 (default)
  
  insert_performance:
    batch_size: 1000
    latency: <200ms per batch
  
  collection_size:
    initial: 1M vectors
    max: 100M vectors

# Neo4j
neo4j_requirements:
  query_performance:
    simple_match: <10ms
    2_hop_traversal: <50ms
    3_hop_traversal: <100ms
  
  concurrent_transactions: 100+
  
  memory:
    heap: 4GB minimum
    page_cache: 8GB minimum
```

## 14.2 Scalability Requirements

### 14.2.1 Horizontal Scaling

```python
# Auto-scaling configuration
AUTOSCALING_CONFIG = {
    "api_pods": {
        "min_replicas": 3,
        "max_replicas": 50,
        "target_cpu": 70,        # Scale at 70% CPU
        "target_memory": 80,      # Scale at 80% memory
        "scale_up_policy": {
            "stabilization_window": 60,   # Wait 60s before scaling up
            "percent_increase": 100       # Double pods on scale up
        },
        "scale_down_policy": {
            "stabilization_window": 300,  # Wait 5min before scaling down
            "percent_decrease": 10        # Remove 10% pods on scale down
        }
    },
    
    "custom_metrics": {
        "requests_per_pod": 100,  # Scale when >100 rps per pod
        "queue_depth": 50         # Scale when queue >50 messages
    }
}

# Kubernetes HPA manifest
"""
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: pika-mem0-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: pika-mem0-api
  minReplicas: 3
  maxReplicas: 50
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
"""
```

### 14.2.2 Data Growth Projection

```python
# Capacity planning over time
DATA_GROWTH_PROJECTION = {
    "baseline": {
        "users": 10000,
        "conversations_per_user_per_month": 10,
        "facts_per_conversation": 5,
        "total_facts_per_month": 500000,  # 10K √ó 10 √ó 5
        "avg_fact_size": "7KB",  # 1KB text + 6KB vector
        "storage_per_month": "3.5GB"
    },
    
    "6_months": {
        "users": 50000,
        "total_facts": 15000000,
        "storage_needed": "105GB",
        "milvus_memory": "20GB",
        "postgres_storage": "30GB"
    },
    
    "1_year": {
        "users": 150000,
        "total_facts": 90000000,
        "storage_needed": "630GB",
        "milvus_memory": "100GB",
        "postgres_storage": "150GB",
        "recommended_action": "Partition tables by month"
    },
    
    "2_years": {
        "users": 500000,
        "total_facts": 600000000,
        "storage_needed": "4.2TB",
        "milvus_memory": "500GB",
        "postgres_storage": "1TB",
        "recommended_action": "Shard Milvus, PostgreSQL read replicas"
    }
}
```

## 14.3 Availability & Reliability Requirements

### 14.3.1 Uptime SLA

```yaml
sla_targets:
  availability:
    target: 99.9%
    calculation_period: monthly
    allowed_downtime:
      per_month: 43.8 minutes
      per_year: 8.76 hours
    
  measurement:
    method: "synthetic_monitoring"
    probe_interval: 60s
    probe_locations:
      - us-east-1
      - ap-southeast-1
    
  exclusions:
    - scheduled_maintenance (with 7 days notice)
    - force_majeure events
    - third_party_failures (OpenAI API down)

error_budget:
  availability_slo: 99.9%
  error_budget: 0.1%  # 1 in 1000 requests can fail
  
  burn_rate_alerts:
    critical: "Consuming >10% budget in 1 hour"
    warning: "Consuming >5% budget in 6 hours"
  
  policy_when_budget_exhausted:
    - halt_non_critical_deployments
    - focus_on_reliability_improvements
    - increase_monitoring
```

### 14.3.2 Recovery Objectives

```python
RECOVERY_OBJECTIVES = {
    "RPO": {
        "value": "15 minutes",
        "description": "Recovery Point Objective - Maximum data loss acceptable",
        "implementation": [
            "PostgreSQL continuous archiving (WAL)",
            "Backup every 15 minutes to S3",
            "Milvus snapshots every 1 hour"
        ]
    },
    
    "RTO": {
        "value": "1 hour",
        "description": "Recovery Time Objective - Time to restore service",
        "implementation": [
            "Automated failover to standby",
            "Blue-green deployment for rollback",
            "Documented runbooks"
        ]
    },
    
    "MTBF": {
        "value": "720 hours (30 days)",
        "description": "Mean Time Between Failures",
        "target": "> 1 month without incidents"
    },
    
    "MTTR": {
        "value": "30 minutes",
        "description": "Mean Time To Repair",
        "target": "< 30 min from alert to resolution"
    }
}

# Backup strategy
BACKUP_STRATEGY = {
    "databases": {
        "postgresql": {
            "method": "continuous_archiving",
            "frequency": "15 minutes (WAL)",
            "full_backup": "daily at 02:00 UTC",
            "retention": "35 days",
            "storage": "AWS S3",
            "encryption": "AES-256"
        },
        
        "milvus": {
            "method": "snapshots",
            "frequency": "1 hour",
            "retention": "7 days",
            "storage": "S3"
        }
    },
    
    "testing": {
        "restore_test": "monthly",
        "dr_drill": "quarterly",
        "documentation": "runbook updated after each drill"
    }
}
```

## 14.4 Security Requirements

```yaml
security_requirements:
  authentication:
    method: "API Key"
    key_rotation: "90 days"
    key_storage: "hashed with SHA-256"
    
  authorization:
    model: "RBAC"
    principle: "Least privilege"
    
  encryption:
    at_rest:
      algorithm: "AES-256-GCM"
      key_management: "AWS KMS / Environment variables"
      scope: "Sensitive PII data"
    
    in_transit:
      protocol: "TLS 1.3"
      certificate: "Let's Encrypt (auto-renewal)"
      scope: "All communications"
  
  input_validation:
    framework: "Pydantic"
    sanitization: "HTML escape, SQL parameterization"
    max_input_size: "5MB"
  
  rate_limiting:
    default: "1000 requests/minute per API key"
    burst: "100 requests/second"
    
  audit_logging:
    scope: "All data access and modifications"
    retention: "7 years (compliance)"
    storage: "Immutable append-only"
  
  compliance:
    - GDPR (data privacy)
    - COPPA (children's data protection)
```

## 14.5 Maintainability Requirements

```python
MAINTAINABILITY_REQUIREMENTS = {
    "code_quality": {
        "test_coverage": {
            "unit": ">80%",
            "integration": ">60%",
            "tool": "pytest-cov"
        },
        
        "code_complexity": {
            "cyclomatic_complexity": "<10 per function",
            "max_function_length": "50 lines",
            "tool": "radon, pylint"
        },
        
        "code_style": {
            "formatter": "black",
            "linter": "ruff",
            "type_checker": "mypy"
        },
        
        "documentation": {
            "docstrings": "100% public APIs (Google style)",
            "api_docs": "OpenAPI 3.0 spec",
            "architecture_docs": "C4 diagrams"
        }
    },
    
    "tech_debt": {
        "ratio": "<5%",
        "tool": "SonarQube",
        "review_frequency": "monthly"
    },
    
    "dependency_management": {
        "update_frequency": "monthly",
        "security_scan": "on every PR (Snyk, Safety)",
        "lock_file": "requirements.txt + pip-tools"
    },
    
    "monitoring": {
        "dashboard": "Grafana with key metrics",
        "alerts": "PagerDuty integration",
        "logs": "Structured JSON to stdout"
    }
}

# Pre-commit hooks
"""
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.12.0
    hooks:
      - id: black
        
  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.1.9
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
        
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]
        
  - repo: https://github.com/PyCQA/bandit
    rev: 1.7.6
    hooks:
      - id: bandit
        args: [-c, .bandit.yaml]
        
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: check-yaml
      - id: check-json
      - id: end-of-file-fixer
      - id: trailing-whitespace
"""
```

## 14.6 Usability Requirements (API Design)

```python
API_USABILITY_REQUIREMENTS = {
    "consistency": {
        "naming": "snake_case for JSON fields",
        "timestamps": "ISO 8601 format (YYYY-MM-DDTHH:MM:SSZ)",
        "errors": "RFC 7807 Problem Details",
        "pagination": "Cursor-based for large datasets"
    },
    
    "error_handling": {
        "format": {
            "status": "error",
            "error": {
                "code": "ERROR_CODE",
                "message": "Human-readable message",
                "details": {},
                "trace_id": "abc123",
                "documentation_url": "https://docs.pika.ai/errors/ERROR_CODE"
            }
        },
        
        "http_status_codes": {
            "400": "Invalid request (validation errors)",
            "401": "Unauthorized (missing/invalid API key)",
            "403": "Forbidden (insufficient permissions)",
            "404": "Resource not found",
            "429": "Rate limit exceeded",
            "500": "Internal server error",
            "503": "Service unavailable"
        }
    },
    
    "versioning": {
        "strategy": "URL path versioning (/v1/, /v2/)",
        "deprecation_policy": "6 months notice",
        "sunset_header": "Sunset: Sat, 31 Dec 2025 23:59:59 GMT"
    },
    
    "documentation": {
        "openapi_spec": "100% coverage of endpoints",
        "examples": "Request/response examples for each endpoint",
        "postman_collection": "Importable collection",
        "interactive_docs": "Swagger UI at /docs"
    }
}
```

---

# 18. INCIDENT RESPONSE & RUNBOOKS

## 18.1 Incident Severity Levels

```python
INCIDENT_SEVERITY = {
    "SEV1": {
        "definition": "Complete service outage or data loss",
        "examples": [
            "API returning 500 for all requests",
            "Database corruption",
            "Security breach",
            "Data loss incident"
        ],
        "response_time": "5 minutes",
        "resolution_time": "1 hour",
        "notification": [
            "Page on-call engineer immediately",
            "Alert engineering manager",
            "Notify VP Engineering"
        ],
        "communication": "Status page update every 15 minutes"
    },
    
    "SEV2": {
        "definition": "Major feature broken, affecting many users",
        "examples": [
            "Extract facts API down",
            "Search returning empty results",
            "Latency >1 second (10x normal)"
        ],
        "response_time": "15 minutes",
        "resolution_time": "4 hours",
        "notification": [
            "Page on-call engineer",
            "Notify engineering manager"
        ],
        "communication": "Slack update every 30 minutes"
    },
    
    "SEV3": {
        "definition": "Minor feature degraded or performance issue",
        "examples": [
            "Latency 300-500ms (2x normal)",
            "Non-critical API endpoint slow",
            "Cache miss rate high"
        ],
        "response_time": "1 hour",
        "resolution_time": "24 hours",
        "notification": [
            "Notify on-call via Slack",
            "Create Jira ticket"
        ],
        "communication": "Daily standup update"
    },
    
    "SEV4": {
        "definition": "Cosmetic issue or minor bug",
        "examples": [
            "Typo in error message",
            "Non-critical log message incorrect",
            "Documentation outdated"
        ],
        "response_time": "4 hours",
        "resolution_time": "1 week",
        "notification": "Create Jira ticket",
        "communication": "Sprint review"
    }
}
```

## 18.2 On-Call Procedures

### 18.2.1 On-Call Schedule

```yaml
on_call_schedule:
  rotation: weekly
  handoff_time: "09:00 Monday (Hanoi time)"
  
  primary:
    - week_1: "Backend Engineer A"
    - week_2: "Backend Engineer B"
    - week_3: "Backend Engineer C"
    - week_4: "Backend Lead (SU)"
  
  secondary:
    - always: "Backend Lead (SU)"
  
  escalation:
    - level_1: "Primary on-call" (0-5 minutes)
    - level_2: "Secondary on-call" (5-15 minutes)
    - level_3: "Engineering Manager" (15-30 minutes)
    - level_4: "VP Engineering" (30+ minutes, SEV1 only)
  
  tools:
    alerting: "PagerDuty"
    communication: "Slack #incidents"
    incident_tracking: "Jira"
    status_page: "https://status.pika.stepup.edu.vn"
```

### 18.2.2 On-Call Handoff Checklist

```markdown
# On-Call Handoff Checklist

## Outgoing Engineer (End of Shift)
- [ ] Review all open incidents from past week
- [ ] Document any unresolved issues in Jira
- [ ] Update runbooks with any new learnings
- [ ] Check monitoring dashboards (all green?)
- [ ] Review recent deployments (any issues?)
- [ ] Prepare handoff notes for incoming engineer

## Incoming Engineer (Start of Shift)
- [ ] Verify PagerDuty notifications working (test alert)
- [ ] Verify access to all systems:
  - [ ] Kubernetes cluster (kubectl access)
  - [ ] PostgreSQL database (read/write access)
  - [ ] Milvus dashboard
  - [ ] Neo4j browser
  - [ ] Datadog dashboards
  - [ ] AWS Console
- [ ] Read handoff notes from outgoing engineer
- [ ] Review open incidents in Jira
- [ ] Check monitoring dashboards:
  - [ ] API error rate
  - [ ] API latency (P95, P99)
  - [ ] Database connections
  - [ ] Milvus health
  - [ ] Neo4j health
- [ ] Update personal contact info in PagerDuty
- [ ] Join Slack #incidents channel
```

## 18.3 Runbooks

### 18.3.1 Runbook: High Error Rate Alert

````markdown
# Runbook: High Error Rate (>1% for 5 minutes)

## Alert Details
- **Alert Name**: HighErrorRate
- **Severity**: SEV2
- **Trigger**: Error rate >1% for 5 minutes
- **PagerDuty**: Automatic page to on-call

## Symptoms
- Users reporting "Internal Server Error" (500)
- Error rate spike in Datadog dashboard
- Increased error logs in stdout

## Diagnosis Steps

### Step 1: Identify Affected Endpoint (2 minutes)
```bash
# Check which endpoint has high errors
kubectl logs -n production deployment/pika-mem0-api --tail=100 | grep "ERROR"

# Or use Datadog query
# Group errors by endpoint path
````

### Step 2: Check Service Health (2 minutes)

```bash
# Check pod status
kubectl get pods -n production -l app=pika-mem0-api

# Check pod resource usage
kubectl top pods -n production -l app=pika-mem0-api

# Check recent pod restarts
kubectl get pods -n production -l app=pika-mem0-api -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.containerStatuses[0].restartCount}{"\n"}{end}'
```

### Step 3: Check Dependencies (3 minutes)

```bash
# Check Milvus
curl -f http://124.197.21.40:8000/_health || echo "Milvus DOWN"

# Check Neo4j
curl -u neo4j:mem0graph http://124.197.21.40:8474/db/neo4j/tx/commit \
  -H "Content-Type: application/json" \
  -d '{"statements":[{"statement":"RETURN 1"}]}' || echo "Neo4j DOWN"

# Check PostgreSQL
kubectl exec -n production deployment/pika-mem0-api -- \
  python -c "import asyncpg; asyncpg.connect('$DATABASE_URL')" || echo "PostgreSQL DOWN"

# Check Redis
kubectl exec -n production deployment/pika-mem0-api -- \
  redis-cli -h redis ping || echo "Redis DOWN"
```

### Step 4: Check Recent Changes (2 minutes)

```bash
# Check recent deployments
kubectl rollout history deployment/pika-mem0-api -n production

# Check recent config changes
git log --oneline -10 -- k8s/production/

# Check if there's a recent code deploy
kubectl describe deployment/pika-mem0-api -n production | grep Image:
```

## Resolution Steps

### If: Bad Deployment (Most Common)

```bash
# IMMEDIATE: Rollback to previous version
kubectl rollout undo deployment/pika-mem0-api -n production

# Verify rollback successful
kubectl rollout status deployment/pika-mem0-api -n production

# Check error rate after 2 minutes
# Expected: Error rate back to <0.1%
```

### If: Database Connection Pool Exhausted

```bash
# Check connection count
kubectl exec -n production deployment/pika-mem0-api -- \
  python -c "
import asyncpg
conn = asyncpg.connect('$DATABASE_URL')
result = conn.fetchval('SELECT count(*) FROM pg_stat_activity')
print(f'Active connections: {result}')
"

# If > 80% of max_connections (default 100):
# Restart pods to reset connections
kubectl rollout restart deployment/pika-mem0-api -n production
```

### If: Milvus Down

```bash
# Check Milvus logs on server
ssh user@124.197.21.40
docker logs milvus-standalone --tail=100

# If Milvus unresponsive:
# System will use fallback (PostgreSQL search)
# Contact DevOps to restart Milvus

# Temporary: Scale up API pods to handle fallback load
kubectl scale deployment/pika-mem0-api --replicas=10 -n production
```

### If: Memory Leak (OOM)

```bash
# Check memory usage
kubectl top pods -n production -l app=pika-mem0-api

# If pods using >90% memory:
# Restart pods in rolling fashion
kubectl rollout restart deployment/pika-mem0-api -n production

# Scale up resources temporarily
kubectl set resources deployment/pika-mem0-api \
  --limits=memory=4Gi \
  --requests=memory=2Gi \
  -n production
```

## Escalation Path

1. **0-15 minutes**: Primary on-call attempts resolution
2. **15-30 minutes**: If not resolved, escalate to Backend Lead (SU)
3. **30+ minutes**: If still not resolved, escalate to Engineering Manager
4. **If data loss suspected**: Immediately escalate to VP Engineering (SEV1)

## Communication

- Update Slack #incidents every 15 minutes
- Update status page if customer-facing impact
- Create incident post-mortem after resolution

## Post-Resolution

- [ ] Create Jira ticket for root cause analysis
- [ ] Update runbook with any new findings
- [ ] Schedule post-mortem meeting within 48 hours
- [ ] Document lessons learned

````

### 18.3.2 Runbook: High Latency Alert

```markdown
# Runbook: High Latency (P95 >500ms for 10 minutes)

## Alert Details
- **Severity**: SEV3
- **Trigger**: P95 latency >500ms for 10 minutes

## Diagnosis Steps

### Step 1: Identify Slow Endpoint
```bash
# Check Datadog APM
# Sort endpoints by P95 latency
# Identify which operation is slow:
# - /extract_facts (expected ~300ms)
# - /search_facts (expected ~200ms)
````

### Step 2: Check Database Performance

```sql
-- Check slow queries in PostgreSQL
SELECT 
    query,
    calls,
    mean_exec_time,
    max_exec_time
FROM pg_stat_statements
WHERE mean_exec_time > 100  -- >100ms average
ORDER BY mean_exec_time DESC
LIMIT 10;

-- Check for locks
SELECT 
    pid,
    usename,
    pg_blocking_pids(pid) as blocked_by,
    query
FROM pg_stat_activity
WHERE cardinality(pg_blocking_pids(pid)) > 0;
```

### Step 3: Check Milvus Performance

```bash
# Check Milvus query latency
curl http://124.197.21.40:8000/metrics | grep milvus_search_latency

# Check Milvus CPU/memory on server
ssh user@124.197.21.40
docker stats milvus-standalone
```

### Step 4: Check OpenAI API

```bash
# Check OpenAI API status
curl https://status.openai.com/api/v2/summary.json

# Check our OpenAI API latency in logs
kubectl logs -n production deployment/pika-mem0-api --tail=500 | \
  grep "openai_api_latency" | \
  awk '{print $NF}' | \
  sort -n | \
  tail -20
```

## Resolution Steps

### If: Slow Database Query

```sql
-- Add missing index
CREATE INDEX CONCURRENTLY idx_facts_user_created 
ON facts_metadata(user_id, created_at DESC);

-- Or optimize query
EXPLAIN ANALYZE <slow_query>;
```

### If: Milvus Overloaded

```bash
# Reduce search top_k temporarily
# Edit config to reduce from 40 to 20

# Restart Milvus to clear cache
ssh user@124.197.21.40
docker restart milvus-standalone
```

### If: OpenAI API Slow

```python
# Enable caching for embeddings
# Most queries are repeated, cache can help

# Increase timeout tolerance
OPENAI_TIMEOUT = httpx.Timeout(
    connect=5.0,
    read=60.0,  # Increase from 30s to 60s
)
```

````

### 18.3.3 Runbook: Service Completely Down

```markdown
# Runbook: Complete Service Outage (SEV1)

## Alert: All Health Checks Failing

## IMMEDIATE ACTIONS (First 5 Minutes)

### Step 1: Verify It's Actually Down (1 minute)
```bash
# Test from external monitoring
curl https://api-memory.pika.stepup.edu.vn/health

# Test from inside cluster
kubectl exec -n production deployment/pika-mem0-api -- curl localhost:8000/health
````

### Step 2: Check Cluster Status (2 minutes)

```bash
# Check if pods are running
kubectl get pods -n production -l app=pika-mem0-api

# Common states:
# - CrashLoopBackOff: Application crashing on start
# - ImagePullBackOff: Can't pull Docker image
# - Pending: Can't schedule (resources?)
# - Error: Pod failed

# Check events
kubectl get events -n production --sort-by='.lastTimestamp' | tail -20
```

### Step 3: Check Logs (2 minutes)

```bash
# Get logs from failing pods
kubectl logs -n production -l app=pika-mem0-api --tail=100

# Look for:
# - Connection refused (database down?)
# - Out of memory (OOMKilled)
# - Import errors (bad deployment)
# - Permission denied (security issue)
```

## RESOLUTION STEPS

### Scenario A: Pods CrashLoopBackOff (Application Error)

```bash
# IMMEDIATE: Rollback to last known good version
kubectl rollout undo deployment/pika-mem0-api -n production

# Monitor rollback
kubectl rollout status deployment/pika-mem0-api -n production

# Expected time to recovery: 2-3 minutes
```

### Scenario B: All Dependencies Down

```bash
# Check Milvus
ssh user@124.197.21.40 "docker ps | grep milvus"

# Check Neo4j
ssh user@124.197.21.40 "docker ps | grep neo4j"

# If down, restart:
ssh user@124.197.21.40 "docker restart milvus-standalone neo4j"

# Wait 1 minute for restart, then restart API pods
kubectl rollout restart deployment/pika-mem0-api -n production
```

### Scenario C: Cluster Resource Exhaustion

```bash
# Check node resources
kubectl top nodes

# If nodes at 100% CPU/memory:
# Scale down non-critical workloads
kubectl scale deployment/non-critical-app --replicas=0 -n production

# Or request new nodes (if cloud)
# Contact DevOps for emergency node provisioning
```

### Scenario D: Network Issue

```bash
# Check ingress controller
kubectl get ingress -n production
kubectl get svc -n production

# Check if LoadBalancer has external IP
kubectl get svc pika-mem0-api -n production -o jsonpath='{.status.loadBalancer.ingress[0].ip}'

# If missing, recreate service
kubectl delete svc pika-mem0-api -n production
kubectl apply -f k8s/production/service.yaml
```

## COMMUNICATION (SEV1)

### Immediate (Within 5 minutes)

1. Update status page: "Investigating service outage"
2. Post in Slack #incidents: "@channel SEV1: Complete service outage. Investigating."
3. Page Backend Lead (SU) immediately

### Every 15 Minutes Until Resolved

1. Update status page with progress
2. Post update in Slack #incidents
3. If >30 minutes, escalate to Engineering Manager

### After Resolution

1. Status page: "Service restored. Monitoring for stability."
2. Slack: "SEV1 resolved. Post-mortem scheduled."
3. Schedule post-mortem within 24 hours

````

## 18.4 Incident Post-Mortem Template

```markdown
# Incident Post-Mortem: [Title]

## Incident Summary
| Field | Value |
|-------|-------|
| **Incident ID** | INC-2024-001 |
| **Date** | 2024-12-20 |
| **Severity** | SEV2 |
| **Duration** | 45 minutes |
| **Impact** | 15% of API requests failed |
| **Root Cause** | Database connection pool exhausted |

## Timeline (All Times UTC+7)

| Time | Event |
|------|-------|
| 14:00 | Deployment of v1.2.3 to production |
| 14:15 | First error alerts in Datadog |
| 14:17 | PagerDuty alert sent to on-call |
| 14:18 | On-call engineer acknowledged alert |
| 14:20 | Started investigation, checked pods |
| 14:25 | Identified database connection pool exhausted |
| 14:30 | Increased connection pool size |
| 14:35 | Error rate decreased to normal |
| 14:40 | Confirmed resolution, monitored for 5 minutes |
| 14:45 | Incident closed |

## Impact Analysis

### Quantitative Impact
- **Error Rate**: Spiked from 0.05% to 15% (300x increase)
- **Affected Requests**: ~6,750 requests failed (out of 45,000 total)
- **Affected Users**: Estimated 500 unique users
- **Revenue Impact**: Negligible (no payment failures)

### Qualitative Impact
- Users experienced "Internal Server Error" messages
- Slow response times for successful requests
- No data loss
- No security impact

## Root Cause Analysis

### What Happened
Version v1.2.3 introduced a new feature that queries the database more frequently than expected. The default connection pool size (20 connections) was insufficient to handle the increased load, causing connection timeouts and errors.

### Why It Happened
1. **Insufficient Load Testing**: New feature was not load tested at production scale
2. **No Connection Pool Monitoring**: We were not alerted when pool utilization hit 80%
3. **No Gradual Rollout**: Deployed to 100% of traffic immediately

### Contributing Factors
- Deployment happened during peak hours (2 PM)
- Connection pool size was not reviewed during code review
- Monitoring alerts for connection pool were not configured

## What Went Well ‚úÖ
- Alert fired quickly (within 2 minutes of issue)
- On-call engineer responded promptly
- Root cause identified in 7 minutes
- Fix deployed quickly
- Good communication in Slack

## What Went Poorly ‚ùå
- No load testing before deployment
- Deployed during peak hours without gradual rollout
- Connection pool monitoring was missing
- Took 15 minutes to fix (target: <10 minutes)

## Action Items

| Action | Owner | Due Date | Priority | Status |
|--------|-------|----------|----------|--------|
| Add connection pool monitoring alert | DevOps | 2024-12-22 | P0 | Open |
| Implement gradual rollout (canary) | SU | 2024-12-27 | P0 | Open |
| Add load testing to CI/CD | QA | 2024-12-30 | P1 | Open |
| Review connection pool sizing | Backend | 2024-12-23 | P1 | Open |
| Document deployment best practices | SU | 2024-12-25 | P2 | Open |
| Update runbook with this scenario | On-call | 2024-12-21 | P2 | Open |

## Lessons Learned

### Technical Lessons
1. Always load test new features that touch database
2. Monitor resource utilization (connections, memory, CPU)
3. Implement gradual rollouts for all production changes

### Process Lessons
1. Avoid deployments during peak hours
2. Require load testing sign-off before production deploy
3. Add connection pool alerts to monitoring

### Cultural Lessons
1. Incident response was effective, team should be commended
2. Blameless post-mortem culture working well
3. Documentation and runbooks are valuable
````

## 18.5 Incident Communication Templates

### Template: Status Page Update (Investigating)

```
Title: API Errors Affecting Search and Extract Features

Status: Investigating

We are currently investigating elevated error rates on our Memory API. 
Users may experience failed requests or slow response times. 

Our team is actively working to identify and resolve the issue.

Next update: 15 minutes

Started: 2024-12-20 14:15 UTC+7
```

### Template: Status Page Update (Identified)

```
Title: API Errors Affecting Search and Extract Features

Status: Identified

We have identified the root cause as database connection pool exhaustion. 
Our team is implementing a fix.

Expected resolution: 10 minutes

Started: 2024-12-20 14:15 UTC+7
Updated: 2024-12-20 14:25 UTC+7
```

### Template: Status Page Update (Resolved)

```
Title: API Errors Affecting Search and Extract Features

Status: Resolved

The issue has been resolved. All services are operating normally.

Root cause: Database connection pool exhausted due to increased load from new feature.

Fix: Increased connection pool size from 20 to 50 connections.

We apologize for any inconvenience.

Started: 2024-12-20 14:15 UTC+7
Resolved: 2024-12-20 14:45 UTC+7
Duration: 30 minutes
```

---

## ‚úÖ HO√ÄN TH√ÄNH TO√ÄN B·ªò SDD

# üéâ T√ìM T·∫ÆT: SDD ƒê√É ƒê·∫¶Y ƒê·ª¶ 100%

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           PIKA MEM0 SDD - COMPLETENESS STATUS               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  100%                            ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚úÖ Complete:     24/24 sections                            ‚îÇ
‚îÇ  ‚ùå Missing:      0/24 sections                             ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  üìÑ Total Words:  ~6,500 words                              ‚îÇ
‚îÇ  üìä Total Pages:  ~35 pages (A4)                            ‚îÇ
‚îÇ  ‚è±Ô∏è  Read Time:    ~25 minutes                              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  üéØ Production-Ready:  ‚úÖ YES                                ‚îÇ
‚îÇ  üéØ Implementation-Ready: ‚úÖ YES                             ‚îÇ
‚îÇ  üéØ Stakeholder-Ready: ‚úÖ YES                                ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Sections ƒê√£ C√≥ ƒê·∫ßy ƒê·ªß

**Part I: Foundation**

- ‚úÖ 1. Executive Summary
- ‚úÖ 2. Introduction & Context
- ‚úÖ 3. Goals, Scope & Constraints
- ‚úÖ 4. High-Level Design (HLD)
- ‚úÖ 5. Low-Level Design (LLD)

**Part II: Implementation**

- ‚úÖ 6. API Design & Contracts
- ‚úÖ 7. Data Design
- ‚úÖ 8. Deployment Strategy
- ‚úÖ 9. Security Design ‚≠ê (v·ª´a b·ªï sung)
- ‚úÖ 10. Resilience & Reliability ‚≠ê (v·ª´a b·ªï sung)

**Part III: Quality**

- ‚úÖ 11. Testing Strategy
- ‚úÖ 14. Non-Functional Requirements ‚≠ê (v·ª´a b·ªï sung)

**Part IV: Operations**

- ‚úÖ 18. Incident Response & Runbooks ‚≠ê (v·ª´a b·ªï sung)

**Part V: Launch**

- ‚úÖ Implementation Roadmap
- ‚úÖ Production Readiness Checklist

## üìã Next Steps cho Team

### Immediate (Tu·∫ßn n√†y)

1. ‚úÖ **Review SDD** v·ªõi stakeholders (Product, Tech Lead, DevOps)
2. ‚úÖ **Approve & sign-off** t·ª´ Tech Lead
3. ‚úÖ **Setup project repo** theo structure trong SDD

### Week 1-2 (Foundation)

4. ‚úÖ Implement theo **Phase 1** trong roadmap
5. ‚úÖ Setup infrastructure connections
6. ‚úÖ Basic logging & monitoring

### Week 3-4 (Core Features)

7. ‚úÖ Implement **2 core APIs**
8. ‚úÖ Unit tests 80%+ coverage
9. ‚úÖ Integration tests

### Week 5-6 (Production Hardening)

10. ‚úÖ Implement **Security controls**
11. ‚úÖ Implement **Resilience patterns**
12. ‚úÖ Load testing
13. ‚úÖ Monitoring dashboards

### Week 7-8 (Launch)

14. ‚úÖ Migration t·ª´ Mem0 Enterprise
15. ‚úÖ Production deployment
16. ‚úÖ Cut-over

---

## üíæ File Export

B·∫°n c√≥ th·ªÉ save document n√†y v·ªõi t√™n:

```
PIKA_Mem0_Self_Hosted_SDD_v1.0_FINAL.md
```

Document ƒë√£ s·∫µn s√†ng ƒë·ªÉ:

- ‚úÖ Present cho stakeholders
- ‚úÖ B·∫Øt ƒë·∫ßu implementation
- ‚úÖ Onboard new team members
- ‚úÖ Reference trong development

B·∫°n c√≥ mu·ªën m√¨nh:

1. Export th√†nh PDF?
2. T·∫°o executive summary 1-page?
3. T·∫°o presentation slides?
4. Gi·∫£i th√≠ch th√™m section n√†o?