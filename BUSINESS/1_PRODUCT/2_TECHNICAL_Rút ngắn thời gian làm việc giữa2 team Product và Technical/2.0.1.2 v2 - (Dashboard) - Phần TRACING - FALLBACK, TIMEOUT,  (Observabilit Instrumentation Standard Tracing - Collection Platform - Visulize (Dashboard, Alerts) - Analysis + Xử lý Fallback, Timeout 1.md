```
Cáº§n MECE toÃ n bá»™ cÃ¡c pháº§n module input, output, xá»­ lÃ½ bÃªn trong, báº¯n giao tiáº¿p ra bÃªn ngoÃ i, ...

  

MECE toÃ n bá»™ táº¥t cáº£ má»i thá»©.

Cáº§n timeout < 10 s, cÃ³ cÆ¡ cháº¿ fallback vÃ  báº¯n alert ra bÃªn ngoÃ i.

  

(chÆ°a cáº§n code vá»™i, lÃªn MECE overview toÃ n bá»™ cÃ¡c rá»§i ro chung Ä‘Ã£).
```

<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# P1 - Brainstorming: Tracing ToÃ n Bá»™ Há»‡ Thá»‘ng AI

## Tá»•ng Quan Chiáº¿n LÆ°á»£c

Äá»ƒ Ä‘áº£m báº£o sá»± á»•n Ä‘á»‹nh cho toÃ n bá»™ há»‡ thá»‘ng AI, viá»‡c triá»ƒn khai **tracing vÃ  observability** toÃ n diá»‡n lÃ  yáº¿u tá»‘ then chá»‘t. Dá»±a trÃªn tÃ i liá»‡u finAI vÃ  nghiÃªn cá»©u má»›i nháº¥t nÄƒm 2025, dÆ°á»›i Ä‘Ã¢y lÃ  framework chi tiáº¿t Ä‘á»ƒ monitor vÃ  trace há»‡ thá»‘ng AI production.

## 1. Ba Trá»¥ Cá»™t Observability Cho Há»‡ Thá»‘ng AI

### 1.1 Metrics (Chá»‰ Sá»‘ Äo LÆ°á»ng)

Metrics cung cáº¥p dá»¯ liá»‡u Ä‘á»‹nh lÆ°á»£ng vá» hiá»‡u suáº¥t há»‡ thá»‘ng:[^1][^2]

**AI-Specific Metrics:**

- **Latency**: Thá»i gian pháº£n há»“i cá»§a model (target: <500ms cho production)[^3][^4]
- **Token Usage**: Input/output tokens Ä‘á»ƒ kiá»ƒm soÃ¡t chi phÃ­[^5][^1]
- **Accuracy Score**: Äo lÆ°á»ng cháº¥t lÆ°á»£ng output (factuality, relevance)[^4][^1]
- **Hallucination Rate**: Tá»· lá»‡ AI táº¡o thÃ´ng tin sai (target: <2%)[^6][^5]
- **Cost per Request**: Chi phÃ­ API call (quan trá»ng vá»›i GPT-4)[^5]

**Infrastructure Metrics:**

- **CPU/GPU Utilization**: Sá»­ dá»¥ng tÃ i nguyÃªn compute[^7][^8]
- **Memory Usage**: Theo dÃµi memory leaks[^9]
- **Throughput**: Sá»‘ requests/second há»‡ thá»‘ng xá»­ lÃ½ Ä‘Æ°á»£c[^3][^4]
- **Error Rate**: Tá»· lá»‡ lá»—i há»‡ thá»‘ng[^2]


### 1.2 Logs (Nháº­t KÃ½ Há»‡ Thá»‘ng)

Logs ghi láº¡i chi tiáº¿t tá»«ng sá»± kiá»‡n trong há»‡ thá»‘ng:[^10][^2]

**Cáº§n Log:**

- Má»—i prompt gá»­i Ä‘áº¿n LLM vá»›i metadata (user_id, session_id, timestamp)[^11][^1]
- Response tá»« model kÃ¨m confidence score[^6]
- Tool calls vÃ  external API interactions[^1][^11]
- Errors vá»›i full stack trace[^2]
- User feedback (thumbs up/down)[^12]

**Log Structure Example:**

```json
{
  "trace_id": "abc-123",
  "span_id": "span-456",
  "timestamp": "2025-12-13T09:24:00Z",
  "user_id": "user_789",
  "prompt": "Analyze AAPL 10-K",
  "model": "gpt-4",
  "tokens_used": {"input": 1500, "output": 800},
  "latency_ms": 2340,
  "cost_usd": 0.045,
  "eval_score": {"accuracy": 0.92, "relevance": 0.88}
}
```


### 1.3 Traces (Dáº¥u Váº¿t PhÃ¢n TÃ¡n)

Distributed tracing theo dÃµi flow cá»§a request qua toÃ n bá»™ há»‡ thá»‘ng microservices:[^13][^10][^11]

**Táº¡i Sao Cáº§n Traces:**

- Má»™t user query cÃ³ thá»ƒ Ä‘i qua: API Gateway â†’ Context Retrieval â†’ LLM Endpoint â†’ Tool Execution â†’ Response Generation[^13][^11]
- Traces giÃºp identify bottleneck á»Ÿ Ä‘Ã¢u trong pipeline[^14][^10]
- Cho phÃ©p debug failures end-to-end[^15][^1]


## 2. Kiáº¿n TrÃºc Tracing Cho Há»‡ Thá»‘ng AI - 4 Lá»›p

	### Lá»›p 1: Instrumentation (Thu Tháº­p Dá»¯ Liá»‡u)

**Sá»­ dá»¥ng OpenTelemetry Standard**:[^16][^15][^11][^1]

- OpenTelemetry lÃ  CNCF standard, vendor-neutral[^17][^15]
- TÃ­ch há»£p vá»›i má»i observability platform (Langfuse, Datadog, New Relic)[^17][^16]
- Support Python, TypeScript, Go, Java[^1]

**Implementation cho finAI:**

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# Setup tracer
provider = TracerProvider()
otlp_exporter = OTLPSpanExporter(endpoint="https://your-observability-endpoint")
trace.set_tracer_provider(provider)

tracer = trace.get_tracer(__name__)

# Instrument LLM call
def analyze_10k(document_url):
    with tracer.start_as_current_span("analyze_10k") as span:
        span.set_attribute("document.url", document_url)
        span.set_attribute("llm.model", "gpt-4")
        
        # Call LLM
        result = call_llm(document_url)
        
        span.set_attribute("llm.tokens.input", result.tokens_input)
        span.set_attribute("llm.tokens.output", result.tokens_output)
        span.set_attribute("llm.cost", result.cost)
        
        return result
```

**Best Practices Instrumentation**:[^15][^1]

- Tag má»—i trace vá»›i metadata: `environment`, `user_id`, `model_version`[^1]
- Attach unique session_id vÃ  trace_id[^11]
- Log cáº£ prompt vÃ  completion (vá»›i PII controls)[^2][^1]


### Lá»›p 2: Collection (Thu Tháº­p \& LÆ°u Trá»¯)

**Chá»n Observability Platform:**

Dá»±a trÃªn use case cá»§a finAI (financial research agent), gá»£i Ã½ platform:[^18][^6]

1. **LangSmith** (cho LangChain apps):[^19][^20][^6]
    - Tá»± Ä‘á»™ng trace LangChain workflows
    - Built-in prompt evaluation
    - Production monitoring dashboard
    - Setup: `LANGCHAIN_TRACING_V2="true"`[^6]
2. **Langfuse** (open-source):[^21][^16]
    - OpenTelemetry native
    - Cost tracking per user
    - Human-in-the-loop review
    - Compliance-grade logging (quan trá»ng cho finance)[^16][^1]
3. **Maxim AI** (enterprise):[^11][^1]
    - Full-stack observability cho AI agents
    - Automated evals (factuality, toxicity)
    - SOC 2, HIPAA, GDPR compliant[^1]

**Cho finAI, recommend: LangSmith + Langfuse hybrid**

- LangSmith cho development \& debugging
- Langfuse cho production logging (open-source, cost control)


### Lá»›p 3: Analysis (PhÃ¢n TÃ­ch \& Alert)

**Real-Time Dashboards**:[^22][^12][^5]

Thiáº¿t láº­p 3 dashboard chÃ­nh:

**Dashboard 1: System Health**[^4][^5]

- Request volume (traces/hour)
- Average latency \& p95/p99
- Error rate
- GPU/CPU utilization

**Dashboard 2: AI Quality**[^12][^4][^5]

- Hallucination detection rate
- Evaluation scores (accuracy, relevance)
- User feedback scores
- Model confidence distribution

**Dashboard 3: Cost \& Efficiency**[^5][^12]

- Token usage breakdown (input vs output)
- Cost per user/per feature
- API call volume by model
- ROI metrics

**Alert Configuration**:[^23][^3][^2]

```yaml
alerts:
  - name: high_latency
    condition: avg_latency > 500ms
    window: 5min
    action: slack_notify
    
  - name: hallucination_spike
    condition: hallucination_rate > 3%
    window: 1hour
    action: pagerduty_alert
    
  - name: cost_anomaly
    condition: daily_cost > baseline * 1.5
    window: 1day
    action: email_finance_team
```


### Lá»›p 4: Continuous Evaluation (ÄÃ¡nh GiÃ¡ LiÃªn Tá»¥c)

**Automated Evals**:[^12][^6][^1]

- Cháº¡y evaluators trÃªn production traffic (sample 10-20%)[^1]
- Metrics: factuality, groundedness, relevance, coherence[^4]
- So sÃ¡nh output vs reference answers[^6]

**Human-in-the-Loop**:[^12][^1]

- Route low-confidence outputs (score <0.7) Ä‘áº¿n human review[^1]
- Thu tháº­p user feedback (thumbs up/down, comments)[^12]
- Convert failures thÃ nh test cases[^24]


## 3. Implementation Roadmap cho finAI (12 Tuáº§n)

### Sprint 0-2: Foundation (Tuáº§n 1-6)

**Má»¥c tiÃªu:** Thiáº¿t láº­p instrumentation cÆ¡ báº£n

- [ ] **Tuáº§n 1-2**: TÃ­ch há»£p OpenTelemetry vÃ o codebase[^17][^1]
    - Install SDK: `opentelemetry-api`, `opentelemetry-sdk`
    - Setup tracer provider vá»›i OTLP exporter
    - Instrument 5 core functions: `analyze_document`, `extract_metrics`, `flag_risks`, `generate_snapshot`, `api_gateway`
- [ ] **Tuáº§n 3-4**: Chá»n \& setup observability platform[^16][^6]
    - Pilot LangSmith cho development environment
    - Setup Langfuse cho staging/production
    - Configure authentication \& project structure
- [ ] **Tuáº§n 5-6**: Define baseline metrics[^23][^4]
    - Thu tháº­p data 2 tuáº§n tá»« 20-50 beta users
    - TÃ­nh baseline: median latency, average tokens, error rate
    - Document acceptable ranges

**Success Criteria:**

- 90% API calls Ä‘Æ°á»£c trace successfully[^14]
- CÃ³ thá»ƒ view end-to-end trace trong <2 phÃºt khi debug[^11]
- Zero PII leaks trong logs[^1]


### Sprint 3-5: Core Monitoring (Tuáº§n 7-15)

**Má»¥c tiÃªu:** Production-ready monitoring

- [ ] **Tuáº§n 7-9**: Build 3 core dashboards[^22][^5][^12]
    - System Health dashboard (Grafana hoáº·c LangSmith UI)
    - AI Quality dashboard vá»›i eval scores
    - Cost \& ROI dashboard
- [ ] **Tuáº§n 10-12**: Implement automated evaluations[^6][^1]
    - Setup evaluators: accuracy (so vá»›i ground truth 10-K data), hallucination detection, source attribution check
    - Run evals trÃªn 10% production traffic
    - Log eval scores vÃ o traces
- [ ] **Tuáº§n 13-15**: Configure alerts \& on-call[^3][^2]
    - Define 10 critical alerts (latency, error rate, cost spike, etc.)
    - Setup PagerDuty/Slack integration
    - Create runbooks cho common issues

**Success Criteria:**

- Mean Time to Detect (MTTD) issues <5 phÃºt[^7]
- Alert false-positive rate <10%[^25]
- 100% critical incidents cÃ³ trace data Ä‘áº§y Ä‘á»§[^11]


### Sprint 6-8: Advanced Observability (Tuáº§n 16-24)

**Má»¥c tiÃªu:** Proactive monitoring \& compliance

- [ ] **Tuáº§n 16-18**: Implement drift detection[^23][^4]
    - Monitor input distribution vs training data
    - Track model accuracy decay over time
    - Auto-retrain triggers
- [ ] **Tuáº§n 19-21**: Security \& compliance layer[^26][^1]
    - Audit trail logging (má»—i decision cÃ³ source traceback)
    - PII detection trong prompts/completions
    - SOC 2 compliance check (náº¿u cáº§n cho enterprise tier)
- [ ] **Tuáº§n 22-24**: Human-in-the-loop workflows[^12][^1]
    - Build review queue cho low-confidence outputs
    - Integrate user feedback vÃ o traces
    - A/B testing framework cho prompt iterations

**Success Criteria:**

- Model drift detected trong <24h khi accuracy drop >5%[^23]
- 100% audit trail cho compliance requirements[^26][^1]
- User feedback collection rate >30%[^12]


## 4. Metrics Dashboard - KPIs Quan Trá»ng Nháº¥t

### Tier 1: Critical (Monitor 24/7)

| Metric | Target | Alert Threshold | Why Critical |
| :-- | :-- | :-- | :-- |
| API Uptime | 99.9% | <99.5% | User trust, revenue impact[^2] |
| P95 Latency | <2s | >3s | User experience[^1][^3] |
| Error Rate | <0.5% | >1% | System stability[^2][^4] |
| Hallucination Rate | <2% | >3% | Trust \& compliance (finance)[^6][^5] |
| Daily Cost | Baseline Â±20% | >+50% | Budget control[^5][^12] |

### Tier 2: Important (Review Daily)

| Metric | Target | Review Frequency |
| :-- | :-- | :-- |
| Accuracy Score | >90% | Daily[^4] |
| Token Usage/Request | <3000 tokens | Daily[^5] |
| User Feedback Score | NPS >40 | Daily[^12] |
| Source Attribution Rate | 100% | Daily (compliance)[^6] |

### Tier 3: Strategic (Review Weekly)

| Metric | Target | Review Frequency |
| :-- | :-- | :-- |
| Model Confidence Score | Median >0.8 | Weekly[^1] |
| Feature Adoption Rate | >60% WAU | Weekly |
| Cost per Active User | <\$5/month | Weekly[^5] |
| Data Drift Score | <0.1 deviation | Weekly[^23] |

## 5. Tech Stack Recommendation cho finAI (Observability/ Instrumentation/Standard Tracing - Collection Platform - Visulize (Dashboard, Alerts) - Analysis

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application Layer (finAI)           â”‚
â”‚  LangChain + LLMs + Custom Agents         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Instrumentation   â”‚
        â”‚  OpenTelemetry/Langfuse    â”‚ â† Standard tracing
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Collection Layer         â”‚
    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
    â”‚ â”‚Langfuse â”‚ â”‚ Langfuse  â”‚â”‚ â† Dual platform
    â”‚ â”‚  (Dev)   â”‚ â”‚  (Prod)   â”‚â”‚
    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Analysis Layer   â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚ â”‚  Dashboards   â”‚ â”‚ â† Grafana/LangSmith UI
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚ â”‚  Alerts       â”‚ â”‚ â† Slack/PagerDuty/GoogleChat
        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Specific Tools:**

- **Tracing:** OpenTelemetry SDK[^15][^16][^1]
- **Observability Platform (Dev):** LangSmith[^19][^6]
- **Observability Platform (Prod):** Langfuse (open-source)[^21][^16]
- **Dashboards:** LangSmith built-in + Grafana for infra[^5][^12]
- **Alerts:** Slack webhooks + PagerDuty for critical[^2]
- **Evals:** LangSmith evaluators + custom Pydantic validators[^6]
- **Logs:** Structured JSON logs â†’ Elasticsearch/CloudWatch[^2]


## 6. Security \& Compliance Considerations

**Äáº·c Biá»‡t Quan Trá»ng Cho finAI (Financial Domain):**

### 6.1 Audit Trail Requirements[^26][^1]

- Má»—i sá»‘ liá»‡u trÃ­ch xuáº¥t pháº£i link back to source document + page number
- Log full reasoning chain: input â†’ model â†’ output â†’ user action
- Retention: 7 nÄƒm cho compliance (SEC requirements)


### 6.2 PII Protection[^16][^1]

- Redact sensitive data trÆ°á»›c khi log (SSN, account numbers)
- Encrypt traces at rest
- Role-based access control (RBAC) cho observability platform


### 6.3 Regulatory Compliance[^26]

- SOC 2 Type II náº¿u lÃ m enterprise tier
- GDPR cho EU users (right to deletion)
- Disclaimer logging: má»—i output pháº£i cÃ³ "research tool, not investment advice"


## 7. Common Pitfalls \& Mitigations

| Pitfall | Impact | Mitigation |
| :-- | :-- | :-- |
| **Over-instrumentation** | Performance overhead, noise[^1] | Chá»‰ trace critical paths; sample 10-20% production traffic[^1][^12] |
| **Alert fatigue** | Team ignores alerts[^2] | Tune thresholds dá»±a trÃªn baseline; max 5 alerts/day[^3] |
| **No baseline metrics** | KhÃ´ng biáº¿t "normal" lÃ  gÃ¬[^23] | Thu tháº­p 2-4 tuáº§n data trÆ°á»›c khi set alerts[^4][^23] |
| **Trace data silos** | KhÃ³ debug cross-service issues[^10] | Sá»­ dá»¥ng unified trace_id across all microservices[^11][^14] |
| **Ignoring user feedback** | Miss real-world failures[^12] | Build feedback loop vÃ o UI; review weekly[^12][^24] |
| **Cost explosion** | Logging costs > LLM costs[^5] | Sample logs, set retention policies, compress old traces[^5] |

## 8. Success Metrics cho Observability Initiative

**Sau 6 thÃ¡ng triá»ƒn khai, Ä‘o lÆ°á»ng:**

1. **Mean Time to Detect (MTTD)**: <5 phÃºt[^7]
2. **Mean Time to Resolve (MTTR)**: <30 phÃºt cho P1 issues[^4]
3. **Incident Prevention Rate**: 70% issues phÃ¡t hiá»‡n trÆ°á»›c khi user report[^7]
4. **Observability Coverage**: 95% code paths Ä‘Æ°á»£c trace[^11]
5. **Team Productivity**: Debug time giáº£m 60%[^6][^1]
6. **Cost Visibility**: 100% LLM costs traceable to user/feature[^5][^12]

## 9. Next Immediate Actions (This Week)

### Day 1-2: Research \& Decision

- [ ] Review 3 observability platforms: LangSmith, Langfuse, Maxim AI
- [ ] Äá»c OpenTelemetry documentation[^14][^15]
- [ ] List top 10 critical metrics cho finAI


### Day 3-4: POC Setup

- [ ] Install OpenTelemetry SDK trong 1 service
- [ ] Setup LangSmith free tier account[^19]
- [ ] Instrument 1 endpoint (`analyze_10k`) vÃ  test tracing


### Day 5: Planning

- [ ] Document baseline metrics target
- [ ] Create Jira tickets cho Sprint 0-2 (Foundation)
- [ ] Assign owner cho observability workstream

***

## Káº¿t Luáº­n

Tracing toÃ n bá»™ há»‡ thá»‘ng AI khÃ´ng pháº£i lÃ  optional feature, mÃ  lÃ  **table stakes cho production AI systems**. Vá»›i finAI trong financial domain, nÆ¡i accuracy vÃ  compliance lÃ  sinh má»‡nh, má»™t observability stack máº¡nh máº½ sáº½:[^15][^1]

1. **Äáº£m báº£o á»•n Ä‘á»‹nh**: PhÃ¡t hiá»‡n issues trÆ°á»›c khi impact users[^7][^2]
2. **Kiá»ƒm soÃ¡t chi phÃ­**: Track má»—i Ä‘á»“ng API cost[^5][^12]
3. **TÄƒng cháº¥t lÆ°á»£ng**: Continuous evaluation \& improvement[^4][^6]
4. **ÄÃ¡p á»©ng compliance**: Full audit trail cho regulators[^26][^1]
5. **TÄƒng tá»‘c development**: Debug nhanh hÆ¡n 60%[^6][^1]

Framework trÃªn Ä‘Æ°á»£c thiáº¿t káº¿ dá»±a trÃªn **best practices 2025 tá»« cÃ¡c leader nhÆ° Maxim AI, LangSmith, vÃ  OpenTelemetry community**, Ä‘á»“ng thá»i tailored cho needs cá»¥ thá»ƒ cá»§a finAI trong tÃ i liá»‡u chiáº¿n lÆ°á»£c.[^15][^1][^6]
<span style="display:none">[^27][^28][^29][^30][^31][^32][^33][^34][^35][^36][^37][^38][^39][^40][^41][^42][^43][^44][^45][^46][^47][^48]</span>

<div align="center">â‚</div>

[^1]: https://www.getmaxim.ai/articles/llm-observability-best-practices-for-2025/

[^2]: https://uptimerobot.com/knowledge-hub/observability/ai-observability-the-complete-guide/

[^3]: https://uptimerobot.com/knowledge-hub/monitoring/ai-monitoring-guide/

[^4]: https://coralogix.com/ai-blog/evaluation-metrics-for-ai-observability/

[^5]: https://datadope.io/en/observabilidad-y-monitorizacion-de-llms-en-produccion/

[^6]: https://murf.ai/blog/llm-observability-with-langsmith

[^7]: https://sparkemtech.co.uk/blog/infrastructure-monitoring-ai-revolutionising-operational-efficiency

[^8]: https://www.solo.io/topics/ai-connectivity/observability-in-ai-gateways-key-metrics

[^9]: https://logz.io/blog/infrastructure-monitoring/

[^10]: https://www.larksuite.com/en_us/topics/ai-glossary/tracing-in-distributed-systems

[^11]: https://dev.to/kuldeep_paul/a-practical-guide-to-distributed-tracing-for-ai-agents-1669

[^12]: https://langwatch.ai/blog/llm-monitoring-evaluation-for-real-world-production-use

[^13]: https://coralogix.com/ai-blog/advanced-techniques-for-monitoring-traces-in-ai-workflows/

[^14]: https://www.apica.io/blog/what-is-distributed-tracing-how-it-works-and-best-practices/

[^15]: https://sparkco.ai/blog/mastering-observability-in-ai-agent-actions-2025-deep-dive

[^16]: https://langfuse.com/integrations/native/opentelemetry

[^17]: https://docs.litellm.ai/docs/observability/opentelemetry_integration

[^18]: https://dev.to/kuldeep_paul/top-5-ai-observability-platforms-in-2025-4216

[^19]: https://academy.langchain.com/courses/intro-to-langsmith

[^20]: https://docs.langchain.com/langsmith/trace-with-opentelemetry

[^21]: https://last9.io/blog/langchain-observability/

[^22]: https://blog.langchain.com/langsmith-production-logging-automations/

[^23]: https://magnimindacademy.com/blog/best-practices-for-monitoring-and-logging-in-ai-systems/

[^24]: https://www.braintrust.dev/articles/best-llm-monitoring-tools-2026

[^25]: https://www.symestic.com/en-us/what-is/system-stability-monitoring

[^26]: https://www.fiddler.ai/guides/ai-observability-checklist-healthcare

[^27]: 1.7-V2-Step-Up-Template-finAI-Finance-Agent-Web-Browser-Chien-luoc-toan-dien-CEO-PM.md

[^28]: 1.7-V1-finAI-Finance-Agent-Web-Browser-Chien-luoc-CEO-PM.md

[^29]: 1.7.1-Rui-ro-phap-ly.md

[^30]: 1.8-Cach-lam-nhung-thu-moi.md

[^31]: https://www.facebook.com/100063494024929/posts/táº¡i-sao-cÃ¡c-báº¡n-báº£o-lÃ -e-dÃ¹ng-ai-viáº¿t-tÃ i-liá»‡u-nhÆ°ng-cáº£m-tháº¥y-váº«n-ko-Ä‘áº¡t-yÃªu-cáº§u/1442239101235877/

[^32]: https://www.studocu.vn/vn/document/truong-dai-hoc-khoa-hoc-xa-hoi-va-nhan-van-dai-hoc-quoc-gia-thanh-pho-ho-chi-minh/tin-hoc-co-so/bai-thuc-hanh-tuan-2-ko-co/120243037

[^33]: https://www.pace.edu.vn/tin-kho-tri-thuc/brainstorming-la-gi

[^34]: https://www.gauthmath.com/solution/1827492379056146/I-PH-N-C-HI-U-4-0-i-m-c-o-n-tr-ch-sau-Tr-th-ng-minh-nh-n-t-o-AI-1Tr-th-ng-minh-n

[^35]: https://loigiaihay.com/bai-tap-165064.html

[^36]: https://www.facebook.com/groups/tamsu.content/posts/trong-thá»i-Ä‘áº¡i-ai-tá»±-nháº­n-thá»©c-lÃ -chÃ¬a-khÃ³a-Ä‘á»ƒ-dá»‹ch-chuyá»ƒn-báº£n-thÃ¢n-lÃªn-vÃ¹ng-cÃ´n/1352443112534766/

[^37]: https://ielts-nguyenhuyen.com/bai-mau-ielts-writing-artificial-intelligence/

[^38]: https://therealielts.vn/ielts-writing-chu-de-tri-tue-nhan-tao

[^39]: https://www.manageengine.com/it-operations-management/ai-infrastructure-monitoring.html

[^40]: https://uptrace.dev/blog/opentelemetry-ai-systems

[^41]: https://smith.langchain.com

[^42]: https://dev.to/leena_malhotra/design-patterns-emerging-from-multi-agent-ai-systems-2aje

[^43]: https://www.studocu.vn/vn/document/dai-hoc-van-lang/anh-van-3/ky-thuat-phien-dich/106931693

[^44]: https://b-eye.com/blog/real-time-ai-design-patterns-guide/

[^45]: http://sixthsense.rakuten.com/blog/The-Ultimate-Checklist-for-Implementing-Data-Observability-in-Your-Organization

[^46]: https://www.leanware.co/insights/ai-agent-architecture

[^47]: https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/ai-agent-design-patterns

[^48]: https://www.kosedigital.com/wp-content/uploads/2025/07/AI-Implementation-Checklist.pdf


---
# P2 - MECE Framework: API Failure & Timeout Tracking Solutions
## Complete Analysis & Best Practices

---

## ğŸ“‹ Executive Summary

**Business Problem**: Track vÃ  visualize tá»‰ lá»‡ fail/timeout cá»§a 1000 API requests trong production workflow
**Current State**: ÄÃ£ cÃ³ timeout mechanism (8s), fallback strategy, vÃ  alert system
**Gap**: Thiáº¿u visualization layer Ä‘á»ƒ quantify "1000 API â†’ X timeout, Y failed"

---

## ğŸ¯ MECE Decomposition: All Possible Solutions

### Dimension 1: BY TOOL/PLATFORM (Where to track?)

```
â”œâ”€â”€ 1. APPLICATION LAYER (Code-based)
â”‚   â”œâ”€â”€ 1.1 In-Memory Metrics Collector
â”‚   â”œâ”€â”€ 1.2 Structured Logging (JSON logs)
â”‚   â””â”€â”€ 1.3 Custom Dashboard (FastAPI + Plotly)
â”‚
â”œâ”€â”€ 2. OBSERVABILITY PLATFORM (External SaaS/Self-hosted)
â”‚   â”œâ”€â”€ 2.1 Langfuse (LLM-focused observability)
â”‚   â”œâ”€â”€ 2.2 OpenTelemetry + Jaeger/Tempo
â”‚   â”œâ”€â”€ 2.3 DataDog / New Relic / Sentry
â”‚   â””â”€â”€ 2.4 Elastic Stack (ELK)
â”‚
â”œâ”€â”€ 3. METRICS & MONITORING (Time-series)
â”‚   â”œâ”€â”€ 3.1 Prometheus + Grafana
â”‚   â”œâ”€â”€ 3.2 CloudWatch (AWS)
â”‚   â”œâ”€â”€ 3.3 Azure Monitor
â”‚   â””â”€â”€ 3.4 InfluxDB + Chronograf
â”‚
â””â”€â”€ 4. DATABASE ANALYTICS (Historical data)
    â”œâ”€â”€ 4.1 PostgreSQL + Metabase
    â”œâ”€â”€ 4.2 ClickHouse + Superset
    â””â”€â”€ 4.3 BigQuery + Looker Studio
```

---

### Dimension 2: BY IMPLEMENTATION EFFORT

```
â”œâ”€â”€ MINIMAL EFFORT (0-2 hours)
â”‚   â”œâ”€â”€ Langfuse Dashboard filtering
â”‚   â”œâ”€â”€ CloudWatch Logs Insights
â”‚   â””â”€â”€ JSON log parsing script
â”‚
â”œâ”€â”€ LOW EFFORT (2-8 hours)
â”‚   â”œâ”€â”€ Prometheus metrics export
â”‚   â”œâ”€â”€ In-memory collector + endpoint
â”‚   â””â”€â”€ Langfuse Python SDK script
â”‚
â”œâ”€â”€ MEDIUM EFFORT (1-3 days)
â”‚   â”œâ”€â”€ Custom dashboard (Plotly/Streamlit)
â”‚   â”œâ”€â”€ Grafana dashboard setup
â”‚   â””â”€â”€ Database persistence layer
â”‚
â””â”€â”€ HIGH EFFORT (1-2 weeks)
    â”œâ”€â”€ Full OpenTelemetry instrumentation
    â”œâ”€â”€ Data warehouse pipeline
    â””â”€â”€ Enterprise monitoring stack
```

---

### Dimension 3: BY DATA PERSISTENCE

```
â”œâ”€â”€ EPHEMERAL (In-memory, session-based)
â”‚   â”œâ”€â”€ In-memory collector (resets on restart)
â”‚   â””â”€â”€ Browser-based dashboard
â”‚
â”œâ”€â”€ SHORT-TERM (Minutes to days)
â”‚   â”œâ”€â”€ Prometheus (15-30 days retention)
â”‚   â”œâ”€â”€ Langfuse traces (90 days default)
â”‚   â””â”€â”€ CloudWatch logs (retention policy)
â”‚
â”œâ”€â”€ MEDIUM-TERM (Weeks to months)
â”‚   â”œâ”€â”€ PostgreSQL with partitioning
â”‚   â””â”€â”€ Elasticsearch with ILM
â”‚
â””â”€â”€ LONG-TERM (Years, compliance)
    â”œâ”€â”€ S3 + Athena
    â”œâ”€â”€ BigQuery
    â””â”€â”€ Data Lake (Parquet files)
```

---

### Dimension 4: BY VISUALIZATION TYPE

```
â”œâ”€â”€ REAL-TIME DASHBOARD
â”‚   â”œâ”€â”€ Auto-refresh web page
â”‚   â”œâ”€â”€ WebSocket live updates
â”‚   â””â”€â”€ Streaming dashboard (Grafana Live)
â”‚
â”œâ”€â”€ STATIC REPORTS
â”‚   â”œâ”€â”€ Daily/weekly email report
â”‚   â”œâ”€â”€ PDF export
â”‚   â””â”€â”€ CSV export for analysis
â”‚
â”œâ”€â”€ INTERACTIVE ANALYTICS
â”‚   â”œâ”€â”€ Jupyter notebooks
â”‚   â”œâ”€â”€ Superset/Metabase
â”‚   â””â”€â”€ Looker Studio
â”‚
â””â”€â”€ ALERTING
    â”œâ”€â”€ Slack/Teams notifications
    â”œâ”€â”€ PagerDuty incidents
    â””â”€â”€ Email digests
```

---

## âœ… RECOMMENDED SOLUTION MATRIX

### Scenario-based Recommendations

| Scenario | Primary Tool | Secondary Tool | Effort | Cost |
|----------|-------------|----------------|---------|------|
| **Startup (MVP, <10K req/day)** | Langfuse Dashboard | Python script | Low | Free |
| **Scaling (10K-100K req/day)** | Prometheus + Grafana | Langfuse | Medium | $0-50/mo |
| **Enterprise (>100K req/day)** | OpenTelemetry + Tempo | ClickHouse + Superset | High | $500+/mo |
| **Budget-constrained** | JSON logs + Script | In-memory collector | Minimal | $0 |
| **Quick demo for stakeholders** | Langfuse Dashboard | Custom HTML dashboard | Minimal | $0 |
| **Production-grade monitoring** | Prometheus + Grafana | DataDog/New Relic | Medium | $200+/mo |
| **Compliance/audit requirements** | PostgreSQL + audit log | BigQuery archival | High | $100+/mo |

---

## ğŸ† BEST PRACTICES: The Optimal Stack

### **Tier 1: Foundation (Must Have) - "The Essentials"**

#### 1.1 Standardized Request Metadata
```python
# âœ… BEST PRACTICE: Unified metadata schema
@dataclass
class RequestMetadata:
    """Standardized across all layers"""
    request_id: str           # UUID for tracing
    timestamp: datetime       # ISO 8601
    execution_time_ms: float  # Milliseconds
    status: Literal["success", "timeout", "failed"]
    error_code: Optional[str] # Standardized error codes
    agent_id: str            # Workflow/agent identifier
    user_id: Optional[str]   # For user-level analytics
    session_id: Optional[str] # For session tracking
```

**Why**: Consistency across logging, metrics, traces, and analytics

#### 1.2 Three-Layer Recording Strategy
```python
# Layer 1: Structured Logging (for debugging)
logger.info("Request completed", extra=metadata.dict())

# Layer 2: Metrics (for alerting & SLOs)
api_requests_total.labels(status=metadata.status).inc()
api_duration_seconds.observe(metadata.execution_time_ms / 1000)

# Layer 3: Traces (for deep dive)
langfuse_client.update_current_trace(metadata=metadata.dict())
```

**Why**: Defense in depth - each layer serves different purpose

#### 1.3 Timeout Threshold as Configuration
```python
# âœ… BEST PRACTICE: Externalize thresholds
class TimeoutConfig:
    API_TIMEOUT = float(os.getenv("API_TIMEOUT", "8.0"))
    GRAPH_TIMEOUT = float(os.getenv("GRAPH_TIMEOUT", "7.0"))
    WARN_THRESHOLD = API_TIMEOUT * 0.75  # 6s
    CRITICAL_THRESHOLD = API_TIMEOUT * 0.9  # 7.2s
```

**Why**: Easy to adjust per environment (dev/staging/prod)

---

### **Tier 2: Observability (Should Have) - "The Langfuse Layer"**

#### 2.1 Comprehensive Trace Updates
```python
# âœ… BEST PRACTICE: Update at every decision point
from langfuse.decorators import observe

@observe(capture_input=False, capture_output=False)
async def run_agent_and_wait(...):
    start = time.time()
    trace_id = langfuse_client.get_trace_id()
    
    try:
        # Execute
        result = await asyncio.wait_for(service.perform(...), timeout=8.0)
        elapsed = time.time() - start
        
        # âœ… Update on SUCCESS
        langfuse_client.update_current_trace(
            metadata={
                "execution_time": elapsed,
                "is_timeout": False,
                "threshold_breached": elapsed >= WARN_THRESHOLD,
                "status": "success"
            }
        )
        
        # âœ… Score for analytics
        langfuse_client.score(
            trace_id=trace_id,
            name="execution_time_seconds",
            value=elapsed,
            data_type="NUMERIC"
        )
        
        langfuse_client.score(
            trace_id=trace_id,
            name="timeout_breach",
            value=1 if elapsed >= 8.0 else 0,
            data_type="NUMERIC"
        )
        
        return result
        
    except asyncio.TimeoutError:
        elapsed = time.time() - start
        
        # âœ… Update on TIMEOUT
        langfuse_client.update_current_trace(
            status="error",  # Critical: mark as error
            status_message="API timeout exceeded",
            metadata={
                "execution_time": elapsed,
                "is_timeout": True,
                "error_code": "API_TIMEOUT",
                "threshold": 8.0,
                "status": "timeout"
            }
        )
        
        # Score
        langfuse_client.score(
            trace_id=trace_id,
            name="timeout_breach",
            value=1,
            data_type="NUMERIC"
        )
        
        raise
```

**Why**: 
- `status="error"` enables dashboard filtering
- Scores enable aggregation queries
- Metadata enables root cause analysis

#### 2.2 Langfuse Query Patterns
```python
# âœ… BEST PRACTICE: Automated daily summary
from langfuse import Langfuse
from datetime import datetime, timedelta

def generate_daily_timeout_report():
    langfuse = Langfuse()
    
    # Fetch last 24h traces
    traces = langfuse.fetch_traces(
        from_timestamp=datetime.utcnow() - timedelta(hours=24),
        limit=10000
    )
    
    # Analyze
    total = len(traces.data)
    timeout = sum(1 for t in traces.data 
                  if t.metadata.get("is_timeout", False))
    
    # Query scores for exact metrics
    scores = langfuse.fetch_scores(
        name="timeout_breach",
        from_timestamp=datetime.utcnow() - timedelta(hours=24)
    )
    
    timeout_count = sum(s.value for s in scores.data)
    timeout_rate = (timeout_count / total * 100) if total > 0 else 0
    
    return {
        "period": "last_24h",
        "total_requests": total,
        "timeout_count": timeout_count,
        "timeout_rate": f"{timeout_rate:.2f}%",
        "threshold": "8.0s"
    }

# Schedule daily
import schedule
schedule.every().day.at("09:00").do(generate_daily_timeout_report)
```

**Why**: 
- Automated reporting â†’ no manual work
- Historical trend tracking
- Early warning system

#### 2.3 Langfuse Dashboard Bookmarks
```markdown
# âœ… BEST PRACTICE: Create saved filters

## Dashboard URLs (bookmark these):
1. **All Timeout Traces (24h)**
   - Filter: `status = error AND metadata.error_code = API_TIMEOUT`
   - Sort: execution_time DESC
   
2. **Near-Timeout Warnings (24h)**
   - Filter: `metadata.threshold_breached = true AND status = success`
   - Use case: Proactive optimization
   
3. **Timeout Trend (7 days)**
   - View: Analytics > Scores
   - Metric: `timeout_breach`
   - Aggregation: SUM per day
   
4. **Slowest Endpoints (24h)**
   - Filter: `status = success`
   - Sort: metadata.execution_time DESC
   - Limit: 100
```

**Why**: Instant access to key metrics, no re-filtering

---

### **Tier 3: Metrics & Alerting (Should Have) - "The Prometheus Layer"**

#### 3.1 Comprehensive Prometheus Metrics
```python
# âœ… BEST PRACTICE: Full metric coverage
from prometheus_client import Counter, Histogram, Gauge, Summary

# === COUNTERS (cumulative) ===
api_requests_total = Counter(
    'api_requests_total',
    'Total API requests by status',
    ['agent_id', 'status']  # Labels for breakdown
)

api_timeout_total = Counter(
    'api_timeout_total',
    'Total timeout events',
    ['agent_id', 'timeout_type']  # API_TIMEOUT, GRAPH_TIMEOUT
)

api_errors_total = Counter(
    'api_errors_total',
    'Total errors by code',
    ['agent_id', 'error_code']
)

# === HISTOGRAMS (distribution) ===
api_duration_seconds = Histogram(
    'api_duration_seconds',
    'API request duration distribution',
    ['agent_id'],
    buckets=[0.1, 0.5, 1, 2, 5, 8, 10, 15, 30]  # Include 8s threshold
)

# === GAUGES (current value) ===
api_timeout_rate = Gauge(
    'api_timeout_rate_percent',
    'Current timeout rate percentage (last 1h rolling)'
)

api_requests_in_flight = Gauge(
    'api_requests_in_flight',
    'Current number of ongoing requests'
)

# === SUMMARY (quantiles) ===
api_duration_summary = Summary(
    'api_duration_seconds_summary',
    'API duration with quantiles',
    ['agent_id']
)

# Usage in endpoint
@router.post("/runs/wait")
async def run_agent_and_wait(...):
    api_requests_in_flight.inc()  # Start
    start = time.time()
    
    try:
        result = await asyncio.wait_for(...)
        elapsed = time.time() - start
        
        # Record metrics
        api_requests_total.labels(
            agent_id=payload.agent_id,
            status="success"
        ).inc()
        
        api_duration_seconds.labels(
            agent_id=payload.agent_id
        ).observe(elapsed)
        
        api_duration_summary.labels(
            agent_id=payload.agent_id
        ).observe(elapsed)
        
        return result
        
    except asyncio.TimeoutError:
        elapsed = time.time() - start
        
        # Record timeout
        api_requests_total.labels(
            agent_id=payload.agent_id,
            status="timeout"
        ).inc()
        
        api_timeout_total.labels(
            agent_id=payload.agent_id,
            timeout_type="API_TIMEOUT"
        ).inc()
        
        api_duration_seconds.labels(
            agent_id=payload.agent_id
        ).observe(elapsed)
        
        raise
        
    finally:
        api_requests_in_flight.dec()  # End
```

**Why**:
- Counters: Track totals (for rate calculations)
- Histograms: See distribution (P50, P95, P99)
- Gauges: Monitor current state
- Summary: Efficient quantile calculation

#### 3.2 Grafana Dashboard JSON
```json
{
  "dashboard": {
    "title": "API Timeout & Failure Monitoring",
    "panels": [
      {
        "id": 1,
        "title": "Timeout Rate (Last 1h)",
        "targets": [{
          "expr": "(sum(rate(api_timeout_total[1h])) / sum(rate(api_requests_total[1h]))) * 100"
        }],
        "type": "stat",
        "fieldConfig": {
          "thresholds": {
            "steps": [
              {"value": 0, "color": "green"},
              {"value": 5, "color": "yellow"},
              {"value": 10, "color": "red"}
            ]
          }
        }
      },
      {
        "id": 2,
        "title": "Request Duration Distribution",
        "targets": [{
          "expr": "histogram_quantile(0.95, sum(rate(api_duration_seconds_bucket[5m])) by (le))"
        }],
        "type": "graph"
      },
      {
        "id": 3,
        "title": "Timeout Breakdown by Agent",
        "targets": [{
          "expr": "sum by (agent_id) (rate(api_timeout_total[1h]))"
        }],
        "type": "bargauge"
      }
    ]
  }
}
```

#### 3.3 Alerting Rules
```yaml
# prometheus-alerts.yml
groups:
  - name: api_timeout_alerts
    interval: 1m
    rules:
      # Alert: High timeout rate
      - alert: HighAPITimeoutRate
        expr: |
          (sum(rate(api_timeout_total[5m])) / sum(rate(api_requests_total[5m]))) * 100 > 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "API timeout rate exceeded 10%"
          description: "Current timeout rate: {{ $value | humanize }}%"
          
      # Alert: P95 latency approaching threshold
      - alert: HighP95Latency
        expr: |
          histogram_quantile(0.95, sum(rate(api_duration_seconds_bucket[5m])) by (le)) > 7
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "P95 latency approaching 8s timeout"
          description: "P95 latency: {{ $value | humanize }}s"
          
      # Alert: Specific agent degradation
      - alert: AgentTimeoutSpike
        expr: |
          sum by (agent_id) (rate(api_timeout_total[5m])) > 0.1
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "Agent {{ $labels.agent_id }} experiencing timeouts"
          description: "Timeout rate: {{ $value | humanize }}/s"
```

**Why**: Proactive alerting before SLO breach

---

### **Tier 4: Persistence & Analytics (Optional) - "The Database Layer"**

#### 4.1 PostgreSQL Schema for Long-term Storage
```sql
-- âœ… BEST PRACTICE: Optimized for time-series queries
CREATE TABLE api_request_metrics (
    id BIGSERIAL PRIMARY KEY,
    request_id UUID NOT NULL UNIQUE,
    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    -- Request details
    agent_id VARCHAR(100) NOT NULL,
    user_id VARCHAR(100),
    session_id VARCHAR(100),
    
    -- Performance
    execution_time_ms NUMERIC(10,2) NOT NULL,
    status VARCHAR(20) NOT NULL, -- success, timeout, failed
    error_code VARCHAR(50),
    error_message TEXT,
    
    -- Flags
    is_timeout BOOLEAN NOT NULL DEFAULT FALSE,
    is_failed BOOLEAN NOT NULL DEFAULT FALSE,
    threshold_breached BOOLEAN NOT NULL DEFAULT FALSE,
    
    -- Metadata (JSONB for flexibility)
    metadata JSONB,
    
    -- Indexes
    CONSTRAINT valid_status CHECK (status IN ('success', 'timeout', 'failed'))
);

-- Indexes for fast queries
CREATE INDEX idx_timestamp ON api_request_metrics (timestamp DESC);
CREATE INDEX idx_status ON api_request_metrics (status);
CREATE INDEX idx_agent_timeout ON api_request_metrics (agent_id, is_timeout);
CREATE INDEX idx_metadata_gin ON api_request_metrics USING GIN (metadata);

-- Partitioning by time (for large scale)
CREATE TABLE api_request_metrics_2025_12 PARTITION OF api_request_metrics
    FOR VALUES FROM ('2025-12-01') TO ('2026-01-01');

-- Materialized view for fast aggregation
CREATE MATERIALIZED VIEW api_timeout_summary_hourly AS
SELECT 
    date_trunc('hour', timestamp) AS hour,
    agent_id,
    COUNT(*) AS total_requests,
    SUM(CASE WHEN is_timeout THEN 1 ELSE 0 END) AS timeout_count,
    AVG(execution_time_ms) AS avg_duration_ms,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY execution_time_ms) AS p95_duration_ms
FROM api_request_metrics
GROUP BY 1, 2
WITH DATA;

-- Refresh every 5 minutes
CREATE INDEX ON api_timeout_summary_hourly (hour DESC, agent_id);
```

#### 4.2 Data Persistence Service
```python
# âœ… BEST PRACTICE: Async batch insertion
from asyncpg import create_pool
from asyncio import Queue
import asyncio

class MetricsPersistenceService:
    def __init__(self, db_url: str, batch_size: int = 100):
        self.db_url = db_url
        self.batch_size = batch_size
        self.queue = Queue()
        self.pool = None
        
    async def start(self):
        """Start persistence worker"""
        self.pool = await create_pool(self.db_url, min_size=2, max_size=10)
        asyncio.create_task(self._batch_worker())
        
    async def record(self, metrics: APIRequestMetrics):
        """Add metrics to queue"""
        await self.queue.put(metrics)
        
    async def _batch_worker(self):
        """Background worker for batch insertion"""
        while True:
            batch = []
            
            # Collect batch
            while len(batch) < self.batch_size:
                try:
                    item = await asyncio.wait_for(self.queue.get(), timeout=1.0)
                    batch.append(item)
                except asyncio.TimeoutError:
                    break
            
            if batch:
                await self._insert_batch(batch)
            
            await asyncio.sleep(0.1)
    
    async def _insert_batch(self, batch: List[APIRequestMetrics]):
        """Bulk insert batch"""
        async with self.pool.acquire() as conn:
            await conn.executemany(
                """
                INSERT INTO api_request_metrics 
                (request_id, timestamp, agent_id, execution_time_ms, 
                 status, error_code, is_timeout, is_failed, metadata)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                ON CONFLICT (request_id) DO NOTHING
                """,
                [(m.request_id, m.timestamp, m.agent_id, 
                  m.execution_time * 1000, m.status.value, m.error_code,
                  m.is_timeout, m.is_failed, json.dumps(m.metadata))
                 for m in batch]
            )
```

#### 4.3 Analytics Queries
```sql
-- âœ… Query 1: Timeout rate by hour (last 24h)
SELECT 
    hour,
    SUM(total_requests) AS requests,
    SUM(timeout_count) AS timeouts,
    ROUND(SUM(timeout_count)::NUMERIC / NULLIF(SUM(total_requests), 0) * 100, 2) AS timeout_rate_pct
FROM api_timeout_summary_hourly
WHERE hour >= NOW() - INTERVAL '24 hours'
GROUP BY hour
ORDER BY hour DESC;

-- âœ… Query 2: Top timeout agents
SELECT 
    agent_id,
    COUNT(*) AS total,
    SUM(CASE WHEN is_timeout THEN 1 ELSE 0 END) AS timeouts,
    ROUND(AVG(execution_time_ms), 2) AS avg_ms,
    ROUND(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY execution_time_ms), 2) AS p95_ms
FROM api_request_metrics
WHERE timestamp >= NOW() - INTERVAL '24 hours'
GROUP BY agent_id
HAVING SUM(CASE WHEN is_timeout THEN 1 ELSE 0 END) > 0
ORDER BY timeouts DESC
LIMIT 10;

-- âœ… Query 3: Timeout trend (7 days)
SELECT 
    DATE(timestamp) AS date,
    COUNT(*) AS total,
    SUM(CASE WHEN is_timeout THEN 1 ELSE 0 END) AS timeouts,
    ROUND(SUM(CASE WHEN is_timeout THEN 1 ELSE 0 END)::NUMERIC / COUNT(*) * 100, 2) AS rate_pct
FROM api_request_metrics
WHERE timestamp >= NOW() - INTERVAL '7 days'
GROUP BY DATE(timestamp)
ORDER BY date DESC;
```

---

## ğŸ¯ THE RECOMMENDED STACK (All Tiers Combined)

### Architecture Diagram
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Application                      â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         @router.post("/runs/wait")                   â”‚  â”‚
â”‚  â”‚                                                      â”‚  â”‚
â”‚  â”‚   try:                                              â”‚  â”‚
â”‚  â”‚       result = await service.perform()             â”‚  â”‚
â”‚  â”‚       â”œâ”€â†’ Log to stdout (JSON)                     â”‚  â”‚
â”‚  â”‚       â”œâ”€â†’ Prometheus metrics.inc()                 â”‚  â”‚
â”‚  â”‚       â”œâ”€â†’ Langfuse.update_trace()                  â”‚  â”‚
â”‚  â”‚       â””â”€â†’ DB persist (async queue)                 â”‚  â”‚
â”‚  â”‚   except TimeoutError:                             â”‚  â”‚
â”‚  â”‚       â”œâ”€â†’ Log error                                â”‚  â”‚
â”‚  â”‚       â”œâ”€â†’ Prometheus timeout_total.inc()           â”‚  â”‚
â”‚  â”‚       â”œâ”€â†’ Langfuse.update_trace(status="error")    â”‚  â”‚
â”‚  â”‚       â”œâ”€â†’ Alert system (Slack/PagerDuty)           â”‚  â”‚
â”‚  â”‚       â””â”€â†’ DB persist                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“  â†“  â†“  â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                               â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”‚
   â”‚ STDOUT  â”‚      â”‚Prometheusâ”‚      â”‚Langfuse â”‚      â”‚
   â”‚ Logs    â”‚      â”‚ /metrics â”‚      â”‚  API    â”‚      â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚
        â”‚                â”‚                â”‚            â”‚
        â–¼                â–¼                â–¼            â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
   â”‚ Loki/   â”‚      â”‚ Grafana â”‚      â”‚Langfuse â”‚     â”‚
   â”‚ Cloud   â”‚      â”‚Dashboardâ”‚      â”‚Dashboardâ”‚     â”‚
   â”‚ Watch   â”‚      â”‚         â”‚      â”‚         â”‚     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                                                      â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                      â”‚
                    â”‚PostgreSQLâ”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Metrics  â”‚
                    â”‚  Table   â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚Metabase/â”‚
                    â”‚ Supersetâ”‚
                    â”‚ (BI)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Checklist

#### Phase 1: Foundation (Week 1)
- [ ] Add standardized `APIRequestMetrics` dataclass
- [ ] Implement structured JSON logging
- [ ] Add `@observe` decorator to all API endpoints
- [ ] Update Langfuse traces with `status="error"` on timeout
- [ ] Create Langfuse dashboard bookmarks

**Deliverable**: Basic timeout visibility in Langfuse

#### Phase 2: Metrics (Week 2)
- [ ] Add Prometheus client library
- [ ] Instrument all endpoints with metrics
- [ ] Deploy Prometheus server
- [ ] Create Grafana dashboard
- [ ] Setup alerting rules

**Deliverable**: Real-time metrics + alerts

#### Phase 3: Persistence (Week 3-4)
- [ ] Design PostgreSQL schema
- [ ] Implement async persistence service
- [ ] Create materialized views
- [ ] Setup Metabase/Superset
- [ ] Automate daily reports

**Deliverable**: Historical analytics + reports

---

## ğŸ“Š DECISION MATRIX: Which Solution to Choose?

### Use Case 1: "I need quick visibility NOW (today)"
**Solution**: Langfuse Dashboard filtering
- Effort: 30 minutes
- Cost: $0
- Steps:
  1. Ensure `status="error"` on timeout
  2. Open Langfuse â†’ Traces â†’ Filter `status=error`
  3. View count: "150 errors / 1000 total = 15% timeout rate"

### Use Case 2: "I need a dashboard to show PM/CEO"
**Solution**: Langfuse Dashboard + Python script for summary
- Effort: 2 hours
- Cost: $0
- Steps:
  1. Use Langfuse Dashboard (pretty UI)
  2. Write Python script for daily email report
  3. Show both in weekly meeting

### Use Case 3: "I need production monitoring with alerts"
**Solution**: Prometheus + Grafana + AlertManager
- Effort: 1-2 days
- Cost: $0 (self-hosted) or $50/mo (managed)
- Steps:
  1. Add Prometheus metrics to code
  2. Deploy Prometheus + Grafana
  3. Create dashboard + alert rules
  4. Integrate with Slack/PagerDuty

### Use Case 4: "I need historical analysis (3+ months)"
**Solution**: PostgreSQL + Metabase
- Effort: 3-4 days
- Cost: $20-50/mo (database)
- Steps:
  1. Create database schema
  2. Implement persistence service
  3. Setup Metabase
  4. Create saved queries

### Use Case 5: "I need everything (enterprise-grade)"
**Solution**: Full Stack (Langfuse + Prometheus + PostgreSQL + Grafana)
- Effort: 2 weeks
- Cost: $200-500/mo
- Steps:
  1. Implement all layers
  2. Setup CI/CD for deployments
  3. Document runbooks
  4. Train team

---

## ğŸš¨ ANTI-PATTERNS TO AVOID

### âŒ DON'T: Store everything in memory without persistence
**Problem**: Data lost on restart
**Solution**: Use Redis or database for persistence

### âŒ DON'T: Log to file without rotation
**Problem**: Disk full crash
**Solution**: Use logrotate or cloud logging

### âŒ DON'T: Block main thread with metrics collection
**Problem**: Adds latency to requests
**Solution**: Use async background workers

### âŒ DON'T: Create too many Prometheus labels
**Problem**: Cardinality explosion â†’ OOM
**Solution**: Limit labels to <10 unique values per label

### âŒ DON'T: Query production DB for analytics
**Problem**: Slow queries impact users
**Solution**: Use read replicas or materialized views

### âŒ DON'T: Send alerts on every single timeout
**Problem**: Alert fatigue
**Solution**: Alert on rate (e.g., >10% for 5min)

### âŒ DON'T: Store raw logs forever
**Problem**: Storage cost explosion
**Solution**: Archive to S3 after 30 days

### âŒ DON'T: Hardcode thresholds in code
**Problem**: Need redeploy to adjust
**Solution**: Use environment variables or config service

---

## ğŸ“ˆ SUCCESS METRICS

### Leading Indicators (What to track)
- Timeout rate < 5%
- P95 latency < 7s (buffer before 8s timeout)
- Alert fatigue: <3 alerts/day
- MTTD (Mean Time To Detect): <5 minutes
- MTTR (Mean Time To Resolve): <1 hour

### Lagging Indicators (Business impact)
- User satisfaction (CSAT) â†‘
- Support tickets related to "slow API" â†“
- SLA compliance â†‘
- Revenue impact from downtime â†“

---

## ğŸ“ TRAINING CHECKLIST

### For Developers
- [ ] How to add metrics to new endpoints
- [ ] How to query Langfuse for debugging
- [ ] How to interpret Grafana dashboards
- [ ] When to use each layer (logs vs metrics vs traces)

### For DevOps/SRE
- [ ] How to tune Prometheus retention
- [ ] How to respond to timeout alerts
- [ ] How to export data for postmortem
- [ ] Database maintenance (partitions, vacuuming)

### For Product/Business
- [ ] How to read Langfuse dashboard
- [ ] How to export weekly reports
- [ ] What timeout rate is acceptable
- [ ] How to request new metrics

---

## ğŸ”„ CONTINUOUS IMPROVEMENT

### Monthly Review Checklist
- [ ] Review timeout rate trend (going up/down?)
- [ ] Identify slowest endpoints
- [ ] Check alert accuracy (false positives?)
- [ ] Update thresholds based on data
- [ ] Archive old data (>90 days)
- [ ] Update runbooks with new learnings

### Quarterly Goals
- Q1: Implement foundation (Langfuse + metrics)
- Q2: Reduce timeout rate from 15% â†’ 5%
- Q3: Add persistence layer + historical analytics
- Q4: Optimize P95 latency < 6s

---

## ğŸ“š FURTHER READING

### Essential Resources
1. **Langfuse Docs**: https://langfuse.com/docs
2. **Prometheus Best Practices**: https://prometheus.io/docs/practices/
3. **Grafana Dashboards**: https://grafana.com/grafana/dashboards/
4. **OpenTelemetry**: https://opentelemetry.io/

### Books
- "Observability Engineering" - Charity Majors
- "Site Reliability Engineering" - Google
- "The Art of Monitoring" - James Turnbull

---

## âœ… FINAL RECOMMENDATION: The 80/20 Solution

For most teams (startup to mid-size), this is the sweet spot:

```
PRIMARY:   Langfuse (traces + dashboard)
           â”œâ”€ Already integrated âœ“
           â”œâ”€ Free tier generous âœ“
           â””â”€ Beautiful UI for stakeholders âœ“

SECONDARY: Prometheus + Grafana (metrics + alerts)
           â”œâ”€ Industry standard âœ“
           â”œâ”€ Open source / self-hostable âœ“
           â””â”€ SLO monitoring âœ“

TERTIARY:  Python script (automated reports)
           â”œâ”€ Daily email summary âœ“
           â”œâ”€ Slack bot for quick queries âœ“
           â””â”€ CSV export for deep analysis âœ“
```

**Effort**: 1 week
**Cost**: $0-50/month
**Maintenance**: ~2 hours/month
**Value**: High visibility + proactive alerting

---

**Remember**: The best monitoring system is one that actually gets used. Start simple with Langfuse, add Prometheus when you need alerting, add database when you need historical analysis. Don't over-engineer on day 1! ğŸš€


---
# Timeout 

# P3 - Deep Research: Giáº£i phÃ¡p ToÃ n diá»‡n Ä‘á»ƒ Visualize Timeout Metrics

**TÃ¡c giáº£:** Manus AI  
**NgÃ y:** 14/12/2025  
**TÃ³m táº¯t:** TÃ i liá»‡u nÃ y cung cáº¥p má»™t báº£n phÃ¢n tÃ­ch sÃ¢u (deep research) vá» cÃ¡c giáº£i phÃ¡p ká»¹ thuáº­t Ä‘á»ƒ theo dÃµi, Ä‘o lÆ°á»ng, vÃ  visualize (trá»±c quan hÃ³a) sá»‘ lÆ°á»£ng API requests bá»‹ quÃ¡ timeout (> 8 giÃ¢y) trong má»™t há»‡ thá»‘ng AI workflow. ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t kiáº¿n trÃºc hoÃ n chá»‰nh, kÃ¨m theo code máº«u vÃ  cáº¥u hÃ¬nh chi tiáº¿t Ä‘á»ƒ triá»ƒn khai má»™t há»‡ thá»‘ng giÃ¡m sÃ¡t Ä‘áº³ng cáº¥p tháº¿ giá»›i (world-class).

---

## 1. Bá»‘i cáº£nh vÃ  YÃªu cáº§u

### 1.1. Váº¥n Ä‘á»

Há»‡ thá»‘ng AI workflow hiá»‡n táº¡i cÃ³ cÆ¡ cháº¿ timeout, fallback vÃ  alert khi má»™t request vÆ°á»£t quÃ¡ 8 giÃ¢y. Tuy nhiÃªn, ngoÃ i viá»‡c nháº­n alert, team cáº§n má»™t giáº£i phÃ¡p Ä‘á»ƒ **visualize (trá»±c quan hÃ³a)** cÃ¡c sá»‘ liá»‡u nÃ y. Cá»¥ thá»ƒ, cáº§n tráº£ lá»i cÃ¡c cÃ¢u há»i:

1.  Trong 1000 API requests, cÃ³ **bao nhiÃªu request bá»‹ timeout** (> 8 giÃ¢y)?
2.  **Tá»· lá»‡ timeout** lÃ  bao nhiÃªu pháº§n trÄƒm?
3.  Xu hÆ°á»›ng timeout **thay Ä‘á»•i nhÆ° tháº¿ nÃ o theo thá»i gian**?
4.  PhÃ¢n phá»‘i Ä‘á»™ trá»… (latency distribution) cá»§a cÃ¡c request trÃ´ng nhÆ° tháº¿ nÃ o?

### 1.2. Má»¥c tiÃªu

XÃ¢y dá»±ng má»™t há»‡ thá»‘ng giÃ¡m sÃ¡t (monitoring) vÃ  trá»±c quan hÃ³a (visualization) cho phÃ©p:

-   **Theo dÃµi (Track):** Ghi nháº­n má»i request vÃ  Ä‘o lÆ°á»ng thá»i gian xá»­ lÃ½ cá»§a nÃ³.
-   **Äo lÆ°á»ng (Measure):** TÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ quan trá»ng nhÆ° tá»· lá»‡ timeout, tá»· lá»‡ thÃ nh cÃ´ng, vÃ  cÃ¡c percentile Ä‘á»™ trá»… (p50, p95, p99).
-   **Trá»±c quan hÃ³a (Visualize):** Hiá»ƒn thá»‹ cÃ¡c chá»‰ sá»‘ nÃ y trÃªn má»™t dashboard dá»… hiá»ƒu, giÃºp team nhanh chÃ³ng náº¯m báº¯t tÃ¬nh hÃ¬nh sá»©c khá»e cá»§a há»‡ thá»‘ng.
-   **Cáº£nh bÃ¡o (Alert):** Gá»­i cáº£nh bÃ¡o khi cÃ¡c chá»‰ sá»‘ vÆ°á»£t ngÆ°á»¡ng cho phÃ©p (vi pháº¡m SLO).

---

## 2. CÃ¡c KhÃ¡i niá»‡m Cá»‘t lÃµi: SLI, SLO, vÃ  SLA

Äá»ƒ xÃ¢y dá»±ng má»™t há»‡ thá»‘ng giÃ¡m sÃ¡t chuyÃªn nghiá»‡p, chÃºng ta cáº§n Ã¡p dá»¥ng framework SRE (Site Reliability Engineering) cá»§a Google, báº¯t Ä‘áº§u vá»›i SLI, SLO, vÃ  SLA.

| KhÃ¡i niá»‡m | Äá»‹nh nghÄ©a | VÃ­ dá»¥ cho bÃ i toÃ¡n Timeout | CÃ´ng thá»©c |
| :--- | :--- | :--- | :--- |
| **SLI (Service Level Indicator)** | **Chá»‰ sá»‘ Ä‘o lÆ°á»ng** hiá»‡u suáº¥t thá»±c táº¿ cá»§a dá»‹ch vá»¥. | Tá»· lá»‡ cÃ¡c API requests thÃ nh cÃ´ng trong vÃ²ng 8 giÃ¢y. | `(Sá»‘ request < 8s) / (Tá»•ng sá»‘ request)` |
| **SLO (Service Level Objective)** | **Má»¥c tiÃªu** mÃ  báº¡n Ä‘áº·t ra cho SLI. | **99%** cÃ¡c API requests pháº£i hoÃ n thÃ nh trong vÃ²ng 8 giÃ¢y. | `SLI >= 99%` |
| **SLA (Service Level Agreement)** | **Cam káº¿t** vá»›i khÃ¡ch hÃ ng, cÃ³ rÃ ng buá»™c phÃ¡p lÃ½/tÃ i chÃ­nh. | Náº¿u SLO khÃ´ng Ä‘áº¡t, khÃ¡ch hÃ ng sáº½ Ä‘Æ°á»£c giáº£m giÃ¡ dá»‹ch vá»¥. | Thá»a thuáº­n trong há»£p Ä‘á»“ng |

**Error Budget (NgÃ¢n sÃ¡ch lá»—i):**

Error Budget lÃ  lÆ°á»£ng lá»—i Ä‘Æ°á»£c phÃ©p xáº£y ra mÃ  khÃ´ng vi pháº¡m SLO. NÃ³ cho phÃ©p team cÃ¢n báº±ng giá»¯a viá»‡c phÃ¡t triá»ƒn tÃ­nh nÄƒng má»›i vÃ  viá»‡c duy trÃ¬ sá»± á»•n Ä‘á»‹nh.

-   **CÃ´ng thá»©c:** `Error Budget = 100% - SLO`
-   **VÃ­ dá»¥:** Vá»›i SLO lÃ  99%, Error Budget lÃ  `1%`.
-   Trong 1000 API calls, báº¡n Ä‘Æ°á»£c phÃ©p cÃ³ `1000 * 1% = 10` calls bá»‹ timeout.

Viá»‡c visualize chÃ­nh lÃ  Ä‘á»ƒ theo dÃµi SLI vÃ  Error Budget nÃ y má»™t cÃ¡ch trá»±c quan.

---

## 3. PhÃ¢n tÃ­ch vÃ  So sÃ¡nh cÃ¡c Giáº£i phÃ¡p Ká»¹ thuáº­t

ChÃºng tÃ´i Ä‘Ã£ thá»±c hiá»‡n deep research 8 giáº£i phÃ¡p kháº£ thi, tá»« cÃ¡c cÃ´ng cá»¥ mÃ£ nguá»“n má»Ÿ Ä‘áº¿n cÃ¡c dá»‹ch vá»¥ thÆ°Æ¡ng máº¡i.

| Giáº£i phÃ¡p | Chi phÃ­ | CÃ i Ä‘áº·t | Kháº£ nÄƒng Má»Ÿ rá»™ng | TÃ¹y biáº¿n | ÄÃ¡nh giÃ¡ cho BÃ i toÃ¡n | LÃ½ do |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **1. Prometheus + Grafana** | ğŸ† **Miá»…n phÃ­** | Trung bÃ¬nh | Cao | **Ráº¥t cao** | â­â­â­â­â­ | **Lá»±a chá»n tá»‘t nháº¥t.** MÃ£ nguá»“n má»Ÿ, tiÃªu chuáº©n ngÃ nh, linh hoáº¡t nháº¥t. |
| **2. OpenTelemetry + Backend** | ğŸ† **Miá»…n phÃ­** | Trung bÃ¬nh | Cao | **Ráº¥t cao** | â­â­â­â­â­ | **Kiáº¿n trÃºc tÆ°Æ¡ng lai.** TiÃªu chuáº©n chung, khÃ´ng bá»‹ vendor lock-in. |
| **3. Datadog APM** | Ráº¥t Ä‘áº¯t | Dá»… | Ráº¥t cao | Trung bÃ¬nh | â­â­â­â­ | Dá»… sá»­ dá»¥ng, máº¡nh máº½ nhÆ°ng chi phÃ­ cao vÃ  bá»‹ lock-in. |
| **4. New Relic APM** | Ráº¥t Ä‘áº¯t | Dá»… | Ráº¥t cao | Trung bÃ¬nh | â­â­â­â­ | TÆ°Æ¡ng tá»± Datadog, phÃ¹ há»£p cho doanh nghiá»‡p lá»›n. |
| **5. LGTM Stack (Grafana)** | ğŸ† **Miá»…n phÃ­** | KhÃ³ | Cao | Ráº¥t cao | â­â­â­â­ | Bá»™ cÃ´ng cá»¥ hoÃ n chá»‰nh cá»§a Grafana nhÆ°ng phá»©c táº¡p Ä‘á»ƒ váº­n hÃ nh. |
| **6. Custom In-App Metrics** | ğŸ† **Miá»…n phÃ­** | Trung bÃ¬nh | Trung bÃ¬nh | Ráº¥t cao | â­â­â­ | Linh hoáº¡t nhÆ°ng tá»‘n cÃ´ng xÃ¢y dá»±ng vÃ  báº£o trÃ¬ tá»« Ä‘áº§u. |
| **7. Elasticsearch Stack** | Miá»…n phÃ­/Äáº¯t | KhÃ³ | Ráº¥t cao | Cao | â­â­â­ | Máº¡nh vá» log vÃ  search, nhÆ°ng phá»©c táº¡p hÆ¡n cho metrics. |
| **8. Langfuse (Hiá»‡n cÃ³)** | Trung bÃ¬nh | Dá»… | Cao | Trung bÃ¬nh | â­â­â­ | **NÃªn táº­n dá»¥ng, nhÆ°ng khÃ´ng Ä‘á»§.** Ráº¥t tá»‘t cho LLM trace, nhÆ°ng khÃ´ng pháº£i cÃ´ng cá»¥ chuyÃªn cho general API monitoring. |

### **Káº¿t luáº­n Lá»±a chá»n:**

Kiáº¿n trÃºc Ä‘Æ°á»£c Ä‘á» xuáº¥t lÃ  **OpenTelemetry (Ä‘á»ƒ instrument) + Prometheus (Ä‘á»ƒ lÆ°u trá»¯) + Grafana (Ä‘á»ƒ visualize)**. ÄÃ¢y lÃ  sá»± káº¿t há»£p máº¡nh máº½, linh hoáº¡t, chi phÃ­ hiá»‡u quáº£ vÃ  theo Ä‘Ãºng tiÃªu chuáº©n ngÃ nh hiá»‡n nay.

-   **OpenTelemetry (OTel):** Cung cáº¥p má»™t bá»™ SDK duy nháº¥t Ä‘á»ƒ thu tháº­p metrics, logs, vÃ  traces. GiÃºp báº¡n khÃ´ng bá»‹ phá»¥ thuá»™c vÃ o má»™t nhÃ  cung cáº¥p backend nÃ o.
-   **Prometheus:** Há»‡ thá»‘ng time-series database máº¡nh máº½ nháº¥t cho viá»‡c lÆ°u trá»¯ metrics. HÃ m `histogram_quantile` lÃ  cÃ´ng cá»¥ hoÃ n háº£o Ä‘á»ƒ tÃ­nh toÃ¡n percentile latency.
-   **Grafana:** CÃ´ng cá»¥ visualize máº¡nh máº½, cÃ³ thá»ƒ táº¡o ra má»i loáº¡i biá»ƒu Ä‘á»“ báº¡n cáº§n.

---

## 4. Kiáº¿n trÃºc Äá» xuáº¥t: OTel + Prometheus + Grafana

ÄÃ¢y lÃ  luá»“ng dá»¯ liá»‡u chi tiáº¿t cá»§a giáº£i phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t:

```mermaid
graph TD
    subgraph "Application Layer (Your FastAPI App)"
        A[API Endpoint] -->|Request| B(Middleware);
        B --> C{Execute Workflow};
        C --> D[Instrumented API Call];
    end

    subgraph "Observability Layer"
        D -- OTel SDK --> E(Metrics Collector);
        E -- Histogram --> F[workflow_api_duration_seconds];
        E -- Counter --> G[workflow_api_timeout_total];
        E -- Counter --> H[workflow_api_success_total];
    end

    subgraph "Monitoring Backend"
        I[Prometheus Server] -- Scrape /metrics --> E;
        I -- Stores Time Series Data --> J(Prometheus TSDB);
        K[Alertmanager] -- Reads Rules --> I;
        K -- Fires Alert --> L((Slack/PagerDuty));
    end

    subgraph "Visualization Layer"
        M[Grafana Server] -- Queries (PromQL) --> I;
        M --> N(User Browser);
        style N fill:#fff,stroke:#333,stroke-width:2px
    end

    style A fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
    style M fill:#ff9,stroke:#333,stroke-width:2px
```

**Luá»“ng hoáº¡t Ä‘á»™ng:**

1.  **Instrumentation:** Khi má»™t request Ä‘áº¿n FastAPI app, má»™t middleware sá»­ dá»¥ng **OpenTelemetry SDK** sáº½ Ä‘Æ°á»£c kÃ­ch hoáº¡t.
2.  **Metrics Collection:** Middleware nÃ y sáº½ Ä‘o thá»i gian báº¯t Ä‘áº§u vÃ  káº¿t thÃºc cá»§a request. NÃ³ sáº½ ghi nháº­n káº¿t quáº£ (thÃ nh cÃ´ng, timeout, hay lá»—i) vÃ  thá»i gian xá»­ lÃ½ vÃ o cÃ¡c `metrics` (Histogram vÃ  Counter).
3.  **Scraping:** **Prometheus server** sáº½ Ä‘á»‹nh ká»³ (vÃ­ dá»¥: 15 giÃ¢y má»™t láº§n) gá»i Ä‘áº¿n endpoint `/metrics` cá»§a app Ä‘á»ƒ "cÃ o" (scrape) cÃ¡c sá»‘ liá»‡u má»›i nháº¥t.
4.  **Storage:** Prometheus lÆ°u trá»¯ cÃ¡c sá»‘ liá»‡u nÃ y dÆ°á»›i dáº¡ng time-series data.
5.  **Visualization:** **Grafana** Ä‘Æ°á»£c káº¿t ná»‘i vá»›i Prometheus lÃ m data source. NgÆ°á»i dÃ¹ng táº¡o cÃ¡c dashboard trong Grafana báº±ng cÃ¡ch sá»­ dá»¥ng ngÃ´n ngá»¯ truy váº¥n **PromQL** Ä‘á»ƒ láº¥y dá»¯ liá»‡u tá»« Prometheus vÃ  váº½ biá»ƒu Ä‘á»“.
6.  **Alerting:** Prometheus Ä‘Ã¡nh giÃ¡ cÃ¡c luáº­t (alerting rules) Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a sáºµn. Náº¿u má»™t Ä‘iá»u kiá»‡n vi pháº¡m SLO Ä‘Æ°á»£c phÃ¡t hiá»‡n (vÃ­ dá»¥: tá»· lá»‡ timeout > 1% trong 5 phÃºt), nÃ³ sáº½ gá»­i cáº£nh bÃ¡o Ä‘áº¿n **Alertmanager**, vÃ  Alertmanager sáº½ Ä‘áº©y thÃ´ng bÃ¡o Ä‘áº¿n Slack hoáº·c PagerDuty.

---

## 5. HÆ°á»›ng dáº«n Triá»ƒn khai Chi tiáº¿t (Step-by-Step)

### BÆ°á»›c 1: CÃ i Ä‘áº·t ThÆ° viá»‡n

```bash
# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
pip install opentelemetry-sdk opentelemetry-api \
           opentelemetry-exporter-prometheus fastapi uvicorn
```

### BÆ°á»›c 2: Instrument Code trong FastAPI App

Táº¡o má»™t file `monitoring.py` Ä‘á»ƒ quáº£n lÃ½ táº¥t cáº£ logic vá» metrics.

**`monitoring.py`**
```python
from opentelemetry import metrics
from opentelemetry.exporter.prometheus import PrometheusMetricReader
from opentelemetry.sdk.metrics import MeterProvider
from prometheus_client import start_http_server
import time
import asyncio

# 1. Khá»Ÿi táº¡o Prometheus Exporter
# Sáº½ táº¡o ra má»™t endpoint /metrics trÃªn port 8000
start_http_server(port=8000, addr=\'0.0.0.0\')

# 2. Khá»Ÿi táº¡o MeterProvider
reader = PrometheusMetricReader()
provider = MeterProvider(metric_readers=[reader])
metrics.set_meter_provider(provider)
meter = provider.get_meter("robot-workflow-app", "1.0")

# 3. Äá»‹nh nghÄ©a cÃ¡c Metrics
# Histogram Ä‘á»ƒ Ä‘o phÃ¢n phá»‘i Ä‘á»™ trá»…
HTTP_SERVER_REQUEST_DURATION = meter.create_histogram(
    name="http.server.request.duration_seconds",
    description="Duration of HTTP server requests in seconds",
    unit="s"
)

# Counter Ä‘á»ƒ Ä‘áº¿m tá»•ng sá»‘ request
HTTP_SERVER_ACTIVE_REQUESTS = meter.create_up_down_counter(
    name="http.server.active_requests",
    description="Number of active HTTP server requests",
    unit="requests"
)

# Counter cho cÃ¡c loáº¡i status code
HTTP_SERVER_REQUESTS_BY_STATUS = meter.create_counter(
    name="http.server.requests_by_status_total",
    description="Total number of HTTP requests by status code"
)

# Counter cho cÃ¡c request timeout
WORKFLOW_TIMEOUT_TOTAL = meter.create_counter(
    name="workflow.timeout_total",
    description="Total number of workflow timeouts"
)

# 4. Táº¡o Middleware cho FastAPI
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request

class OTelMetricsMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # CÃ¡c thuá»™c tÃ­nh chung cho táº¥t cáº£ metrics
        common_attrs = { "http.method": request.method, "http.url": str(request.url) }
        
        HTTP_SERVER_ACTIVE_REQUESTS.add(1, common_attrs)
        start_time = time.time()
        
        response = None
        status_code = 500 # Máº·c Ä‘á»‹nh lÃ  lá»—i server
        
        try:
            # Äáº·t timeout cho toÃ n bá»™ request á»Ÿ Ä‘Ã¢y
            response = await asyncio.wait_for(call_next(request), timeout=8.0)
            status_code = response.status_code
        except asyncio.TimeoutError:
            # Ghi nháº­n timeout
            WORKFLOW_TIMEOUT_TOTAL.add(1, common_attrs)
            status_code = 408 # Request Timeout
            # á» Ä‘Ã¢y báº¡n cÃ³ thá»ƒ táº¡o response lá»—i tÃ¹y chá»‰nh
            from starlette.responses import JSONResponse
            response = JSONResponse({"error": "Request timed out after 8 seconds"}, status_code=408)
        except Exception as e:
            # Ghi nháº­n cÃ¡c lá»—i khÃ¡c
            status_code = 500
            raise e
        finally:
            duration = time.time() - start_time
            HTTP_SERVER_ACTIVE_REQUESTS.add(-1, common_attrs)
            
            # Ghi nháº­n duration vÃ o histogram
            attrs = {**common_attrs, "http.status_code": status_code}
            HTTP_SERVER_REQUEST_DURATION.record(duration, attrs)
            
            # Äáº¿m sá»‘ lÆ°á»£ng request theo status code
            HTTP_SERVER_REQUESTS_BY_STATUS.add(1, attrs)

        return response
```

**Cáº­p nháº­t `main.py` cá»§a báº¡n:**

```python
from fastapi import FastAPI
from .monitoring import OTelMetricsMiddleware

app = FastAPI()

# ThÃªm middleware vÃ o app
app.add_middleware(OTelMetricsMiddleware)

@app.get("/")
async def root():
    # Logic cá»§a báº¡n á»Ÿ Ä‘Ã¢y
    await asyncio.sleep(0.1) # Giáº£ láº­p cÃ´ng viá»‡c
    return {"message": "Hello World"}
```

### BÆ°á»›c 3: Cáº¥u hÃ¬nh Prometheus

Táº¡o file `prometheus.yml`:

```yaml
# prometheus.yml
global:
  scrape_interval: 15s # CÃ o metrics má»—i 15 giÃ¢y

scrape_configs:
  - job_name: "robot-workflow"
    static_configs:
      - targets: ["app:8000"] # "app" lÃ  tÃªn service cá»§a FastAPI app trong Docker Compose
```

### BÆ°á»›c 4: Cáº¥u hÃ¬nh Grafana Dashboard

Sau khi cÃ i Ä‘áº·t Grafana vÃ  káº¿t ná»‘i vá»›i Prometheus, báº¡n cÃ³ thá»ƒ táº¡o má»™t dashboard má»›i vÃ  thÃªm cÃ¡c panel vá»›i cÃ¡c truy váº¥n PromQL sau:

**1. Tá»· lá»‡ Timeout (%) - Sá»­ dá»¥ng Gauge Panel**

```promql
# TÃ­nh tá»· lá»‡ request cÃ³ status 408 (Request Timeout)
sum(rate(http_server_requests_by_status_total{http_status_code="408"}[5m]))
/
sum(rate(http_server_request_duration_seconds_count[5m]))
* 100
```

**2. Sá»‘ lÆ°á»£ng Timeout trong 1000 requests gáº§n nháº¥t - Sá»­ dá»¥ng Stat Panel**

```promql
# Äáº¿m sá»‘ láº§n timeout trong khoáº£ng thá»i gian cá»§a 1000 requests gáº§n nháº¥t
# Giáº£ sá»­ trung bÃ¬nh 1 request/giÃ¢y -> 1000s
increase(workflow_timeout_total[1000s])
```

**3. PhÃ¢n phá»‘i Äá»™ trá»… - Sá»­ dá»¥ng Heatmap Panel**

```promql
# Hiá»ƒn thá»‹ phÃ¢n phá»‘i Ä‘á»™ trá»… cá»§a cÃ¡c request thÃ nh cÃ´ng
sum(rate(http_server_request_duration_seconds_bucket{http_status_code=~"2.."}[5m])) by (le)
```

**4. Latency Percentiles (p99, p95, p50) - Sá»­ dá»¥ng Time Series Panel**

```promql
// P99 Latency
histogram_quantile(0.99, sum(rate(http_server_request_duration_seconds_bucket[5m])) by (le))

// P95 Latency
histogram_quantile(0.95, sum(rate(http_server_request_duration_seconds_bucket[5m])) by (le))

// P50 Latency (Median)
histogram_quantile(0.50, sum(rate(http_server_request_duration_seconds_bucket[5m])) by (le))
```

**5. Tá»· lá»‡ Lá»—i (Error Rate %) - Sá»­ dá»¥ng Gauge Panel**

```promql
# TÃ­nh tá»· lá»‡ cÃ¡c request cÃ³ status 5xx
sum(rate(http_server_requests_by_status_total{http_status_code=~"5.."}[5m]))
/
sum(rate(http_server_request_duration_seconds_count[5m]))
* 100
```

---

## 6. Thiáº¿t káº¿ Dashboard Trá»±c quan

Má»™t dashboard hiá»‡u quáº£ cáº§n tráº£ lá»i cÃ¢u há»i nhanh chÃ³ng. DÆ°á»›i Ä‘Ã¢y lÃ  má»™t layout máº«u.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¤– Robot Workflow - API Performance Dashboard                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚      Success Rate (SLO)    â”‚  â”‚      Timeout Rate        â”‚  â”‚   Error Rate      â”‚ â”‚
â”‚  â”‚          99.5%             â”‚  â”‚          0.3%            â”‚  â”‚      0.2%         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚      P99 Latency           â”‚  â”‚      P95 Latency         â”‚  â”‚   Avg Latency     â”‚ â”‚
â”‚  â”‚          1.2s              â”‚  â”‚          450ms           â”‚  â”‚      180ms        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚  Latency Distribution (Heatmap) - Last 6 Hours                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ >8s  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ â”‚
â”‚  â”‚ 5-8s â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ â”‚
â”‚  â”‚ 2-5s â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ â”‚
â”‚  â”‚ <2s  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚  Latency Trends (P99, P95, P50) - Last 24 Hours                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                              â”‚ â”‚
â”‚  â”‚  P99 ----------------- /\ -------------------------------------------------- â”‚ â”‚
â”‚  â”‚                     /  \                                                      â”‚ â”‚
â”‚  â”‚  P95 -------------- /    \ ------------------------------------------------- â”‚ â”‚
â”‚  â”‚                  /      \                                                     â”‚ â”‚
â”‚  â”‚  P50 -----------/--------\-------------------------------------------------- â”‚ â”‚
â”‚  â”‚                                                                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Giáº£i thÃ­ch cÃ¡c Panel:**

1.  **Key Metrics (Stat Panels):** CÃ¡c con sá»‘ quan trá»ng nháº¥t Ä‘Æ°á»£c Ä‘áº·t á»Ÿ trÃªn cÃ¹ng: Tá»· lá»‡ thÃ nh cÃ´ng (so vá»›i SLO), tá»· lá»‡ timeout, tá»· lá»‡ lá»—i, vÃ  cÃ¡c percentile Ä‘á»™ trá»…. GiÃºp nhÃ¬n nhanh ra váº¥n Ä‘á».
2.  **Latency Heatmap:** ÄÃ¢y lÃ  panel quan trá»ng nháº¥t Ä‘á»ƒ visualize sá»± phÃ¢n phá»‘i. Má»—i Ã´ vuÃ´ng biá»ƒu thá»‹ má»™t khoáº£ng thá»i gian (trá»¥c X) vÃ  má»™t khoáº£ng Ä‘á»™ trá»… (trá»¥c Y). MÃ u sáº¯c cá»§a Ã´ cÃ ng Ä‘áº­m, sá»‘ lÆ°á»£ng request rÆ¡i vÃ o khoáº£ng Ä‘Ã³ cÃ ng nhiá»u. Báº¡n cÃ³ thá»ƒ ngay láº­p tá»©c tháº¥y náº¿u cÃ³ má»™t lÆ°á»£ng lá»›n request bá»‹ cháº­m Ä‘i (cÃ¡c Ã´ á»Ÿ hÃ ng trÃªn cao Ä‘áº­m lÃªn).
3.  **Latency Trends:** Biá»ƒu Ä‘á»“ Ä‘Æ°á»ng cho tháº¥y xu hÆ°á»›ng cá»§a cÃ¡c percentile theo thá»i gian. Náº¿u Ä‘Æ°á»ng P99 tÄƒng Ä‘á»™t biáº¿n, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  cÃ¡c request cháº­m nháº¥t Ä‘ang trá»Ÿ nÃªn cháº­m hÆ¡n ná»¯a, má»™t dáº¥u hiá»‡u cá»§a váº¥n Ä‘á» tiá»m áº©n.

---

## 7. Chiáº¿n lÆ°á»£c Cáº£nh bÃ¡o (Alerting)

Visualizing lÃ  Ä‘á»ƒ con ngÆ°á»i xem, nhÆ°ng alerting lÃ  Ä‘á»ƒ mÃ¡y mÃ³c tá»± Ä‘á»™ng thÃ´ng bÃ¡o. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c luáº­t cáº£nh bÃ¡o quan trá»ng cáº§n cÃ³ trong Prometheus.

**`alert.rules.yml`**
```yaml
groups:
  - name: workflow_api_slo_alerts
    rules:
      - alert: HighTimeoutRateWarning
        expr: (sum(rate(http_server_requests_by_status_total{http_status_code="408"}[5m])) / sum(rate(http_server_request_duration_seconds_count[5m]))) > 0.02
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Tá»· lá»‡ timeout vÆ°á»£t ngÆ°á»¡ng 2%"
          description: "Tá»· lá»‡ timeout hiá»‡n táº¡i lÃ  {{ $value | humanizePercentage }}. Cáº§n kiá»ƒm tra há»‡ thá»‘ng."

      - alert: HighTimeoutRateCritical
        expr: (sum(rate(http_server_requests_by_status_total{http_status_code="408"}[5m])) / sum(rate(http_server_request_duration_seconds_count[5m]))) > 0.05
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "VI PHáº M SLO! Tá»· lá»‡ timeout vÆ°á»£t 5%"
          description: "Tá»· lá»‡ timeout hiá»‡n táº¡i lÃ  {{ $value | humanizePercentage }}. Error budget Ä‘ang bá»‹ Ä‘á»‘t chÃ¡y nhanh chÃ³ng."

      - alert: HighP99Latency
        expr: histogram_quantile(0.99, sum(rate(http_server_request_duration_seconds_bucket[5m])) by (le)) > 6
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "P99 latency cao báº¥t thÆ°á»ng (> 6s)"
          description: "99% request Ä‘ang máº¥t hÆ¡n 6 giÃ¢y Ä‘á»ƒ xá»­ lÃ½. Nguy cÆ¡ vi pháº¡m SLO timeout 8s."
```

---

## 8. Káº¿t luáº­n vÃ  CÃ¡c bÆ°á»›c Tiáº¿p theo

Báº±ng cÃ¡ch triá»ƒn khai kiáº¿n trÃºc **OpenTelemetry + Prometheus + Grafana**, báº¡n sáº½ cÃ³ má»™t há»‡ thá»‘ng giÃ¡m sÃ¡t vÃ  trá»±c quan hÃ³a timeout metrics máº¡nh máº½, linh hoáº¡t vÃ  theo Ä‘Ãºng tiÃªu chuáº©n ngÃ nh.

**Lá»™ trÃ¬nh triá»ƒn khai:**

1.  **Tuáº§n 1:** Triá»ƒn khai `monitoring.py` vá»›i OpenTelemetry SDK vÃ o FastAPI app. CÃ i Ä‘áº·t Prometheus vÃ  cáº¥u hÃ¬nh Ä‘á»ƒ scrape metrics.
2.  **Tuáº§n 2:** CÃ i Ä‘áº·t Grafana, káº¿t ná»‘i Prometheus, vÃ  xÃ¢y dá»±ng phiÃªn báº£n Ä‘áº§u tiÃªn cá»§a dashboard vá»›i cÃ¡c panel chÃ­nh.
3.  **Tuáº§n 3:** Tinh chá»‰nh dashboard, thÃªm cÃ¡c biá»ƒu Ä‘á»“ chi tiáº¿t hÆ¡n. CÃ i Ä‘áº·t Alertmanager vÃ  cáº¥u hÃ¬nh cÃ¡c luáº­t cáº£nh bÃ¡o cÆ¡ báº£n.
4.  **Tuáº§n 4:** TÃ­ch há»£p Alertmanager vá»›i Slack/PagerDuty. Review vÃ  tá»‘i Æ°u hÃ³a toÃ n bá»™ há»‡ thá»‘ng.

Vá»›i há»‡ thá»‘ng nÃ y, báº¡n khÃ´ng chá»‰ biáº¿t *khi nÃ o* cÃ³ lá»—i, mÃ  cÃ²n cÃ³ Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ hiá»ƒu *táº¡i sao* lá»—i xáº£y ra, giÃºp viá»‡c debug vÃ  cáº£i thiá»‡n hiá»‡u nÄƒng há»‡ thá»‘ng trá»Ÿ nÃªn dá»… dÃ ng hÆ¡n ráº¥t nhiá»u.

---

## 9. Phá»¥ lá»¥c: Tham kháº£o

1.  [Prometheus Documentation: Histograms and Summaries](https://prometheus.io/docs/practices/histograms/)
2.  [Grafana Documentation: Heatmap Visualization](https://grafana.com/docs/grafana/latest/visualizations/panels-visualizations/visualizations/heatmap/)
3.  [OpenTelemetry Documentation: Python SDK](https://opentelemetry-python.readthedocs.io/)
4.  [Google SRE Book: Service Level Objectives](https://sre.google/sre-book/service-level-objectives/)
5.  [New Relic Blog: SLOs, SLIs, and SLAs](https://newrelic.com/blog/best-practices/what-are-slos-slis-slas)

