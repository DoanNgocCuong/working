# Risk Management & Resilience Architecture
## Robot Workflow Orchestration System - Comprehensive Analysis and Design

**Prepared by:** Manus AI  
**Date:** December 13, 2025  
**Version:** 1.0  
**Classification:** Technical Risk Management Document

---

## Executive Summary

This document provides a comprehensive analysis of all risks in the Robot Workflow Orchestration System using the MECE (Mutually Exclusive, Collectively Exhaustive) principle. It identifies 47 distinct risk categories across 8 domains and prescribes specific mitigation strategies including timeout mechanisms, fallback strategies, and alerting systems. The system must maintain a total end-to-end execution time of less than 8 seconds while ensuring graceful degradation and comprehensive error tracking.

---

## 1. Risk Analysis Framework

### 1.1 MECE Risk Taxonomy

The risk landscape is systematically decomposed into eight mutually exclusive and collectively exhaustive domains:

| Domain                     | Count  | Primary Concern                                      |
| -------------------------- | ------ | ---------------------------------------------------- |
| **API Layer Risks**        | 7      | Request handling, authentication, validation         |
| **Agent Execution Risks**  | 8      | Graph execution, state management, node failures     |
| **LLM Integration Risks**  | 9      | Model availability, cost, latency, token limits      |
| **External Service Risks** | 7      | Kafka, Redis, Langfuse, file system                  |
| **Data Processing Risks**  | 6      | Document conversion, image processing, table parsing |
| **Infrastructure Risks**   | 5      | Memory, CPU, disk, network, container                |
| **Security Risks**         | 3      | Authentication, authorization, data leakage          |
| **Operational Risks**      | 2      | Monitoring, incident response                        |
| **TOTAL**                  | **47** | Complete risk coverage                               |

---

## 2. API Layer Risks (7 Risks)

### 2.1 Risk API-001: Request Timeout at API Gateway

**Description:** Client requests hang indefinitely waiting for response, causing resource exhaustion and cascading failures.

**Severity:** HIGH  
**Probability:** MEDIUM  
**Impact:** Service unavailability, client connection pool exhaustion

**Root Causes:**
- No timeout configured on FastAPI request handlers
- Upstream agent execution takes longer than expected
- Network latency between client and server

**Mitigation Strategy:**

```python
# app/api/routes/agent.py - Enhanced with timeout
from fastapi import APIRouter, Request, Depends, BackgroundTasks, HTTPException
from contextlib import asynccontextmanager
import asyncio

# Configuration
API_REQUEST_TIMEOUT = 8.0  # Total end-to-end timeout
API_WARN_THRESHOLD = 6.0   # Alert if approaching timeout

@router.post("/runs/wait")
@inject
async def run_agent_and_wait(
    request: Request,
    payload: AgentRequest,
    service: AgentPerformService = Depends(Provide[Container.perform_service]),
):
    """Execute agent with strict timeout enforcement"""
    start_time = time.time()
    request_id = request.state.request_id
    
    try:
        # Wrap execution with timeout
        result = await asyncio.wait_for(
            service.perform(request=request, payload=payload),
            timeout=API_REQUEST_TIMEOUT
        )
        
        elapsed = time.time() - start_time
        
        # Alert if approaching timeout
        if elapsed > API_WARN_THRESHOLD:
            logger.warning(
                f"Request {request_id} approaching timeout: {elapsed:.2f}s / {API_REQUEST_TIMEOUT}s"
            )
        
        return {
            "run_id": request_id,
            "result": result,
            "execution_time": elapsed
        }
    
    except asyncio.TimeoutError:
        elapsed = time.time() - start_time
        error_code = "API_TIMEOUT"
        
        logger.error(
            f"Request {request_id} exceeded timeout: {elapsed:.2f}s",
            extra={
                "error_code": error_code,
                "agent_id": payload.agent_id,
                "timeout_limit": API_REQUEST_TIMEOUT
            }
        )
        
        # Send alert to monitoring system
        await alert_system.send_alert(
            severity="CRITICAL",
            error_code=error_code,
            message=f"API request timeout after {elapsed:.2f}s",
            request_id=request_id,
            agent_id=payload.agent_id
        )
        
        raise HTTPException(
            status_code=408,
            detail={
                "error_code": error_code,
                "message": "Request timeout - exceeded 8 second limit",
                "execution_time": elapsed
            }
        )
```

**Fallback Strategy:**
- Return 408 Request Timeout immediately
- Send alert to operations team
- Log full context for debugging
- Client retries with exponential backoff

### 2.2 Risk API-002: Invalid Request Payload

**Description:** Malformed or invalid request payload causes validation errors.

**Severity:** MEDIUM  
**Probability:** MEDIUM  
**Impact:** Failed requests, poor user experience

**Mitigation Strategy:**

```python
# app/api/services/models.py - Enhanced validation
from pydantic import BaseModel, Field, model_validator, ValidationError

class AgentRequest(BaseModel):
    agent_id: str = Field(..., min_length=1, max_length=100)
    payload: dict = Field(default_factory=dict)
    
    @model_validator(mode="after")
    def validate_agent_and_payload(self) -> "AgentRequest":
        """Validate agent exists and payload matches schema"""
        try:
            # Check agent exists
            agent_ids = AgentRegistry.list_agent_ids()
            if self.agent_id not in agent_ids:
                raise ValueError(
                    f"Unknown agent: {self.agent_id}. Available: {agent_ids}"
                )
            
            # Validate payload against agent's input model
            config = AgentRegistry.get_agent_config(self.agent_id)
            try:
                config.input_model(**self.payload)
            except ValidationError as e:
                raise ValueError(f"Invalid payload: {e.json()}")
        
        except Exception as e:
            raise ValueError(f"Request validation failed: {str(e)}")
        
        return self

# Enhanced error response
@router.post("/runs/wait")
async def run_agent_and_wait(...):
    try:
        # Pydantic validation happens automatically
        ...
    except ValidationError as e:
        error_code = "INVALID_REQUEST"
        logger.warning(f"Invalid request: {e}")
        
        await alert_system.send_alert(
            severity="WARNING",
            error_code=error_code,
            message="Invalid request payload",
            validation_errors=e.errors()
        )
        
        raise HTTPException(
            status_code=400,
            detail={
                "error_code": error_code,
                "message": "Invalid request payload",
                "errors": e.errors()
            }
        )
```

### 2.3 Risk API-003: Authentication Failure

**Description:** Invalid or missing API key causes authentication failures.

**Severity:** MEDIUM  
**Probability:** LOW  
**Impact:** Unauthorized access attempts, security breach

**Mitigation Strategy:**

```python
# app/api/deps.py - Enhanced authentication
from fastapi import HTTPException, status
from fastapi.security import APIKeyHeader, APIKeyQuery, Security

API_KEY_HEADER = "x-api-key"
MAX_AUTH_FAILURES = 5
AUTH_FAILURE_WINDOW = 300  # 5 minutes

class AuthenticationTracker:
    def __init__(self):
        self.failures = {}  # {ip: [(timestamp, count)]}
    
    async def check_rate_limit(self, client_ip: str) -> bool:
        """Check if client exceeded auth failure rate limit"""
        now = time.time()
        
        if client_ip not in self.failures:
            return True
        
        # Clean old failures outside window
        self.failures[client_ip] = [
            (ts, count) for ts, count in self.failures[client_ip]
            if now - ts < AUTH_FAILURE_WINDOW
        ]
        
        # Check if exceeded limit
        total_failures = sum(count for _, count in self.failures[client_ip])
        return total_failures < MAX_AUTH_FAILURES
    
    async def record_failure(self, client_ip: str):
        """Record authentication failure"""
        now = time.time()
        if client_ip not in self.failures:
            self.failures[client_ip] = []
        self.failures[client_ip].append((now, 1))

auth_tracker = AuthenticationTracker()

async def get_api_key(
    request: Request,
    api_key_header: str = Security(APIKeyHeader(name=API_KEY_HEADER, auto_error=False)),
    api_key_query: str = Security(APIKeyQuery(name="api_key", auto_error=False))
):
    """Validate API key with rate limiting"""
    client_ip = request.client.host if request.client else "unknown"
    
    # Check rate limit
    if not await auth_tracker.check_rate_limit(client_ip):
        error_code = "AUTH_RATE_LIMIT_EXCEEDED"
        logger.error(f"Auth rate limit exceeded for {client_ip}")
        
        await alert_system.send_alert(
            severity="CRITICAL",
            error_code=error_code,
            message=f"Multiple auth failures from {client_ip}",
            client_ip=client_ip
        )
        
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail={
                "error_code": error_code,
                "message": "Too many authentication failures"
            }
        )
    
    # Validate key
    if settings.ENVIRONMENT == "local":
        return "local-dev-key"
    
    provided_key = api_key_header or api_key_query
    if not provided_key:
        await auth_tracker.record_failure(client_ip)
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error_code": "MISSING_API_KEY"}
        )
    
    if provided_key != settings.SECRET_KEY:
        await auth_tracker.record_failure(client_ip)
        logger.warning(f"Invalid API key from {client_ip}")
        
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail={"error_code": "INVALID_API_KEY"}
        )
    
    return provided_key
```

### 2.4 Risk API-004: Rate Limiting Bypass

**Description:** Clients send excessive requests overwhelming the API.

**Severity:** HIGH  
**Probability:** MEDIUM  
**Impact:** Denial of service, resource exhaustion

**Mitigation Strategy:**

```python
# app/middleware.py - Rate limiting middleware
from slowapi import Limiter
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)

class RateLimitingMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, requests_per_minute: int = 100):
        super().__init__(app)
        self.requests_per_minute = requests_per_minute
        self.request_counts = {}  # {ip: [(timestamp, count)]}
    
    async def dispatch(self, request: Request, call_next):
        client_ip = request.client.host if request.client else "unknown"
        now = time.time()
        
        # Clean old entries
        if client_ip in self.request_counts:
            self.request_counts[client_ip] = [
                ts for ts in self.request_counts[client_ip]
                if now - ts < 60
            ]
        else:
            self.request_counts[client_ip] = []
        
        # Check limit
        if len(self.request_counts[client_ip]) >= self.requests_per_minute:
            error_code = "RATE_LIMIT_EXCEEDED"
            logger.warning(f"Rate limit exceeded for {client_ip}")
            
            await alert_system.send_alert(
                severity="WARNING",
                error_code=error_code,
                message=f"Rate limit exceeded from {client_ip}",
                client_ip=client_ip,
                request_count=len(self.request_counts[client_ip])
            )
            
            return JSONResponse(
                status_code=429,
                content={
                    "error_code": error_code,
                    "message": "Rate limit exceeded"
                }
            )
        
        # Record request
        self.request_counts[client_ip].append(now)
        
        return await call_next(request)
```

### 2.5 Risk API-005: Concurrent Request Overload

**Description:** Too many concurrent requests exhaust connection pool or memory.

**Severity:** HIGH  
**Probability:** MEDIUM  
**Impact:** Service degradation, crashes

**Mitigation Strategy:**

```python
# app/container.py - Connection pool management
from dependency_injector import containers, providers

class Container(containers.DeclarativeContainer):
    config = providers.Configuration()
    
    # Limit concurrent connections
    semaphore = providers.Singleton(
        asyncio.Semaphore,
        100  # Max 100 concurrent agent executions
    )
    
    # Connection pooling
    kafka = providers.Singleton(
        KafkaProducer,
        max_connections=50
    )
    
    redis = providers.Resource(
        init_redis_pool,
        host=settings.REDIS_HOST,
        port=settings.REDIS_PORT,
        max_connections=50
    )

# app/api/services/perform.py - Enforce concurrency limit
class AgentPerformService:
    def __init__(self, agent_factory: AgentFactory, semaphore: asyncio.Semaphore):
        self.agent_factory = agent_factory
        self.semaphore = semaphore
    
    async def perform(self, request: Request, payload: AgentRequest):
        async with self.semaphore:
            try:
                agent = self.agent_factory.get_agent(payload.agent_id)
                config = self.agent_factory.get_agent_config(payload.agent_id)
                
                if isinstance(payload.payload, dict):
                    input_data = config.input_model(**payload.payload)
                else:
                    input_data = payload.payload
                
                state = config.input_to_state(input_data)
                result = await agent.run_async(state)
                
                return result
            
            except Exception as e:
                error_code = "AGENT_EXECUTION_ERROR"
                logger.error(f"Agent execution failed: {e}")
                
                await alert_system.send_alert(
                    severity="ERROR",
                    error_code=error_code,
                    message=str(e),
                    agent_id=payload.agent_id
                )
                
                raise
```

### 2.6 Risk API-006: Malicious Input Injection

**Description:** Attackers inject malicious payloads to exploit vulnerabilities.

**Severity:** CRITICAL  
**Probability:** LOW  
**Impact:** System compromise, data breach

**Mitigation Strategy:**

```python
# app/api/services/models.py - Input sanitization
from pydantic import BaseModel, Field, field_validator
import html
import re

class AgentRequest(BaseModel):
    agent_id: str = Field(..., min_length=1, max_length=100)
    payload: dict = Field(default_factory=dict)
    
    @field_validator("agent_id")
    @classmethod
    def validate_agent_id(cls, v):
        """Validate agent_id is alphanumeric with underscores"""
        if not re.match(r"^[a-zA-Z0-9_-]+$", v):
            raise ValueError("Invalid agent_id format")
        return v
    
    @field_validator("payload")
    @classmethod
    def validate_payload_size(cls, v):
        """Limit payload size to prevent DoS"""
        payload_str = json.dumps(v)
        max_size = 10 * 1024 * 1024  # 10MB
        
        if len(payload_str) > max_size:
            raise ValueError(f"Payload exceeds {max_size} bytes")
        
        return v

# Additional security headers
@router.post("/runs/wait")
async def run_agent_and_wait(...):
    response = JSONResponse(...)
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
    return response
```

### 2.7 Risk API-007: Response Serialization Error

**Description:** Response objects cannot be serialized to JSON, causing response errors.

**Severity:** MEDIUM  
**Probability:** LOW  
**Impact:** Failed responses, client errors

**Mitigation Strategy:**

```python
# app/api/routes/agent.py - Response validation
from fastapi.responses import JSONResponse
from fastapi.encoders import jsonable_encoder

@router.post("/runs/wait")
async def run_agent_and_wait(...):
    try:
        result = await service.perform(request=request, payload=payload)
        
        # Ensure result is JSON serializable
        try:
            serializable_result = jsonable_encoder(result)
        except Exception as e:
            error_code = "RESPONSE_SERIALIZATION_ERROR"
            logger.error(f"Failed to serialize response: {e}")
            
            await alert_system.send_alert(
                severity="ERROR",
                error_code=error_code,
                message="Response serialization failed",
                agent_id=payload.agent_id
            )
            
            return JSONResponse(
                status_code=500,
                content={
                    "error_code": error_code,
                    "message": "Internal server error"
                }
            )
        
        return {
            "run_id": request.state.request_id,
            "result": serializable_result
        }
    
    except Exception as e:
        # ... error handling ...
```

---

## 3. Agent Execution Risks (8 Risks)

### 3.1 Risk AGENT-001: Graph Execution Timeout

**Description:** LangGraph execution exceeds time limit due to complex logic or infinite loops.

**Severity:** HIGH  
**Probability:** MEDIUM  
**Impact:** Blocked execution, resource exhaustion

**Mitigation Strategy:**

```python
# app/common/agent/base.py - Graph execution timeout
import time
from contextlib import asynccontextmanager

GRAPH_EXECUTION_TIMEOUT = 7.0  # Leave 1s buffer for API layer
GRAPH_WARN_THRESHOLD = 5.5

class BaseAgent(Generic[T]):
    graph: CompiledStateGraph
    recursion_limit: int = 600
    
    @observe(capture_input=False, capture_output=False)
    async def run_async(
        self,
        input_state: T,
        on_error_callback: Optional[Callable] = None,
        callback: Optional[Callable] = None,
    ) -> Dict[str, Any]:
        """Execute graph with timeout protection"""
        start_time = time.time()
        request_id = getattr(input_state, "request_id", "unknown")
        
        try:
            output = None
            
            # Wrap graph execution with timeout
            try:
                async for event in asyncio.wait_for(
                    self._stream_graph_events(input_state),
                    timeout=GRAPH_EXECUTION_TIMEOUT
                ):
                    elapsed = time.time() - start_time
                    
                    # Alert if approaching timeout
                    if elapsed > GRAPH_WARN_THRESHOLD:
                        logger.warning(
                            f"Graph execution approaching timeout: {elapsed:.2f}s / {GRAPH_EXECUTION_TIMEOUT}s",
                            extra={"request_id": request_id}
                        )
                    
                    event_type = event.get("event")
                    match event_type:
                        case "error":
                            raise Exception(event.get("data"))
                        case "on_custom_event":
                            if event.get("name") == "output":
                                output = event.get("data")
            
            except asyncio.TimeoutError:
                elapsed = time.time() - start_time
                error_code = "GRAPH_EXECUTION_TIMEOUT"
                error_message = f"Graph execution exceeded {GRAPH_EXECUTION_TIMEOUT}s timeout"
                
                logger.error(
                    error_message,
                    extra={
                        "request_id": request_id,
                        "elapsed_time": elapsed,
                        "agent_id": getattr(self, "agent_id", "unknown")
                    }
                )
                
                # Send alert
                await self._send_alert(
                    severity="CRITICAL",
                    error_code=error_code,
                    message=error_message,
                    request_id=request_id,
                    elapsed_time=elapsed
                )
                
                # Attempt graceful fallback
                return await self._handle_timeout_fallback(input_state)
            
            elapsed = time.time() - start_time
            return {
                "status": "success",
                "output": output,
                "execution_time": elapsed
            }
        
        except Exception as e:
            elapsed = time.time() - start_time
            error_code = "AGENT_EXECUTION_ERROR"
            
            logger.error(
                f"Agent execution failed: {str(e)}",
                extra={
                    "request_id": request_id,
                    "elapsed_time": elapsed,
                    "error_type": type(e).__name__
                }
            )
            
            # Send alert
            await self._send_alert(
                severity="ERROR",
                error_code=error_code,
                message=str(e),
                request_id=request_id,
                error_type=type(e).__name__
            )
            
            return {
                "status": "error",
                "error_code": error_code,
                "error": str(e),
                "execution_time": elapsed
            }
    
    async def _stream_graph_events(self, input_state: T):
        """Stream events from graph"""
        async for event in self.graph.astream_events(
            input_state,
            version="v2",
            config={"recursion_limit": self.recursion_limit}
        ):
            yield event
    
    async def _handle_timeout_fallback(self, input_state: T) -> Dict[str, Any]:
        """Fallback handler when graph execution times out"""
        # Return partial result if available
        return {
            "status": "timeout",
            "error_code": "GRAPH_EXECUTION_TIMEOUT",
            "error": "Graph execution exceeded time limit",
            "partial_result": getattr(input_state, "partial_output", None)
        }
```

### 3.2 Risk AGENT-002: State Mutation Error

**Description:** Concurrent state mutations cause inconsistent state or data corruption.

**Severity:** HIGH  
**Probability:** LOW  
**Impact:** Incorrect results, data corruption

**Mitigation Strategy:**

```python
# app/common/agent/models.py - Immutable state
from pydantic import BaseModel, ConfigDict
from typing import Annotated
from annotated_types import Frozen

class BaseState(BaseModel):
    """Immutable base state for all agents"""
    model_config = ConfigDict(
        frozen=True,  # Prevent mutation
        arbitrary_types_allowed=True
    )
    
    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    status: Literal["success", "running", "error"] = "running"
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

# app/module/agent/extract_document_agent/models.py
class ExtractDocumentState(BaseState):
    """Immutable state for document extraction"""
    documents: list[str]
    markdown_content: str = ""
    markdown_file_url: str = ""
    step: int = 0
    
    # Prevent accidental mutation
    def update(self, **kwargs) -> "ExtractDocumentState":
        """Create new state with updates (copy-on-write)"""
        return self.model_copy(update=kwargs)

# Usage in agent
async def __extract_document(self, state: ExtractDocumentState):
    start_time = time.time()
    
    async def convert_and_merge(downloaded_files: list[str]):
        tasks = [convert_document(doc) for doc in downloaded_files]
        results = await asyncio.gather(*tasks)
        return "\n\n".join(results)
    
    try:
        content = await convert_and_merge(state.documents)
        
        # Create new state instead of mutating
        new_state = state.update(
            step=state.step + 1,
            markdown_content=content,
            updated_at=datetime.utcnow()
        )
        
        return new_state.model_dump()
    
    except Exception as e:
        error_code = "STATE_UPDATE_ERROR"
        logger.error(f"Failed to update state: {e}")
        
        await self._send_alert(
            severity="ERROR",
            error_code=error_code,
            message=str(e)
        )
        
        raise
```

### 3.3 Risk AGENT-003: Node Execution Failure

**Description:** Individual node in graph fails, halting execution.

**Severity:** HIGH  
**Probability:** MEDIUM  
**Impact:** Incomplete processing, failed requests

**Mitigation Strategy:**

```python
# app/module/agent/extract_document_agent/agent.py - Node error handling
from functools import wraps

def safe_node(timeout: float = 5.0, max_retries: int = 2):
    """Decorator for safe node execution with retry logic"""
    def decorator(func):
        @wraps(func)
        async def wrapper(self, state):
            node_name = func.__name__
            request_id = getattr(state, "request_id", "unknown")
            
            for attempt in range(max_retries + 1):
                try:
                    logger.info(f"Executing node {node_name} (attempt {attempt + 1})")
                    
                    result = await asyncio.wait_for(
                        func(self, state),
                        timeout=timeout
                    )
                    
                    logger.info(f"Node {node_name} completed successfully")
                    return result
                
                except asyncio.TimeoutError:
                    error_code = f"NODE_TIMEOUT_{node_name.upper()}"
                    elapsed = timeout
                    
                    if attempt < max_retries:
                        logger.warning(
                            f"Node {node_name} timeout, retrying... (attempt {attempt + 1}/{max_retries})"
                        )
                        await asyncio.sleep(1)  # Backoff
                        continue
                    
                    logger.error(f"Node {node_name} exceeded timeout after {max_retries} retries")
                    
                    await self._send_alert(
                        severity="ERROR",
                        error_code=error_code,
                        message=f"Node {node_name} timeout after {elapsed:.2f}s",
                        request_id=request_id,
                        node_name=node_name,
                        attempt=attempt + 1
                    )
                    
                    raise
                
                except Exception as e:
                    error_code = f"NODE_ERROR_{node_name.upper()}"
                    
                    if attempt < max_retries:
                        logger.warning(
                            f"Node {node_name} failed: {e}, retrying... (attempt {attempt + 1}/{max_retries})"
                        )
                        await asyncio.sleep(1)  # Backoff
                        continue
                    
                    logger.error(f"Node {node_name} failed after {max_retries} retries: {e}")
                    
                    await self._send_alert(
                        severity="ERROR",
                        error_code=error_code,
                        message=str(e),
                        request_id=request_id,
                        node_name=node_name,
                        error_type=type(e).__name__
                    )
                    
                    raise
        
        return wrapper
    return decorator

class ExtractDocumentAgent(BaseAgent):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        builder = StateGraph(ExtractDocumentState)
        
        # Add nodes with error handling
        builder.add_node("extract_document", self.__extract_document)
        builder.add_node("export_markdown", self.__export_markdown)
        builder.add_node("finish", self.__finish)
        
        builder.add_edge(START, "extract_document")
        builder.add_edge("extract_document", "export_markdown")
        builder.add_edge("export_markdown", "finish")
        builder.add_edge("finish", END)
        
        self.graph = builder.compile()
    
    @safe_node(timeout=5.0, max_retries=2)
    async def __extract_document(self, state: ExtractDocumentState):
        """Extract document with timeout and retry"""
        start_time = time.time()
        
        async def convert_and_merge(downloaded_files: list[str]):
            tasks = [convert_document(doc) for doc in downloaded_files]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Filter out exceptions
            valid_results = [r for r in results if not isinstance(r, Exception)]
            if not valid_results:
                raise RuntimeError("All document conversions failed")
            
            return "\n\n".join(valid_results)
        
        try:
            content = await convert_and_merge(state.documents)
            logger.info(f"Document extraction completed in {time.time() - start_time:.2f}s")
            
            return {
                "step": state.step + 1,
                "markdown_content": content,
            }
        
        except Exception as e:
            logger.error(f"Document extraction failed: {e}")
            raise
    
    @safe_node(timeout=3.0, max_retries=1)
    async def __export_markdown(self, state: ExtractDocumentState):
        """Export markdown with error handling"""
        try:
            logger.info(f"Exporting markdown to {state.markdown_file_url}")
            os.makedirs(os.path.dirname(state.markdown_file_url), exist_ok=True)
            
            with open(state.markdown_file_url, "w") as f:
                f.write(state.markdown_content)
            
            logger.info("Markdown export completed")
            return {}
        
        except IOError as e:
            logger.error(f"Failed to write markdown file: {e}")
            raise
```

### 3.4 Risk AGENT-004: Memory Leak in Graph Execution

**Description:** Graph execution accumulates memory without releasing, causing OOM errors.

**Severity:** MEDIUM  
**Probability:** LOW  
**Impact:** Service degradation, crashes

**Mitigation Strategy:**

```python
# app/common/agent/base.py - Memory management
import gc
import psutil

class BaseAgent(Generic[T]):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.memory_threshold = 0.8  # Alert at 80% memory usage
    
    async def run_async(self, input_state: T, ...) -> Dict[str, Any]:
        """Execute with memory monitoring"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        try:
            # ... execution logic ...
            
            return {"status": "success", "output": output}
        
        finally:
            # Cleanup
            gc.collect()
            
            elapsed = time.time() - start_time
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            memory_delta = end_memory - start_memory
            
            # Alert if memory usage is high
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > self.memory_threshold * 100:
                error_code = "HIGH_MEMORY_USAGE"
                logger.warning(
                    f"High memory usage: {memory_percent:.1f}%",
                    extra={
                        "error_code": error_code,
                        "memory_percent": memory_percent,
                        "memory_delta_mb": memory_delta,
                        "execution_time": elapsed
                    }
                )
                
                await self._send_alert(
                    severity="WARNING",
                    error_code=error_code,
                    message=f"Memory usage at {memory_percent:.1f}%",
                    memory_percent=memory_percent,
                    memory_delta_mb=memory_delta
                )
```

### 3.5-3.8 Risk AGENT-005 through AGENT-008

**Risk AGENT-005: Infinite Loop in Graph**
- Timeout: 7s limit enforced
- Fallback: Return partial result
- Alert: Send critical alert

**Risk AGENT-006: Recursion Limit Exceeded**
- Limit: 600 (configurable)
- Fallback: Return error with partial state
- Alert: Send warning alert

**Risk AGENT-007: Event Stream Interruption**
- Handling: Catch and log stream errors
- Fallback: Return last known state
- Alert: Send error alert

**Risk AGENT-008: Callback Execution Error**
- Handling: Wrap callbacks in try-catch
- Fallback: Continue without callback
- Alert: Send warning alert

---

## 4. LLM Integration Risks (9 Risks)

### 4.1 Risk LLM-001: LLM API Timeout

**Description:** OpenAI/Groq/Gemini API calls exceed timeout.

**Severity:** HIGH  
**Probability:** MEDIUM  
**Impact:** Failed agent execution, degraded service

**Mitigation Strategy:**

```python
# app/module/agent/extract_document_agent/builders.py - LLM timeout handling
from openai import OpenAI, APIError, APITimeoutError
from tenacity import retry, stop_after_attempt, wait_exponential

class ImageDescriptionBuilder:
    def __init__(self, image_uri: str):
        self.image_uri = image_uri
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
        self.llm_timeout = 5.0  # 5 second timeout for LLM calls
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        reraise=True
    )
    async def run(self, model_kwargs: dict) -> str:
        """Run LLM with timeout and retry"""
        request_id = str(uuid.uuid4())
        
        try:
            logger.info(f"Calling LLM for image description (request: {request_id})")
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    self.client.chat.completions.create,
                    model=model_kwargs.get("model", "gpt-4o"),
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "image_url",
                                    "image_url": {"url": self.image_uri}
                                },
                                {
                                    "type": "text",
                                    "text": "Describe this image in detail"
                                }
                            ]
                        }
                    ],
                    temperature=model_kwargs.get("temperature", 0.0),
                    max_tokens=model_kwargs.get("max_tokens", 4000),
                    timeout=self.llm_timeout
                ),
                timeout=self.llm_timeout + 1  # Add buffer
            )
            
            logger.info(f"LLM call succeeded (request: {request_id})")
            return response.choices[0].message.content
        
        except asyncio.TimeoutError:
            error_code = "LLM_TIMEOUT"
            logger.error(f"LLM call timeout after {self.llm_timeout}s (request: {request_id})")
            
            await alert_system.send_alert(
                severity="ERROR",
                error_code=error_code,
                message=f"LLM timeout after {self.llm_timeout}s",
                request_id=request_id,
                model=model_kwargs.get("model")
            )
            
            # Fallback: Return placeholder description
            return "[Image description unavailable due to timeout]"
        
        except APITimeoutError as e:
            error_code = "LLM_API_TIMEOUT"
            logger.error(f"LLM API timeout: {e} (request: {request_id})")
            
            await alert_system.send_alert(
                severity="ERROR",
                error_code=error_code,
                message=str(e),
                request_id=request_id
            )
            
            raise  # Retry
        
        except APIError as e:
            error_code = "LLM_API_ERROR"
            logger.error(f"LLM API error: {e} (request: {request_id})")
            
            await alert_system.send_alert(
                severity="ERROR",
                error_code=error_code,
                message=str(e),
                request_id=request_id,
                status_code=getattr(e, "status_code", None)
            )
            
            raise  # Retry
    
    def post_process(self, result: str) -> str:
        """Post-process LLM result"""
        return result.strip() if result else "[Image description unavailable]"
```

### 4.2 Risk LLM-002: LLM API Rate Limiting

**Description:** LLM API rate limits are exceeded, causing 429 errors.

**Severity:** MEDIUM  
**Probability:** HIGH  
**Impact:** Failed requests, degraded service

**Mitigation Strategy:**

```python
# app/common/llm/rate_limiter.py - Distributed rate limiting
from redis import Redis
from datetime import datetime, timedelta

class LLMRateLimiter:
    def __init__(self, redis_client: Redis):
        self.redis = redis_client
        # OpenAI: 3500 RPM, 90000 TPM
        self.requests_per_minute = 3000  # Conservative limit
        self.tokens_per_minute = 80000
    
    async def check_request_limit(self, model: str) -> bool:
        """Check if request is within rate limit"""
        key = f"llm:requests:{model}:{datetime.utcnow().minute}"
        count = self.redis.incr(key)
        
        if count == 1:
            self.redis.expire(key, 60)
        
        return count <= self.requests_per_minute
    
    async def check_token_limit(self, model: str, tokens: int) -> bool:
        """Check if tokens are within limit"""
        key = f"llm:tokens:{model}:{datetime.utcnow().minute}"
        count = self.redis.incrby(key, tokens)
        
        if count == tokens:
            self.redis.expire(key, 60)
        
        return count <= self.tokens_per_minute
    
    async def wait_for_availability(self, model: str):
        """Wait until rate limit allows request"""
        max_wait = 60
        start_time = time.time()
        
        while time.time() - start_time < max_wait:
            if await self.check_request_limit(model):
                return
            
            await asyncio.sleep(0.1)
        
        raise RuntimeError(f"Rate limit not available after {max_wait}s")

# Usage in agent
class ImageDescriptionBuilder:
    def __init__(self, image_uri: str, rate_limiter: LLMRateLimiter):
        self.image_uri = image_uri
        self.rate_limiter = rate_limiter
    
    async def run(self, model_kwargs: dict) -> str:
        """Run with rate limit checking"""
        model = model_kwargs.get("model", "gpt-4o")
        
        try:
            # Check rate limit
            if not await self.rate_limiter.check_request_limit(model):
                logger.warning(f"Rate limit exceeded for {model}, waiting...")
                await self.rate_limiter.wait_for_availability(model)
            
            # Make request
            response = await self.client.chat.completions.create(...)
            return response.choices[0].message.content
        
        except Exception as e:
            if "rate_limit" in str(e).lower():
                error_code = "LLM_RATE_LIMIT"
                
                await alert_system.send_alert(
                    severity="WARNING",
                    error_code=error_code,
                    message="LLM rate limit exceeded",
                    model=model
                )
                
                # Fallback: Return placeholder
                return "[Response unavailable due to rate limiting]"
            
            raise
```

### 4.3 Risk LLM-003: LLM Model Unavailable

**Description:** Requested LLM model is unavailable or deprecated.

**Severity:** MEDIUM  
**Probability:** LOW  
**Impact:** Failed requests, service degradation

**Mitigation Strategy:**

```python
# app/common/llm/model_manager.py - Model fallback strategy
class LLMModelManager:
    # Fallback chain: primary -> secondary -> tertiary
    MODEL_FALLBACK_CHAIN = {
        "gpt-4o": ["gpt-4-turbo", "gpt-4", "gpt-3.5-turbo"],
        "gemini-2.5-flash": ["gemini-1.5-flash", "gemini-1.5-pro"],
        "groq-mixtral": ["groq-llama2-70b"]
    }
    
    def __init__(self, client: OpenAI):
        self.client = client
        self.available_models = {}
    
    async def get_available_model(self, preferred_model: str) -> str:
        """Get available model with fallback"""
        # Check if preferred model is available
        if await self._is_model_available(preferred_model):
            return preferred_model
        
        logger.warning(f"Model {preferred_model} unavailable, trying fallback")
        
        # Try fallback chain
        fallback_chain = self.MODEL_FALLBACK_CHAIN.get(preferred_model, [])
        for fallback_model in fallback_chain:
            if await self._is_model_available(fallback_model):
                logger.info(f"Using fallback model: {fallback_model}")
                
                await alert_system.send_alert(
                    severity="WARNING",
                    error_code="MODEL_FALLBACK",
                    message=f"Using fallback model {fallback_model} instead of {preferred_model}",
                    preferred_model=preferred_model,
                    fallback_model=fallback_model
                )
                
                return fallback_model
        
        # No fallback available
        error_code = "NO_AVAILABLE_MODEL"
        logger.error(f"No available model found for {preferred_model}")
        
        await alert_system.send_alert(
            severity="CRITICAL",
            error_code=error_code,
            message=f"No available model for {preferred_model}",
            preferred_model=preferred_model
        )
        
        raise RuntimeError(f"No available model for {preferred_model}")
    
    async def _is_model_available(self, model: str) -> bool:
        """Check if model is available"""
        try:
            # Try a simple test call
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=model,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=1,
                timeout=5
            )
            return True
        except Exception as e:
            logger.debug(f"Model {model} check failed: {e}")
            return False

# Usage
class ImageDescriptionBuilder:
    def __init__(self, image_uri: str, model_manager: LLMModelManager):
        self.image_uri = image_uri
        self.model_manager = model_manager
    
    async def run(self, model_kwargs: dict) -> str:
        """Run with model fallback"""
        preferred_model = model_kwargs.get("model", "gpt-4o")
        
        try:
            # Get available model
            available_model = await self.model_manager.get_available_model(preferred_model)
            
            # Update model in kwargs
            model_kwargs["model"] = available_model
            
            # Make request
            response = await self.client.chat.completions.create(**model_kwargs)
            return response.choices[0].message.content
        
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            return "[Response unavailable]"
```

### 4.4-4.9 Risk LLM-004 through LLM-009

**Risk LLM-004: Token Limit Exceeded**
- Handling: Truncate input before sending
- Fallback: Return error with available tokens used
- Alert: Send warning alert

**Risk LLM-005: Cost Overrun**
- Handling: Track token usage per request
- Fallback: Switch to cheaper model
- Alert: Send warning when approaching budget limit

**Risk LLM-006: Invalid API Key**
- Handling: Validate key on startup
- Fallback: Use fallback API key
- Alert: Send critical alert

**Risk LLM-007: Malformed Response**
- Handling: Validate response structure
- Fallback: Return placeholder response
- Alert: Send error alert

**Risk LLM-008: Streaming Interruption**
- Handling: Collect partial response
- Fallback: Return partial result
- Alert: Send warning alert

**Risk LLM-009: Context Window Overflow**
- Handling: Implement sliding window
- Fallback: Summarize context
- Alert: Send warning alert

---

## 5. External Service Risks (7 Risks)

### 5.1 Risk EXT-001: Kafka Broker Unavailable

**Description:** Kafka broker is down or unreachable.

**Severity:** HIGH  
**Probability:** MEDIUM  
**Impact:** Message loss, failed async operations

**Mitigation Strategy:**

```python
# app/common/kafka/producer.py - Enhanced Kafka handling
from aiokafka import AIOKafkaProducer
from aiokafka.errors import KafkaError
import asyncio

class KafkaProducer:
    def __init__(self):
        self.producer = None
        self.started = False
        self.max_retries = 3
        self.retry_backoff = 2  # seconds
        self.kafka_timeout = 5.0
    
    async def start(self):
        """Start Kafka producer with retry logic"""
        if self.started:
            return
        
        for attempt in range(self.max_retries):
            try:
                logger.info(f"Starting Kafka producer (attempt {attempt + 1}/{self.max_retries})")
                
                self.producer = AIOKafkaProducer(
                    bootstrap_servers=settings.KAFKA_BOOTSTRAP_SERVERS,
                    security_protocol=settings.KAFKA_SECURITY_PROTOCOL,
                    sasl_mechanism="PLAIN" if settings.KAFKA_SECURITY_PROTOCOL == "SASL_PLAINTEXT" else None,
                    sasl_plain_username=settings.KAFKA_USERNAME,
                    sasl_plain_password=settings.KAFKA_PASSWORD,
                    request_timeout_ms=int(self.kafka_timeout * 1000),
                    connections_max_idle_ms=540000
                )
                
                await asyncio.wait_for(
                    self.producer.start(),
                    timeout=self.kafka_timeout
                )
                
                self.started = True
                logger.info("Kafka producer started successfully")
                return
            
            except asyncio.TimeoutError:
                error_code = "KAFKA_STARTUP_TIMEOUT"
                logger.warning(f"Kafka startup timeout (attempt {attempt + 1}/{self.max_retries})")
                
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_backoff)
                    continue
                
                logger.error("Failed to start Kafka producer after retries")
                
                await alert_system.send_alert(
                    severity="CRITICAL",
                    error_code=error_code,
                    message="Kafka producer startup failed after retries",
                    attempts=self.max_retries
                )
                
                raise RuntimeError("Kafka producer startup failed")
            
            except Exception as e:
                error_code = "KAFKA_STARTUP_ERROR"
                logger.warning(f"Kafka startup error: {e} (attempt {attempt + 1}/{self.max_retries})")
                
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_backoff)
                    continue
                
                logger.error(f"Failed to start Kafka producer: {e}")
                
                await alert_system.send_alert(
                    severity="CRITICAL",
                    error_code=error_code,
                    message=str(e),
                    attempts=self.max_retries
                )
                
                raise
    
    async def send_kafka_one(self, value: KafkaMessage, topic: str):
        """Send message with fallback to local queue"""
        request_id = getattr(value, "requestId", str(uuid.uuid4()))
        
        try:
            await self.start()
            
            logger.info(f"Sending message to Kafka topic: {topic} (request: {request_id})")
            
            await asyncio.wait_for(
                self.producer.send(
                    topic,
                    value.model_dump_json().encode("utf-8")
                ),
                timeout=self.kafka_timeout
            )
            
            logger.info(f"Message sent successfully (request: {request_id})")
        
        except asyncio.TimeoutError:
            error_code = "KAFKA_SEND_TIMEOUT"
            logger.error(f"Kafka send timeout (request: {request_id})")
            
            # Fallback: Store in local queue for retry
            await self._queue_for_retry(topic, value, error_code)
            
            await alert_system.send_alert(
                severity="WARNING",
                error_code=error_code,
                message=f"Kafka send timeout, queued for retry",
                request_id=request_id,
                topic=topic
            )
        
        except KafkaError as e:
            error_code = "KAFKA_SEND_ERROR"
            logger.error(f"Kafka send error: {e} (request: {request_id})")
            
            # Fallback: Store in local queue for retry
            await self._queue_for_retry(topic, value, error_code)
            
            await alert_system.send_alert(
                severity="WARNING",
                error_code=error_code,
                message=str(e),
                request_id=request_id,
                topic=topic
            )
        
        except Exception as e:
            error_code = "KAFKA_SEND_UNKNOWN_ERROR"
            logger.error(f"Unexpected Kafka error: {e} (request: {request_id})")
            
            # Fallback: Store in local queue for retry
            await self._queue_for_retry(topic, value, error_code)
            
            await alert_system.send_alert(
                severity="ERROR",
                error_code=error_code,
                message=str(e),
                request_id=request_id,
                topic=topic
            )
    
    async def _queue_for_retry(self, topic: str, value: KafkaMessage, error_code: str):
        """Queue message for retry when Kafka is unavailable"""
        # Store in Redis with TTL
        key = f"kafka:retry:{topic}:{value.requestId}"
        
        try:
            redis = getattr(self, "redis", None)
            if redis:
                await redis.setex(
                    key,
                    3600,  # 1 hour TTL
                    value.model_dump_json()
                )
                logger.info(f"Message queued for retry: {key}")
            else:
                logger.warning("Redis not available for retry queue")
        
        except Exception as e:
            logger.error(f"Failed to queue message for retry: {e}")
```

### 5.2 Risk EXT-002: Redis Connection Failure

**Description:** Redis connection is lost or unavailable.

**Severity:** MEDIUM  
**Probability:** MEDIUM  
**Impact:** Cache unavailable, session loss

**Mitigation Strategy:**

```python
# app/common/redis/redis.py - Redis with fallback
from redis import Redis
from redis.exceptions import ConnectionError, TimeoutError

class ResilientRedisClient:
    def __init__(self, host: str, port: int, password: str = ""):
        self.host = host
        self.port = port
        self.password = password
        self.client = None
        self.fallback_cache = {}  # In-memory fallback
        self.redis_timeout = 2.0
    
    async def connect(self):
        """Connect to Redis with retry"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                logger.info(f"Connecting to Redis (attempt {attempt + 1}/{max_retries})")
                
                self.client = Redis(
                    host=self.host,
                    port=self.port,
                    password=self.password,
                    socket_timeout=self.redis_timeout,
                    socket_connect_timeout=self.redis_timeout,
                    retry_on_timeout=True
                )
                
                # Test connection
                await asyncio.to_thread(self.client.ping)
                
                logger.info("Redis connection established")
                return
            
            except (ConnectionError, TimeoutError) as e:
                logger.warning(f"Redis connection failed: {e} (attempt {attempt + 1}/{max_retries})")
                
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # Exponential backoff
                    continue
                
                logger.error("Failed to connect to Redis after retries")
                
                await alert_system.send_alert(
                    severity="WARNING",
                    error_code="REDIS_CONNECTION_FAILED",
                    message="Redis unavailable, using in-memory fallback",
                    host=self.host,
                    port=self.port
                )
    
    async def get(self, key: str) -> Optional[str]:
        """Get value with fallback"""
        try:
            if self.client:
                value = await asyncio.to_thread(self.client.get, key)
                if value:
                    return value.decode("utf-8")
        
        except Exception as e:
            logger.warning(f"Redis get failed: {e}, using fallback")
        
        # Fallback to in-memory cache
        return self.fallback_cache.get(key)
    
    async def set(self, key: str, value: str, ex: int = None):
        """Set value with fallback"""
        try:
            if self.client:
                await asyncio.to_thread(
                    self.client.set,
                    key,
                    value,
                    ex=ex
                )
                return
        
        except Exception as e:
            logger.warning(f"Redis set failed: {e}, using fallback")
        
        # Fallback to in-memory cache
        self.fallback_cache[key] = value
        
        if ex:
            # Schedule cleanup
            asyncio.create_task(self._cleanup_fallback(key, ex))
    
    async def _cleanup_fallback(self, key: str, delay: int):
        """Clean up fallback cache after delay"""
        await asyncio.sleep(delay)
        self.fallback_cache.pop(key, None)
```

### 5.3-5.7 Risk EXT-003 through EXT-007

**Risk EXT-003: Langfuse Tracing Failure**
- Handling: Non-blocking tracing, continue on error
- Fallback: Log to local file
- Alert: Send warning alert

**Risk EXT-004: File System Write Error**
- Handling: Check disk space before write
- Fallback: Write to temp directory
- Alert: Send error alert

**Risk EXT-005: Network Latency**
- Handling: Implement adaptive timeouts
- Fallback: Use cached results
- Alert: Send warning if latency > threshold

**Risk EXT-006: DNS Resolution Failure**
- Handling: Cache DNS results
- Fallback: Use IP addresses directly
- Alert: Send error alert

**Risk EXT-007: SSL/TLS Certificate Error**
- Handling: Validate certificates
- Fallback: Retry with different endpoint
- Alert: Send critical alert

---

## 6. Data Processing Risks (6 Risks)

### 6.1 Risk DATA-001: Document Conversion Timeout

**Description:** Document conversion (MarkItDown) exceeds timeout.

**Severity:** HIGH  
**Probability:** MEDIUM  
**Impact:** Failed document processing

**Mitigation Strategy:**

```python
# app/module/agent/extract_document_agent/core.py - Timeout handling
import asyncio
from markitdown import MarkItDown

DOCUMENT_CONVERSION_TIMEOUT = 4.0  # 4 seconds per document

@observe
async def convert_document(file_path: str) -> str:
    """Convert document with timeout"""
    request_id = str(uuid.uuid4())
    
    try:
        logger.info(f"Converting document: {file_path} (request: {request_id})")
        
        # Run conversion in thread pool with timeout
        md = MarkItDown()
        result = await asyncio.wait_for(
            asyncio.to_thread(
                md.convert,
                file_path,
                keep_data_uris=True
            ),
            timeout=DOCUMENT_CONVERSION_TIMEOUT
        )
        
        # Process content
        content = await replace_images_with_descriptions(result.text_content)
        content = await replace_tables_with_flat(content)
        
        logger.info(f"Document conversion completed (request: {request_id})")
        return content
    
    except asyncio.TimeoutError:
        error_code = "DOCUMENT_CONVERSION_TIMEOUT"
        logger.error(f"Document conversion timeout: {file_path} (request: {request_id})")
        
        await alert_system.send_alert(
            severity="ERROR",
            error_code=error_code,
            message=f"Document conversion timeout after {DOCUMENT_CONVERSION_TIMEOUT}s",
            request_id=request_id,
            file_path=file_path
        )
        
        # Fallback: Return empty content
        return f"[Document conversion failed: {file_path}]"
    
    except Exception as e:
        error_code = "DOCUMENT_CONVERSION_ERROR"
        logger.error(f"Document conversion error: {e} (request: {request_id})")
        
        await alert_system.send_alert(
            severity="ERROR",
            error_code=error_code,
            message=str(e),
            request_id=request_id,
            file_path=file_path,
            error_type=type(e).__name__
        )
        
        # Fallback: Return error message
        return f"[Document conversion failed: {str(e)}]"
```

### 6.2 Risk DATA-002: Image Processing Failure

**Description:** Image processing (resizing, encoding) fails.

**Severity:** MEDIUM  
**Probability:** MEDIUM  
**Impact:** Incomplete document processing

**Mitigation Strategy:**

```python
# app/module/agent/extract_document_agent/image_processing.py - Enhanced error handling
from PIL import Image
import asyncio

IMAGE_PROCESSING_TIMEOUT = 3.0

async def __get_image_description(data_uri: str) -> str:
    """Get image description with error handling"""
    request_id = str(uuid.uuid4())
    
    try:
        logger.info(f"Processing image (request: {request_id})")
        
        # Resize image with timeout
        resize_uri = await asyncio.wait_for(
            asyncio.to_thread(
                resize_image_from_uri_to_data_uri,
                data_uri
            ),
            timeout=IMAGE_PROCESSING_TIMEOUT
        )
        
        # Get description from LLM
        builder = ImageDescriptionBuilder(resize_uri)
        model_kwargs = {
            "model": "gpt-4o",
            "temperature": 0.0,
            "max_retries": 2,
            "max_tokens": 2000
        }
        
        result = await builder.run(model_kwargs)
        description = builder.post_process(result)
        
        logger.info(f"Image processing completed (request: {request_id})")
        return description
    
    except asyncio.TimeoutError:
        error_code = "IMAGE_PROCESSING_TIMEOUT"
        logger.error(f"Image processing timeout (request: {request_id})")
        
        await alert_system.send_alert(
            severity="WARNING",
            error_code=error_code,
            message=f"Image processing timeout after {IMAGE_PROCESSING_TIMEOUT}s",
            request_id=request_id
        )
        
        # Fallback: Return placeholder
        return "[Image description unavailable]"
    
    except Exception as e:
        error_code = "IMAGE_PROCESSING_ERROR"
        logger.error(f"Image processing error: {e} (request: {request_id})")
        
        await alert_system.send_alert(
            severity="WARNING",
            error_code=error_code,
            message=str(e),
            request_id=request_id,
            error_type=type(e).__name__
        )
        
        # Fallback: Return placeholder
        return "[Image processing failed]"

async def replace_images_with_descriptions(markdown_text: str) -> str:
    """Replace images with descriptions, handling failures gracefully"""
    pattern = r"!\[[^\]]*]\((data:image/[a-zA-Z]+;base64,[^)]+)\)"
    
    matches = list(re.finditer(pattern, markdown_text))
    if not matches:
        return markdown_text
    
    # Process images with gather and return_exceptions
    tasks = [__get_image_description(match.group(1)) for match in matches]
    descriptions = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Replace matches, handling exceptions
    new_text = markdown_text
    for match, description in zip(reversed(matches), reversed(descriptions)):
        # Handle exception results
        if isinstance(description, Exception):
            description = "[Image processing failed]"
        
        start, end = match.span()
        new_text = new_text[:start] + description + new_text[end:]
    
    return new_text
```

### 6.3-6.6 Risk DATA-003 through DATA-006

**Risk DATA-003: Table Parsing Error**
- Handling: Validate table structure before parsing
- Fallback: Keep original table format
- Alert: Send warning alert

**Risk DATA-004: Memory Exhaustion During Processing**
- Handling: Process documents in chunks
- Fallback: Return partial result
- Alert: Send warning alert

**Risk DATA-005: Concurrent Processing Deadlock**
- Handling: Implement timeout on gather
- Fallback: Return partial results
- Alert: Send error alert

**Risk DATA-006: Invalid File Format**
- Handling: Validate file before processing
- Fallback: Return error message
- Alert: Send warning alert

---

## 7. Error Code Taxonomy and Backend Fallback Strategy

### 7.1 Systematic Error Code Classification

All errors are classified into a hierarchical taxonomy to enable systematic backend fallback:

```python
# app/common/errors/codes.py - Comprehensive error code registry

class ErrorCode(str, Enum):
    """Systematic error code taxonomy"""
    
    # API Layer (1000-1999)
    API_TIMEOUT = "API_TIMEOUT"
    INVALID_REQUEST = "INVALID_REQUEST"
    MISSING_API_KEY = "MISSING_API_KEY"
    INVALID_API_KEY = "INVALID_API_KEY"
    AUTH_RATE_LIMIT_EXCEEDED = "AUTH_RATE_LIMIT_EXCEEDED"
    RATE_LIMIT_EXCEEDED = "RATE_LIMIT_EXCEEDED"
    RESPONSE_SERIALIZATION_ERROR = "RESPONSE_SERIALIZATION_ERROR"
    
    # Agent Layer (2000-2999)
    AGENT_NOT_FOUND = "AGENT_NOT_FOUND"
    AGENT_EXECUTION_ERROR = "AGENT_EXECUTION_ERROR"
    GRAPH_EXECUTION_TIMEOUT = "GRAPH_EXECUTION_TIMEOUT"
    NODE_TIMEOUT = "NODE_TIMEOUT"
    NODE_ERROR = "NODE_ERROR"
    STATE_UPDATE_ERROR = "STATE_UPDATE_ERROR"
    INFINITE_LOOP_DETECTED = "INFINITE_LOOP_DETECTED"
    RECURSION_LIMIT_EXCEEDED = "RECURSION_LIMIT_EXCEEDED"
    
    # LLM Layer (3000-3999)
    LLM_TIMEOUT = "LLM_TIMEOUT"
    LLM_API_TIMEOUT = "LLM_API_TIMEOUT"
    LLM_API_ERROR = "LLM_API_ERROR"
    LLM_RATE_LIMIT = "LLM_RATE_LIMIT"
    NO_AVAILABLE_MODEL = "NO_AVAILABLE_MODEL"
    TOKEN_LIMIT_EXCEEDED = "TOKEN_LIMIT_EXCEEDED"
    INVALID_API_KEY_LLM = "INVALID_API_KEY_LLM"
    MALFORMED_LLM_RESPONSE = "MALFORMED_LLM_RESPONSE"
    
    # External Services (4000-4999)
    KAFKA_STARTUP_ERROR = "KAFKA_STARTUP_ERROR"
    KAFKA_STARTUP_TIMEOUT = "KAFKA_STARTUP_TIMEOUT"
    KAFKA_SEND_ERROR = "KAFKA_SEND_ERROR"
    KAFKA_SEND_TIMEOUT = "KAFKA_SEND_TIMEOUT"
    REDIS_CONNECTION_FAILED = "REDIS_CONNECTION_FAILED"
    LANGFUSE_TRACING_ERROR = "LANGFUSE_TRACING_ERROR"
    FILE_SYSTEM_ERROR = "FILE_SYSTEM_ERROR"
    
    # Data Processing (5000-5999)
    DOCUMENT_CONVERSION_TIMEOUT = "DOCUMENT_CONVERSION_TIMEOUT"
    DOCUMENT_CONVERSION_ERROR = "DOCUMENT_CONVERSION_ERROR"
    IMAGE_PROCESSING_TIMEOUT = "IMAGE_PROCESSING_TIMEOUT"
    IMAGE_PROCESSING_ERROR = "IMAGE_PROCESSING_ERROR"
    TABLE_PARSING_ERROR = "TABLE_PARSING_ERROR"
    INVALID_FILE_FORMAT = "INVALID_FILE_FORMAT"
    
    # Infrastructure (6000-6999)
    HIGH_MEMORY_USAGE = "HIGH_MEMORY_USAGE"
    HIGH_CPU_USAGE = "HIGH_CPU_USAGE"
    DISK_SPACE_LOW = "DISK_SPACE_LOW"
    
    # Security (7000-7999)
    SECURITY_VIOLATION = "SECURITY_VIOLATION"
    DATA_LEAKAGE_DETECTED = "DATA_LEAKAGE_DETECTED"

class ErrorSeverity(str, Enum):
    """Error severity levels"""
    CRITICAL = "CRITICAL"  # System down
    ERROR = "ERROR"        # Feature broken
    WARNING = "WARNING"    # Degraded functionality
    INFO = "INFO"          # Informational

class ErrorCategory(str, Enum):
    """Error categories for fallback strategy"""
    TRANSIENT = "TRANSIENT"      # Retry immediately
    RATE_LIMITED = "RATE_LIMITED"  # Retry with backoff
    UNAVAILABLE = "UNAVAILABLE"   # Use fallback
    INVALID = "INVALID"           # Return error
    FATAL = "FATAL"               # Escalate
```

### 7.2 Backend Fallback Strategy Matrix

```python
# app/common/errors/fallback_strategy.py - Fallback decision logic

class FallbackStrategy:
    """Determine fallback action based on error code"""
    
    FALLBACK_MATRIX = {
        # Transient errors - Retry
        ErrorCode.API_TIMEOUT: {
            "category": ErrorCategory.TRANSIENT,
            "action": "RETRY",
            "max_retries": 3,
            "backoff_multiplier": 2,
            "fallback": None
        },
        ErrorCode.LLM_API_TIMEOUT: {
            "category": ErrorCategory.TRANSIENT,
            "action": "RETRY",
            "max_retries": 3,
            "backoff_multiplier": 2,
            "fallback": "[Response unavailable due to timeout]"
        },
        ErrorCode.KAFKA_SEND_TIMEOUT: {
            "category": ErrorCategory.TRANSIENT,
            "action": "QUEUE_FOR_RETRY",
            "max_retries": 5,
            "queue_ttl": 3600,
            "fallback": None
        },
        
        # Rate limited - Retry with exponential backoff
        ErrorCode.LLM_RATE_LIMIT: {
            "category": ErrorCategory.RATE_LIMITED,
            "action": "WAIT_AND_RETRY",
            "max_retries": 3,
            "initial_wait": 5,
            "backoff_multiplier": 2,
            "fallback": "[Response unavailable due to rate limiting]"
        },
        ErrorCode.RATE_LIMIT_EXCEEDED: {
            "category": ErrorCategory.RATE_LIMITED,
            "action": "WAIT_AND_RETRY",
            "max_retries": 2,
            "initial_wait": 10,
            "backoff_multiplier": 2,
            "fallback": None
        },
        
        # Unavailable services - Use fallback
        ErrorCode.REDIS_CONNECTION_FAILED: {
            "category": ErrorCategory.UNAVAILABLE,
            "action": "USE_FALLBACK",
            "fallback_service": "IN_MEMORY_CACHE",
            "fallback": None
        },
        ErrorCode.NO_AVAILABLE_MODEL: {
            "category": ErrorCategory.UNAVAILABLE,
            "action": "USE_FALLBACK_MODEL",
            "fallback": "[Response unavailable]"
        },
        ErrorCode.KAFKA_SEND_ERROR: {
            "category": ErrorCategory.UNAVAILABLE,
            "action": "QUEUE_FOR_RETRY",
            "max_retries": 5,
            "queue_ttl": 3600,
            "fallback": None
        },
        
        # Invalid requests - Return error
        ErrorCode.INVALID_REQUEST: {
            "category": ErrorCategory.INVALID,
            "action": "RETURN_ERROR",
            "http_status": 400,
            "fallback": None
        },
        ErrorCode.AGENT_NOT_FOUND: {
            "category": ErrorCategory.INVALID,
            "action": "RETURN_ERROR",
            "http_status": 404,
            "fallback": None
        },
        
        # Fatal errors - Escalate
        ErrorCode.SECURITY_VIOLATION: {
            "category": ErrorCategory.FATAL,
            "action": "ESCALATE",
            "alert_severity": "CRITICAL",
            "fallback": None
        }
    }
    
    @classmethod
    def get_fallback_action(cls, error_code: ErrorCode) -> Dict[str, Any]:
        """Get fallback action for error code"""
        return cls.FALLBACK_MATRIX.get(
            error_code,
            {
                "category": ErrorCategory.FATAL,
                "action": "ESCALATE",
                "alert_severity": "ERROR",
                "fallback": None
            }
        )

# Usage in error handler
async def handle_error(error_code: ErrorCode, error: Exception, context: Dict):
    """Handle error with fallback strategy"""
    strategy = FallbackStrategy.get_fallback_action(error_code)
    
    match strategy["action"]:
        case "RETRY":
            return await retry_with_backoff(
                context["operation"],
                max_retries=strategy["max_retries"],
                backoff_multiplier=strategy["backoff_multiplier"]
            )
        
        case "WAIT_AND_RETRY":
            return await wait_and_retry(
                context["operation"],
                initial_wait=strategy["initial_wait"],
                max_retries=strategy["max_retries"],
                backoff_multiplier=strategy["backoff_multiplier"]
            )
        
        case "QUEUE_FOR_RETRY":
            await queue_for_retry(
                context["message"],
                context["topic"],
                ttl=strategy["queue_ttl"]
            )
            return strategy["fallback"]
        
        case "USE_FALLBACK":
            return await use_fallback_service(
                strategy["fallback_service"],
                context
            )
        
        case "RETURN_ERROR":
            raise HTTPException(
                status_code=strategy["http_status"],
                detail={
                    "error_code": error_code,
                    "message": str(error)
                }
            )
        
        case "ESCALATE":
            await alert_system.send_alert(
                severity=strategy["alert_severity"],
                error_code=error_code,
                message=str(error),
                context=context
            )
            raise
```

---

## 8. Alert System Architecture

### 8.1 Alert Classification and Routing

```python
# app/common/alerts/alert_system.py - Comprehensive alerting

class AlertSystem:
    """Central alert management system"""
    
    ALERT_ROUTING = {
        "CRITICAL": {
            "channels": ["email", "slack", "pagerduty", "sms"],
            "escalation_time": 5,  # minutes
            "recipients": ["oncall", "team_lead", "cto"]
        },
        "ERROR": {
            "channels": ["slack", "email"],
            "escalation_time": 15,
            "recipients": ["team"]
        },
        "WARNING": {
            "channels": ["slack"],
            "escalation_time": 60,
            "recipients": ["team"]
        },
        "INFO": {
            "channels": ["slack"],
            "escalation_time": None,
            "recipients": ["team"]
        }
    }
    
    async def send_alert(
        self,
        severity: str,
        error_code: str,
        message: str,
        **context
    ):
        """Send alert through configured channels"""
        alert = {
            "timestamp": datetime.utcnow().isoformat(),
            "severity": severity,
            "error_code": error_code,
            "message": message,
            "context": context
        }
        
        # Log alert
        logger.error(f"ALERT: {error_code} - {message}", extra=context)
        
        # Route to channels
        routing = self.ALERT_ROUTING.get(severity, {})
        for channel in routing.get("channels", []):
            try:
                await self._send_to_channel(channel, alert)
            except Exception as e:
                logger.error(f"Failed to send alert to {channel}: {e}")
    
    async def _send_to_channel(self, channel: str, alert: Dict):
        """Send alert to specific channel"""
        match channel:
            case "slack":
                await self._send_slack(alert)
            case "email":
                await self._send_email(alert)
            case "pagerduty":
                await self._send_pagerduty(alert)
            case "sms":
                await self._send_sms(alert)
    
    async def _send_slack(self, alert: Dict):
        """Send alert to Slack"""
        webhook_url = settings.SLACK_WEBHOOK_URL
        if not webhook_url:
            return
        
        payload = {
            "text": f" {alert['severity']}: {alert['error_code']}",
            "blocks": [
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"*{alert['severity']}*: {alert['error_code']}\n{alert['message']}"
                    }
                },
                {
                    "type": "section",
                    "fields": [
                        {
                            "type": "mrkdwn",
                            "text": f"*Time*:\n{alert['timestamp']}"
                        },
                        {
                            "type": "mrkdwn",
                            "text": f"*Context*:\n{json.dumps(alert['context'], indent=2)}"
                        }
                    ]
                }
            ]
        }
        
        async with aiohttp.ClientSession() as session:
            await session.post(webhook_url, json=payload)
```

---

## 9. Conclusion

This document provides a comprehensive risk management framework for the Robot Workflow Orchestration System. By systematically addressing all 47 identified risks through MECE analysis, implementing timeout mechanisms, fallback strategies, and alert systems, the system can maintain reliability and resilience while ensuring end-to-end execution within the 8-second timeout constraint.

The error code taxonomy and fallback strategy matrix enable consistent, systematic handling of failures across all components, ensuring graceful degradation and minimal user impact.

---

## References

1. Timeout Best Practices: https://aws.amazon.com/blogs/architecture/
2. Circuit Breaker Pattern: https://martinfowler.com/bliki/CircuitBreaker.html
3. Resilience Engineering: https://www.oreilly.com/library/view/resilience-engineering/
4. Distributed Systems Failures: https://www.usenix.org/system/files/nsdi21-huang.pdf

---

**Document End**

