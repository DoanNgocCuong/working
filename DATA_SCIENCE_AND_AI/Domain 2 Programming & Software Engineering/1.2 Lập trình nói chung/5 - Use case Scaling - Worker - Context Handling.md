---
### Khi nÃ o nÃªn scale cÃ¡i gÃ¬ ? 

TÃ´i sáº½ giáº£i thÃ­ch theo cÃ¡ch Ä‘Æ¡n giáº£n vÃ  MECE hÆ¡n:



#### ğŸ¯ **Framework ÄÆ¡n Giáº£n: 3 CÃ¢u Há»i Quyáº¿t Äá»‹nh**
---
git commit -m "[Update Táº¢I UNICORN WORKER cho: docker-compose.api - Mong muá»‘n lÃ  tá»« 50 CCU hiá»‡n táº¡i vá»›i 1 event loop => thÃ nh 3-4 Unicorn worker sáº½ giÃºp tÄƒng táº£i lÃªn 100] - WHY? - Khi nÃ o scale worker khi nÃ o pod?

>> ---
>>
>> 1. Request Ä‘á»™c láº­p
>> 2. App nhÃ n/báº­n
>>    **Tá»· lá»‡ thá»i gian (Æ°á»›c tÃ­nh):**
>>
>> - I/O-bound: ~1-1.5s (DB queries, MinIO) = 70-80%
>>   - DB query: 10-50ms (code line 341-343)
>>   - MinIO store: 200-1000ms (code line 455)
>>   - DB insert: 50-200ms (code line 491-493)
>> - CPU-bound: ~0.1-0.5s (transform) = 20-30%
>>   - Transform: 100-500ms (code line 422, 425)
>>
>> **Káº¿t luáº­n: Chá»§ yáº¿u lÃ  NHÃ€N (I/O-bound)**
>> => tÄƒng worker Ä‘á»ƒ tá»‘i Ä‘a CPU
>> 3. App khÃ´ng giá»¯ state trong memory mÃ  Ä‘á»™c láº­p
>>
>> ---
>>
>> => ### Scale WORKERS trÆ°á»›c (Ä‘Ã£ lÃ m)
>>
>> LÃ½ do:
>>
>> 4. I/O-bound chá»§ yáº¿u (70-80% thá»i gian chá»)
>>    - Worker 1 chá» DB â†’ Worker 2 xá»­ lÃ½ request khÃ¡c
>>    - Táº­n dá»¥ng thá»i gian chá»
>> 2. Shared resources (DB connection pool)
>>    - Workers trong cÃ¹ng pod dÃ¹ng chung connection pool
>>    - Hiá»‡u quáº£ hÆ¡n so vá»›i má»—i pod cÃ³ pool riÃªng
>> 3. Startup overhead
>>    - Táº¡o worker má»›i nhanh (vÃ i giÃ¢y)
>>    - Táº¡o pod má»›i cháº­m hÆ¡n (30-60s)
>>
>> Káº¿t quáº£:
>>
>> - 1 worker â†’ 4 workers
>> - Throughput: 50 RPS â†’ 200-400 RPS
>> - CCU: 50 â†’ 100+
>>
>> ---
>>
>> ### BÆ°á»›c 2: Scale PODS sau (khi cáº§n)
>>
>> Khi nÃ o cáº§n scale pods:
>>
>> 1. Workers Ä‘Ã£ max (4-8 workers/pod)
>> 2. CPU > 80% (CPU-bound operations tÄƒng)
>> 3. Memory > 80%
>> 4. Cáº§n high availability (1 pod cháº¿t khÃ´ng áº£nh hÆ°á»Ÿng)"
>>
>



#### **CÃ¢u Há»i 1: App cá»§a báº¡n giá»‘ng NHÃ‚N VIÃŠN hay giá»‘ng CÃ”NG TY?**

###### ğŸ§‘ **NHÃ‚N VIÃŠN (Worker trong Pod)**

```

VÃ­ dá»¥ thá»±c táº¿:

- 1 nhÃ  hÃ ng cÃ³ nhiá»u Ä‘áº§u báº¿p (workers)

- Chung 1 báº¿p (shared memory)

- DÃ¹ng chung nguyÃªn liá»‡u (shared resources)

- LÃ m cÃ¹ng lÃºc nhiá»u mÃ³n (concurrency)

```

**â†’ Scale WORKERS khi:**

- Cáº§n lÃ m nhiá»u viá»‡c cÃ¹ng lÃºc
- Chia sáº» tÃ i nguyÃªn chung (database connection, cache)
- Khá»Ÿi Ä‘á»™ng cháº­m/tá»‘n kÃ©m

###### ğŸ¢ **CÃ”NG TY (Pod)**

```

VÃ­ dá»¥ thá»±c táº¿:

- Nhiá»u nhÃ  hÃ ng khÃ¡c nhau (pods)

- Má»—i nhÃ  hÃ ng Ä‘á»™c láº­p

- 1 nhÃ  hÃ ng Ä‘Ã³ng cá»­a, cÃ¡c nhÃ  hÃ ng khÃ¡c váº«n hoáº¡t Ä‘á»™ng

- KhÃ¡ch Ä‘áº¿n nhÃ  hÃ ng nÃ o cÅ©ng Ä‘Æ°á»£c

```

**â†’ Scale PODS khi:**

- Má»—i request Ä‘á»™c láº­p, khÃ´ng liÃªn quan
- Cáº§n dá»± phÃ²ng khi 1 pod cháº¿t
- Cáº§n phÃ¢n tÃ¡n ra nhiá»u servers

---

#### **CÃ¢u Há»i 2: App cá»§a báº¡n Báº¬N hay NHÃ€N?**

###### ğŸ’¼ **Báº¬N = CPU-bound**

```

VÃ­ dá»¥:

- TÃ­nh toÃ¡n phá»©c táº¡p

- Xá»­ lÃ½ video/image

- Machine learning inference

- MÃ£ hÃ³a dá»¯ liá»‡u

```

**ğŸ”¹ Äáº·c Ä‘iá»ƒm:**

- CPU cháº¡y 100%
- ThÃªm workers â†’ tranh giÃ nh CPU â†’ cháº­m hÆ¡n

**âœ… Giáº£i phÃ¡p:** **SCALE PODS**

```

Táº¡i sao?

- Má»—i pod = 1 CPU core riÃªng

- KhÃ´ng tranh giÃ nh

- Pods cháº¡y trÃªn servers khÃ¡c nhau

```

###### ğŸ• **NHÃ€N = I/O-bound**

```

VÃ­ dá»¥:

- Chá» database tráº£ káº¿t quáº£

- Call API bÃªn ngoÃ i

- Äá»c/ghi file

- Chá» user nháº­p liá»‡u

```

**ğŸ”¹ Äáº·c Ä‘iá»ƒm:**

- CPU chá»‰ dÃ¹ng 20-30%
- Háº§u háº¿t thá»i gian lÃ  CHá»œ

**âœ… Giáº£i phÃ¡p:** **SCALE WORKERS**

```

Táº¡i sao?

- Trong lÃºc worker 1 chá» DB â†’ worker 2 xá»­ lÃ½ request khÃ¡c

- Giá»‘ng 1 thu ngÃ¢n phá»¥c vá»¥ nhiá»u khÃ¡ch xáº¿p hÃ ng

- Táº­n dá»¥ng thá»i gian chá»

```

---

#### **CÃ¢u Há»i 3: App cá»§a báº¡n CÃ“ NHá»š hay QUÃŠN?**

###### ğŸ§  **CÃ“ NHá»š (Stateful)**

```

VÃ­ dá»¥:

- User Ä‘ang upload file (Ä‘ang giá»¯ session)

- Shopping cart

- WebSocket connections

- In-memory cache

```

**âœ… Giáº£i phÃ¡p:** **SCALE WORKERS**

```

Táº¡i sao?

- Workers trong cÃ¹ng pod chia sáº» bá»™ nhá»›

- User request luÃ´n Ä‘áº¿n Ä‘Ãºng worker Ä‘ang giá»¯ data

- KhÃ´ng máº¥t session khi scale

```

###### ğŸ¤– **QUÃŠN (Stateless)**

```

VÃ­ dá»¥:

- REST API Ä‘Æ¡n giáº£n

- Web server tÄ©nh

- Microservices khÃ´ng giá»¯ state

```

**âœ… Giáº£i phÃ¡p:** **SCALE PODS**

```

Táº¡i sao?

- Request nÃ o Ä‘áº¿n pod nÃ o cÅ©ng Ä‘Æ°á»£c

- Dá»… scale, dá»… load balance

- 1 pod cháº¿t khÃ´ng áº£nh hÆ°á»Ÿng

```

---

#### **ğŸ“Š Báº£ng Quyáº¿t Äá»‹nh ÄÆ¡n Giáº£n**

| **TÃ¬nh Huá»‘ng** | **Scale GÃ¬?** | **LÃ½ Do (1 cÃ¢u)** |

|---|---|---|

| Web API Ä‘Æ¡n giáº£n | **PODS** | Má»—i request Ä‘á»™c láº­p |

| Database worker | **WORKERS** | DÃ¹ng chung connection pool |

| Xá»­ lÃ½ video | **PODS** | CPU-intensive, cáº§n nhiá»u cores |

| Call API bÃªn ngoÃ i | **WORKERS** | Chá» nhiá»u, CPU ráº£nh |

| Cáº§n high availability | **PODS** | 1 pod cháº¿t, cÃ²n pods khÃ¡c |

| Khá»Ÿi Ä‘á»™ng cháº­m (30s+) | **WORKERS** | TrÃ¡nh máº¥t thá»i gian táº¡o pod má»›i |

| Cáº§n shared cache | **WORKERS** | Workers dÃ¹ng chung memory |

| Traffic khÃ´ng Ä‘á»u | **PODS** | Scale nhanh theo traffic |

---

#### **ğŸ¬ VÃ­ Dá»¥ Thá»±c Táº¿: Restaurant Analogy**

###### **Scenario 1: Fast Food (McDonald's)**

```

Äáº·c Ä‘iá»ƒm:

- ÄÆ¡n hÃ ng Ä‘Æ¡n giáº£n, nhanh

- KhÃ¡ch Ä‘áº¿n ngáº«u nhiÃªn

- KhÃ´ng cáº§n nhá»› khÃ¡ch

```

**â†’ Giáº£i phÃ¡p:** **Má»Ÿ nhiá»u chi nhÃ¡nh (SCALE PODS)**

- Peak hour: Má»Ÿ 10 chi nhÃ¡nh
- Off-peak: Chá»‰ 3 chi nhÃ¡nh
- 1 chi nhÃ¡nh Ä‘Ã³ng cá»­a â†’ khÃ¡ch sang chi nhÃ¡nh khÃ¡c

---

###### **Scenario 2: Fine Dining Restaurant**

```

Äáº·c Ä‘iá»ƒm:

- MÃ³n Äƒn phá»©c táº¡p

- Báº¿p trÆ°á»Ÿng cáº§n nhá»› sá»Ÿ thÃ­ch khÃ¡ch

- NguyÃªn liá»‡u Ä‘áº¯t tiá»n (shared resources)

```

**â†’ Giáº£i phÃ¡p:** **ThÃªm Ä‘áº§u báº¿p trong cÃ¹ng nhÃ  hÃ ng (SCALE WORKERS)**

- Peak hour: 8 Ä‘áº§u báº¿p cÃ¹ng 1 báº¿p
- DÃ¹ng chung nguyÃªn liá»‡u cao cáº¥p
- Chia sáº» kiáº¿n thá»©c vá» khÃ¡ch VIP

---

#### **ğŸš€ Best Practice: Chiáº¿n LÆ°á»£c 2 Táº§ng**

###### **Táº§ng 1: Tá»‘i Æ°u WORKERS trÆ°á»›c (Nhanh, ráº»)**

```yaml

Baseline:

- 3 pods

- 2 workers/pod

- Total: 6 workers



Khi tÄƒng táº£i:

- Giá»¯ nguyÃªn 3 pods

- TÄƒng lÃªn 4 workers/pod

- Total: 12 workers (2x capacity)

- Thá»i gian: 5-10 giÃ¢y

- Chi phÃ­: Gáº§n nhÆ° 0

```

###### **Táº§ng 2: SCALE PODS sau (Cháº­m hÆ¡n, Ä‘áº¯t hÆ¡n)**

```yaml

Khi workers Ä‘Ã£ max:

- TÄƒng tá»« 3 â†’ 6 pods

- Giá»¯ nguyÃªn 4 workers/pod

- Total: 24 workers (4x capacity)

- Thá»i gian: 30-60 giÃ¢y

- Chi phÃ­: Cao hÆ¡n (resource overhead)

```

**Táº¡i sao chiáº¿n lÆ°á»£c nÃ y tá»‘t?**

1. Response nhanh vá»›i worker scaling
2. Tiáº¿t kiá»‡m chi phÃ­
3. Táº­n dá»¥ng tá»‘i Ä‘a resources
4. Pod scaling lÃ  "backup plan"

---

#### **âŒ Sai Láº§m ThÆ°á»ng Gáº·p**

###### **1. Scale sai thá»© tá»±**

```

âŒ SAI: CÃ³ 1 pod vá»›i 1 worker â†’ táº¡o 10 pods vá»›i 1 worker

   (LÃ£ng phÃ­ overhead, má»—i pod tá»‘n 100-200MB)



âœ… ÄÃšNG: CÃ³ 1 pod vá»›i 1 worker â†’ tÄƒng lÃªn 1 pod vá»›i 4 workers

   â†’ Náº¿u váº«n khÃ´ng Ä‘á»§ â†’ má»›i tÄƒng sá»‘ pods

```

###### **2. QuÃ¡ nhiá»u workers trong 1 pod**

```

âŒ SAI: 1 pod cÃ³ 50 workers

   â†’ Workers tranh giÃ nh CPU/Memory

   â†’ Cháº­m hÆ¡n



âœ… ÄÃšNG: 1 pod cÃ³ 4-8 workers

   â†’ Sweet spot cho háº§u háº¿t apps

```

###### **3. KhÃ´ng set limits**

```

âŒ SAI: KhÃ´ng giá»›i háº¡n resources

   â†’ 1 pod Äƒn háº¿t tÃ i nguyÃªn node

   â†’ CÃ¡c pods khÃ¡c bá»‹ cháº¿t



âœ… ÄÃšNG: Set limits rÃµ rÃ ng

   cpu: 2 cores, memory: 4GB

```

---

#### **ğŸ¯ CÃ´ng Thá»©c Nhá»› Nhanh**

```

SCALE WORKERS khi:

- I/O-bound (chá» nhiá»u)

- Shared resources (database connections, cache)

- Startup cháº­m

- Cáº§n nhá»› state



SCALE PODS khi:

- CPU-bound (tÃ­nh toÃ¡n nhiá»u)

- Stateless (khÃ´ng nhá»›)

- Cáº§n high availability

- CÃ³ nhiá»u servers

- Traffic spikes

```

---

#### **ğŸ“ˆ Monitoring: Biáº¿t khi nÃ o scale**

###### **Dáº¥u hiá»‡u cáº§n SCALE WORKERS:**

```

- CPU < 50% nhÆ°ng response time cao

- Queue length tÄƒng

- "Waiting for I/O" nhiá»u

```

###### **Dáº¥u hiá»‡u cáº§n SCALE PODS:**

```

- CPU > 80%

- Memory > 80%

- Workers Ä‘Ã£ max (8-16)

- Latency cao liÃªn tá»¥c

```

---

---

PhÃ¢n tÃ­ch case `/conversations/end` theo framework:

[3 tools called]

PhÃ¢n tÃ­ch case `/conversations/end` theo framework:

## PhÃ¢n tÃ­ch case: `/conversations/end` API

### CÃ¢u há»i 1: App giá»‘ng NHÃ‚N VIÃŠN hay CÃ”NG TY?

PhÃ¢n tÃ­ch code:

```python
# endpoint_conversation_events.py:43
async def create_conversation_event(
    request: ConversationEventCreateRequest,
    service: ConversationEventService = Depends(get_conversation_event_service_async),
) -> ConversationEventCreateResponse:
    # Má»—i request Ä‘á»™c láº­p
    # KhÃ´ng giá»¯ state giá»¯a requests
    # REST API stateless
```

Káº¿t luáº­n: Giá»‘ng CÃ”NG TY (Stateless)

- Má»—i request Ä‘á»™c láº­p
- KhÃ´ng cáº§n nhá»› state giá»¯a requests
- REST API, khÃ´ng cÃ³ WebSocket

---

### CÃ¢u há»i 2: App Báº¬N hay NHÃ€N?

PhÃ¢n tÃ­ch timeline xá»­ lÃ½ 1 request vá»›i dáº«n chá»©ng tá»« code:

**Dáº«n chá»©ng code:**

```341:342:src/app/services/conversation_event_service.py
existing = await self.repository.get_by_conversation_id_async(current_conversation_id)
db_query_elapsed = (time.time() - db_query_start) * 1000  # Convert to ms
```

```343:343:src/app/services/conversation_event_service.py
if db_query_elapsed > 100:  # Log if query takes > 100ms
```

```425:430:src/app/services/conversation_event_service.py
payload["conversation_log"] = await asyncio.to_thread(
    transform_conversation_logs,
    raw_logs,
    request.start_time,
    request.end_time,
)
```

```422:422:src/app/services/conversation_event_service.py
# transform_conversation_logs() lÃ  CPU-bound, cÃ³ thá»ƒ tá»‘n 100-500ms vá»›i conversation lá»›n
```

```455:460:src/app/services/conversation_event_service.py
storage_ref = await asyncio.to_thread(
    self.storage_service.store_conversation_log,
    request.conversation_id,
    transformed_log,
    summary_dict
)
```

```491:492:src/app/services/conversation_event_service.py
event = await self.repository.create_async(payload)
db_insert_elapsed = (time.time() - db_insert_start) * 1000  # Convert to ms
```

```493:493:src/app/services/conversation_event_service.py
if db_insert_elapsed > 100:  # Log if insert takes > 100ms
```

**Timeline xá»­ lÃ½ 1 request (Æ°á»›c tÃ­nh dá»±a trÃªn code):**

```
T=0ms:    Request Ä‘áº¿n
          (Code: endpoint_conversation_events.py:55-70)

T=10-50ms:   DB query (check duplicate) - I/O-bound â³
            (Code: conversation_event_service.py:341)
            â†’ Log náº¿u > 100ms (line 343) â†’ thÆ°á»ng < 100ms

T=60-560ms:  Transform conversation_log - CPU-bound (100-500ms) ğŸ’»
            (Code: conversation_event_service.py:425)
            â†’ Comment line 422: "100-500ms vá»›i conversation lá»›n"
            â†’ Cháº¡y trong thread pool (asyncio.to_thread) â†’ khÃ´ng block event loop

T=560-1560ms: Store MinIO - I/O-bound (200-1000ms) â³
            (Code: conversation_event_service.py:455)
            â†’ Cháº¡y trong thread pool â†’ khÃ´ng block event loop

T=1560-1610ms: DB insert - I/O-bound (50-200ms) â³
            (Code: conversation_event_service.py:491)
            â†’ Log náº¿u > 100ms (line 493) â†’ thÆ°á»ng < 100ms

T=1610ms: Return 202 Accepted
          (Code: endpoint_conversation_events.py:117-123)
          â†’ Log total_time_ms vÃ  db_time_ms
```

**Tá»· lá»‡ thá»i gian (Æ°á»›c tÃ­nh):**

- I/O-bound: ~1-1.5s (DB queries, MinIO) = 70-80%
  - DB query: 10-50ms (code line 341-343)
  - MinIO store: 200-1000ms (code line 455)
  - DB insert: 50-200ms (code line 491-493)
- CPU-bound: ~0.1-0.5s (transform) = 20-30%
  - Transform: 100-500ms (code line 422, 425)

**Káº¿t luáº­n: Chá»§ yáº¿u lÃ  NHÃ€N (I/O-bound)**

- CPU chá»‰ dÃ¹ng 20-30% thá»i gian
- 70-80% thá»i gian lÃ  chá» I/O
- CPU-bound operations cháº¡y trong thread pool â†’ khÃ´ng block event loop

---

### CÃ¢u há»i 3: App CÃ“ NHá»š hay QUÃŠN?

PhÃ¢n tÃ­ch:

```python
# KhÃ´ng cÃ³:
- WebSocket connections
- In-memory session
- Shared state giá»¯a requests
- User-specific cache trong memory

# CÃ³:
- Database (shared, persistent)
- RabbitMQ (shared queue)
- MinIO (shared storage)
```

Káº¿t luáº­n: QUÃŠN (Stateless)

- KhÃ´ng giá»¯ state trong memory
- Táº¥t cáº£ state trong DB/external services

---

## Káº¿t luáº­n: Chiáº¿n lÆ°á»£c scale

### BÆ°á»›c 1: Scale WORKERS trÆ°á»›c (Ä‘Ã£ lÃ m)

LÃ½ do:

1. I/O-bound chá»§ yáº¿u (70-80% thá»i gian chá»)
   - Worker 1 chá» DB â†’ Worker 2 xá»­ lÃ½ request khÃ¡c
   - Táº­n dá»¥ng thá»i gian chá»
2. Shared resources (DB connection pool)
   - Workers trong cÃ¹ng pod dÃ¹ng chung connection pool
   - Hiá»‡u quáº£ hÆ¡n so vá»›i má»—i pod cÃ³ pool riÃªng
3. Startup overhead
   - Táº¡o worker má»›i nhanh (vÃ i giÃ¢y)
   - Táº¡o pod má»›i cháº­m hÆ¡n (30-60s)

Káº¿t quáº£:

- 1 worker â†’ 4 workers
- Throughput: 50 RPS â†’ 200-400 RPS
- CCU: 50 â†’ 100+

---

### BÆ°á»›c 2: Scale PODS sau (khi cáº§n)

Khi nÃ o cáº§n scale pods:

1. Workers Ä‘Ã£ max (4-8 workers/pod)
2. CPU > 80% (CPU-bound operations tÄƒng)
3. Memory > 80%
4. Cáº§n high availability (1 pod cháº¿t khÃ´ng áº£nh hÆ°á»Ÿng)

CÃ¡ch scale:

```yaml
# docker-compose.prod.yml hoáº·c Kubernetes
deploy:
  replicas: 3  # TÄƒng tá»« 1 â†’ 3 pods
```

Káº¿t quáº£:

- 1 pod Ã— 4 workers = 4 workers
- 3 pods Ã— 4 workers = 12 workers
- Throughput: 200-400 RPS â†’ 600-1200 RPS

---

## Báº£ng quyáº¿t Ä‘á»‹nh cho case nÃ y

| TÃ¬nh huá»‘ng            | Scale gÃ¬? | LÃ½ do                    |
| ----------------------- | ---------- | ------------------------- |
| Hiá»‡n táº¡i (50 CCU)     | WORKERS    | I/O-bound, shared DB pool |
| ÄÃ£ scale (4 workers)  | Monitor    | Xem cÃ³ Ä‘á»§ khÃ´ng       |
| Váº«n thiáº¿u (< 100 CCU) | WORKERS    | TÄƒng lÃªn 6-8 workers    |
| Workers max (8)         | PODS       | Scale horizontal          |
| CPU > 80%               | PODS       | CPU-bound tÄƒng           |
| Cáº§n HA                 | PODS       | Fault tolerance           |

---

## Chiáº¿n lÆ°á»£c 2 táº§ng cho case nÃ y

### Táº§ng 1: Tá»‘i Æ°u WORKERS (Ä‘Ã£ lÃ m)

```yaml
Baseline:
- 1 pod
- 1 worker
- Total: 1 worker
- Throughput: ~50 RPS
- CCU: 50

ÄÃ£ scale:
- 1 pod
- 4 workers
- Total: 4 workers
- Throughput: ~200-400 RPS
- CCU: 100+
- Thá»i gian: 5-10 giÃ¢y (restart container)
- Chi phÃ­: Gáº§n nhÆ° 0
```

### Táº§ng 2: SCALE PODS (khi cáº§n)

```yaml
Khi workers Ä‘Ã£ max (8 workers):
- TÄƒng tá»« 1 â†’ 3 pods
- Giá»¯ nguyÃªn 4 workers/pod
- Total: 12 workers
- Throughput: ~600-1200 RPS
- CCU: 300+
- Thá»i gian: 30-60 giÃ¢y
- Chi phÃ­: Cao hÆ¡n (3x resources)
```

---

## Monitoring: Khi nÃ o cáº§n scale tiáº¿p?

### Dáº¥u hiá»‡u cáº§n tÄƒng WORKERS (1 â†’ 4 â†’ 8):

```
âœ… CPU < 50% NHÆ¯NG response time cao (> 1s)
   â†’ Event loop quÃ¡ táº£i vá»›i nhiá»u concurrent requests
   â†’ Bottleneck khÃ´ng pháº£i CPU, mÃ  lÃ  event loop capacity

âœ… Concurrent requests > DB_POOL_SIZE Ã— sá»‘ workers
   â†’ VÃ­ dá»¥: 100 requests nhÆ°ng chá»‰ cÃ³ 60 DB connections (4 Ã— 15)
   â†’ Requests pháº£i Ä‘á»£i connection tá»« pool
   â†’ Dáº«n chá»©ng: database_connection.py:120-121 (pool_size=15 per worker)

âœ… Queue length tÄƒng (requests Ä‘á»£i xá»­ lÃ½)
   â†’ Event loop khÃ´ng ká»‹p xá»­ lÃ½ táº¥t cáº£ requests
   â†’ Cáº§n thÃªm event loops (workers) Ä‘á»ƒ phÃ¢n tÃ¡n load

âœ… Memory < 70% (cÃ²n capacity Ä‘á»ƒ thÃªm workers)
   â†’ Má»—i worker ~500MB â†’ cÃ³ thá»ƒ thÃªm workers
   â†’ Dáº«n chá»©ng: docker-compose.prod.yml:36 (memory: 3G limit)
```

### Dáº¥u hiá»‡u cáº§n SCALE PODS:

```
âš ï¸ CPU > 80% (CPU-bound operations tÄƒng)
âš ï¸ Memory > 80%
âš ï¸ Workers Ä‘Ã£ max (8 workers/pod)
âš ï¸ Latency cao liÃªn tá»¥c (> 2s)
âš ï¸ Cáº§n high availability
```

---

## Rá»§i ro load balancing (ChÆ°Æ¡ng 6)

### Váº¥n Ä‘á» khi scale pods

#### 1. Sticky sessions (khÃ´ng Ã¡p dá»¥ng cho case nÃ y)

```
âŒ Váº¥n Ä‘á»: Náº¿u cáº§n sticky sessions
   - User A â†’ Pod 1 (giá»¯ session)
   - User A request tiáº¿p â†’ Pod 2 (máº¥t session)

âœ… Giáº£i phÃ¡p: Case nÃ y stateless â†’ khÃ´ng cáº§n sticky
```

#### 2. Database connection pool exhaustion

```
âš ï¸ Váº¥n Ä‘á»: Nhiá»u pods â†’ nhiá»u connection pools
   - 3 pods Ã— 15 pool_size = 45 connections (base)
   - 3 pods Ã— 10 max_overflow = 30 connections (peak)
   - Total: 75 connections
   - Váº«n OK náº¿u PostgreSQL max_connections = 100

âœ… Giáº£i phÃ¡p: Äiá»u chá»‰nh DB_POOL_SIZE khi scale pods
   - CÃ´ng thá»©c: Total = pods Ã— workers Ã— (DB_POOL_SIZE + DB_MAX_OVERFLOW)
   - Äáº£m báº£o < PostgreSQL max_connections
```

#### 3. Shared resources contention

```
âš ï¸ Váº¥n Ä‘á»: Nhiá»u pods cáº¡nh tranh:
   - RabbitMQ queue
   - MinIO storage
   - Database locks

âœ… Giáº£i phÃ¡p: 
   - RabbitMQ: Auto-distribute messages
   - MinIO: S3-compatible, handle concurrent writes
   - Database: Connection pooling + transactions
```

---

## Connection Total Hiá»‡n Táº¡i

### Cáº¥u hÃ¬nh tá»« code:

**Docker Compose (docker-compose.prod.yml:20,26-27):**

```yaml
UVICORN_WORKERS: ${UVICORN_WORKERS:-4}  # Default: 4 workers
DB_POOL_SIZE: ${DB_POOL_SIZE:-15}       # Default: 15 per worker
DB_MAX_OVERFLOW: ${DB_MAX_OVERFLOW:-10} # Default: 10 per worker
```

**Database Connection (database_connection.py:117-121):**

```python
async_engine = create_async_engine(
    async_database_url,
    pool_pre_ping=True,
    pool_size=settings.DB_POOL_SIZE,      # 15 (tá»« docker-compose)
    max_overflow=settings.DB_MAX_OVERFLOW, # 10 (tá»« docker-compose)
    pool_timeout=settings.DB_POOL_TIMEOUT,
    pool_recycle=settings.DB_POOL_RECYCLE,
)
```

**Dependency Injection (dependency_injection.py:119,130-136):**

```python
async with AsyncSessionLocal() as session:  # DÃ¹ng async_engine
    yield session

async def get_conversation_event_service_async(
    db: AsyncSession = Depends(get_async_db),
) -> ConversationEventService:
    return ConversationEventService(db)
```

**Endpoint sá»­ dá»¥ng (endpoint_conversation_events.py:45):**

```python
service: ConversationEventService = Depends(get_conversation_event_service_async)
```

### TÃ­nh toÃ¡n Connection Total:

**Vá»›i 4 workers (default tá»« docker-compose.prod.yml:20):**

```
Má»—i worker process:
â”œâ”€ CÃ³ async_engine riÃªng (database_connection.py:117)
â”‚   â”œâ”€ pool_size: 15 connections (base)
â”‚   â””â”€ max_overflow: 10 connections (peak)
â””â”€ Total per worker: 15 + 10 = 25 connections max

4 workers:
â”œâ”€ Base pool: 4 Ã— 15 = 60 connections
â”œâ”€ Max overflow: 4 Ã— 10 = 40 connections
â””â”€ Total max: 100 connections
```

**LÆ°u Ã½:**

- Má»—i worker cÃ³ engine riÃªng â†’ má»—i worker cÃ³ pool riÃªng
- Connections Ä‘Æ°á»£c reuse trong cÃ¹ng request (dependency_injection.py:119-127)
- Peak usage phá»¥ thuá»™c vÃ o concurrent requests

### Connection Usage Pattern:

**Vá»›i 100 concurrent requests:**

```
T=0ms:    100 requests Ä‘áº¿n 4 workers
          â”œâ”€ Worker 1: 25 requests â†’ 15 láº¥y connection ngay, 10 Ä‘á»£i
          â”œâ”€ Worker 2: 25 requests â†’ 15 láº¥y connection ngay, 10 Ä‘á»£i
          â”œâ”€ Worker 3: 25 requests â†’ 15 láº¥y connection ngay, 10 Ä‘á»£i
          â””â”€ Worker 4: 25 requests â†’ 15 láº¥y connection ngay, 10 Ä‘á»£i
        
          Total: 60 connections Ä‘ang dÃ¹ng (base pool)
          Total: 40 requests Ä‘á»£i connection

T=500ms:  60 requests Ä‘áº§u xong â†’ tráº£ 60 connections vá» pool
          â”œâ”€ 40 requests tiáº¿p theo láº¥y connection (overflow)
          â””â”€ Total: 40 connections Ä‘ang dÃ¹ng (overflow)

T=1000ms: 40 requests tiáº¿p theo xong
          â””â”€ Total: 0 connections Ä‘ang dÃ¹ng
```

**Káº¿t quáº£:**

- Peak usage: 60 (base) + 40 (overflow) = 100 connections
- Average usage: ~30-50 connections (tÃ¹y traffic)

### So sÃ¡nh: TrÆ°á»›c vs Sau

**TrÆ°á»›c (1 worker, config cÅ© - config_settings.py:56-57):**

```
1 worker Ã— 50 pool_size = 50 connections (base)
1 worker Ã— 100 max_overflow = 100 connections (peak)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total max: 150 connections
```

**Sau (4 workers, config má»›i - docker-compose.prod.yml:26-27):**

```
4 workers Ã— 15 pool_size = 60 connections (base)
4 workers Ã— 10 max_overflow = 40 connections (peak)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total max: 100 connections
```

**Káº¿t quáº£:**

- Giáº£m total max: 150 â†’ 100 connections (-33%)
- TÄƒng concurrent capacity: 50 â†’ 60 requests (+20%)
- An toÃ n hÆ¡n vá»›i PostgreSQL max_connections (thÆ°á»ng 100-200)

---

## Káº¿t luáº­n cho case nÃ y

### ÄÃ£ lÃ m Ä‘Ãºng: Scale WORKERS trÆ°á»›c

**LÃ½ do (cÃ³ dáº«n chá»©ng):**

1. **I/O-bound chá»§ yáº¿u (70-80% thá»i gian chá»)**

   - Dáº«n chá»©ng: conversation_event_service.py:341, 455, 491
   - Worker 1 chá» DB â†’ Worker 2 xá»­ lÃ½ request khÃ¡c
   - Táº­n dá»¥ng thá»i gian chá»
2. **Shared resources (DB connection pool)**

   - Dáº«n chá»©ng: database_connection.py:117-121
   - Má»—i worker cÃ³ pool riÃªng, nhÆ°ng cÃ¹ng database
   - Hiá»‡u quáº£ hÆ¡n so vá»›i má»—i pod cÃ³ pool riÃªng
3. **Startup overhead**

   - Táº¡o worker má»›i nhanh (vÃ i giÃ¢y - restart container)
   - Táº¡o pod má»›i cháº­m hÆ¡n (30-60s)

**Káº¿t quáº£:**

- 1 worker â†’ 4 workers (docker-compose.prod.yml:20)
- DB connections: 50 â†’ 60 (base pool)
- Max connections: 150 â†’ 100 (an toÃ n hÆ¡n)

### Káº¿ hoáº¡ch tiáº¿p theo

1. **Monitor sau khi deploy 4 workers**

   - Check CPU, memory, latency
   - Verify throughput tÄƒng
   - Monitor connection usage vá»›i SQL query:
     ```sql
     SELECT count(*) FROM pg_stat_activity 
     WHERE datname = 'context_handling_db' 
       AND application_name LIKE '%uvicorn%';
     ```
2. **Náº¿u váº«n thiáº¿u (< 100 CCU)**

   - TÄƒng lÃªn 6-8 workers (update UVICORN_WORKERS)
   - Äiá»u chá»‰nh DB_POOL_SIZE tÆ°Æ¡ng á»©ng
3. **Náº¿u workers Ä‘Ã£ max (8)**

   - Scale pods: 1 â†’ 2-3 pods
   - Äiá»u chá»‰nh DB_POOL_SIZE Ä‘á»ƒ trÃ¡nh vÆ°á»£t max_connections
4. **Náº¿u cáº§n HA**

   - Scale pods: tá»‘i thiá»ƒu 2 pods




---