<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# PH√ÇN LO·∫†I MECE: C√ÅC PH∆Ø∆†NG PH√ÅP L·∫¨P TR√åNH CONCURRENT/PARALLEL TRONG PYTHON

## I. KHUNG PH√ÇN LO·∫†I MECE (MUTUALLY EXCLUSIVE, COLLECTIVELY EXHAUSTIVE)

### Chi·ªÅu 1: Execution Model (M√¥ H√¨nh Th·ª±c Thi)

```
Python Concurrency/Parallelism Models
‚îÇ
‚îú‚îÄ 1. SEQUENTIAL (Baseline - Kh√¥ng c√≥ concurrency)
‚îÇ
‚îú‚îÄ 2. CONCURRENCY (ƒê·ªìng th·ªùi - kh√¥ng ph·∫£i song song)
‚îÇ   ‚îú‚îÄ 2A. Asyncio (Event Loop - Single Thread)
‚îÇ   ‚îî‚îÄ 2B. Threading (OS Threads - B·ªã GIL gi·ªõi h·∫°n)
‚îÇ
‚îú‚îÄ 3. PARALLELISM (Song song th·ª±c s·ª±)
‚îÇ   ‚îú‚îÄ 3A. Multiprocessing (Separate Processes)
‚îÇ   ‚îî‚îÄ 3B. Hybrid (Threads + Processes k·∫øt h·ª£p)
‚îÇ
‚îî‚îÄ 4. SPECIALIZED (Chuy√™n bi·ªát)
    ‚îú‚îÄ 4A. concurrent.futures.ThreadPoolExecutor
    ‚îú‚îÄ 4B. concurrent.futures.ProcessPoolExecutor
    ‚îî‚îÄ 4C. asyncio + run_in_executor (Hybrid)
```


***

## II. B·∫¢NG SO S√ÅNH TO√ÄN DI·ªÜN

| Ph∆∞∆°ng Ph√°p | Execution Unit | Memory Model | GIL Impact | Best For | Overhead | Max Throughput | Code Complexity |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| **1. Sequential** | Single thread, single process | Single memory space | N/A | Simple tasks, debugging | ‚ö° Kh√¥ng c√≥ | Th·∫•p nh·∫•t (baseline) | ‚≠ê R·∫•t ƒë∆°n gi·∫£n |
| **2A. Asyncio (FastAPI `async def`)** | Single thread + Event Loop | Single memory space | ‚ùå B·ªã GIL (nh∆∞ng kh√¥ng matter)[^1] | **I/O-bound** (API, DB, file)[^2] | ‚ö°‚ö° R·∫•t th·∫•p (~1 thread)[^3] | **R·∫§T CAO** cho I/O[^3] | ‚≠ê‚≠ê‚≠ê Trung b√¨nh (async/await syntax) |
| **2B. Threading** | Multiple OS threads | **Shared memory** | ‚ùå‚ùå **B·ªä GIL block**[^1] | I/O-bound (legacy code) | ‚ö°‚ö°‚ö° Trung b√¨nh (context switch) | Trung b√¨nh | ‚≠ê‚≠ê‚≠ê‚≠ê Kh√≥ (race conditions, locks) |
| **3A. Multiprocessing** | Multiple processes | **Separate memory** (IPC needed) | ‚úÖ **KH√îNG b·ªã GIL**[^1] | **CPU-bound** (t√≠nh to√°n n·∫∑ng)[^1] | üê¢üê¢üê¢ Cao (spawn process ~ms) | Cao (= s·ªë cores) | ‚≠ê‚≠ê‚≠ê‚≠ê Kh√≥ (IPC, serialization) |
| **4A. ThreadPoolExecutor** | Thread pool (reusable threads) | Shared memory | ‚ùå‚ùå B·ªã GIL | I/O-bound batch jobs | ‚ö°‚ö° Th·∫•p (reuse threads) | Trung b√¨nh-Cao | ‚≠ê‚≠ê D·ªÖ (high-level API) |
| **4B. ProcessPoolExecutor** | Process pool (reusable processes) | Separate memory | ‚úÖ Kh√¥ng b·ªã GIL | CPU-bound batch jobs | üê¢üê¢ Trung b√¨nh (reuse processes) | Cao (= s·ªë cores) | ‚≠ê‚≠ê D·ªÖ (high-level API) |
| **4C. asyncio + run_in_executor** | Event Loop + Thread/Process pool | Mixed | Partial (depends on executor) | **Mix I/O + CPU**[^4] | ‚ö°‚ö°‚ö° Trung b√¨nh | R·∫•t Cao | ‚≠ê‚≠ê‚≠ê‚≠ê Kh√≥ (mix paradigms) |


***

## III. SO S√ÅNH CHI TI·∫æT: ASYNC vs THREADING vs MULTIPROCESSING

### A. Ki·∫øn Tr√∫c \& Execution Model

| Aspect | Asyncio | Threading | Multiprocessing |
| :-- | :-- | :-- | :-- |
| **S·ªë threads** | 1 thread duy nh·∫•t[^5] | Nhi·ªÅu threads (VD: 10-100) | 1 thread/process (nh∆∞ng nhi·ªÅu processes) |
| **S·ªë processes** | 1 process | 1 process | Nhi·ªÅu processes (VD: 4-16) |
| **Context switching** | **Cooperative** (t·ª± nguy·ªán)[^1] | **Preemptive** (OS quy·∫øt ƒë·ªãnh) | **Preemptive** (OS quy·∫øt ƒë·ªãnh) |
| **Shared state** | ‚úÖ D·ªÖ (c√πng memory) | ‚ö†Ô∏è Kh√≥ (c·∫ßn locks, race conditions)[^2] | ‚ùå R·∫•t kh√≥ (c·∫ßn IPC, serialization) |
| **True parallelism** | ‚ùå Kh√¥ng (sequential execution)[^1] | ‚ùå Kh√¥ng (GIL block)[^1] | ‚úÖ **C√≥** (bypass GIL)[^1] |

### B. Performance Characteristics

#### I/O-Bound Task (VD: 1000 HTTP requests)

| Method | Execution Time | Resource Usage | Scalability |
| :-- | :-- | :-- | :-- |
| **Sequential** | ~1000 seconds (1s/request) | 1 CPU, minimal RAM | ‚ùå Kh√¥ng scale |
| **Asyncio** | ~10-20 seconds[^3] | 1 CPU, 50MB RAM | ‚úÖ‚úÖ‚úÖ **Xu·∫•t s·∫Øc** (10K+ concurrent) |
| **Threading** | ~15-30 seconds | 1-2 CPU, 200MB+ RAM | ‚úÖ‚úÖ T·ªët (100-1000 concurrent) |
| **Multiprocessing** | ~25-40 seconds | 4+ CPU, 500MB+ RAM | ‚ö†Ô∏è K√©m (overhead cao)[^1] |

**Winner:** **Asyncio** (10-50x faster, √≠t resources nh·∫•t)[^5][^2]

#### CPU-Bound Task (VD: T√≠nh to√°n 1 tri·ªáu s·ªë)

| Method | Execution Time (4 cores) | CPU Usage | Winner |
| :-- | :-- | :-- | :-- |
| **Sequential** | ~40 seconds (baseline) | 25% (1 core) | - |
| **Asyncio** | ~39 seconds (**G·∫¶N NH∆Ø KH√îNG c·∫£i thi·ªán**)[^1] | 25% (1 core) | ‚ùå |
| **Threading** | ~38 seconds (GIL block, g·∫ßn nh∆∞ kh√¥ng c·∫£i thi·ªán)[^1] | 25-30% | ‚ùå |
| **Multiprocessing** | **~10 seconds** (4x faster)[^1] | 100% (4 cores) | ‚úÖ‚úÖ‚úÖ |

**Winner:** **Multiprocessing** (ch·ªâ l·ª±a ch·ªçn duy nh·∫•t cho CPU-bound)[^1]

***

## IV. FASTAPI: ASYNC VS SYNC ENDPOINTS

### Benchmark Th·ª±c T·∫ø (500 Concurrent Users)[^3]

| Endpoint Type | Code | Requests/sec | Median Response | Threads Created | Failures |
| :-- | :-- | :-- | :-- | :-- | :-- |
| **Sync endpoint + Sync I/O** | `def` + `requests.get()` | **35.6** | 1,300 ms | 41 threads | 0% |
| **Async endpoint + Async I/O** | `async def` + `httpx.get()` | **53.2** | 8,300 ms | **1 thread** | 0% |
| **Async endpoint + Sync I/O** | `async def` + `requests.get()` | **13.1** ‚ùå | 27,000 ms | 1 thread | **93.2%** ‚ùå‚ùå |

### K·∫øt Lu·∫≠n FastAPI:[^6][^3]

1. **‚úÖ ƒê√öNG: `async def` + async I/O (httpx, asyncpg)** ‚Üí Hi·ªáu nƒÉng cao nh·∫•t
2. **‚úÖ OK: `def` + sync I/O (requests, psycopg2)** ‚Üí FastAPI t·ª± ƒë·ªông d√πng ThreadPool
3. **‚ùå‚ùå SAI L·∫¶M CH·∫æT NG∆Ø·ªúI: `async def` + sync I/O** ‚Üí Block event loop, failures 93%[^3]

**Nguy√™n t·∫Øc v√†ng:**

```python
# ‚úÖ ƒê√öNG
@app.get("/users")
async def get_users():
    async with httpx.AsyncClient() as client:  # Async I/O
        return await client.get("https://api.example.com/users")

# ‚úÖ C≈®NG OK (FastAPI handle b·∫±ng ThreadPool)
@app.get("/users")
def get_users():  # Sync endpoint
    return requests.get("https://api.example.com/users")  # Sync I/O

# ‚ùå‚ùå TUY·ªÜT ƒê·ªêI TR√ÅNH
@app.get("/users")
async def get_users():  # Async endpoint
    return requests.get("...")  # Sync I/O ‚Üí BLOCK EVENT LOOP!
```


***

## V. DECISION MATRIX (KHI N√ÄO D√ôNG C√ÅI N√ÄO)

### A. Theo Task Type

| Task Characteristics | Best Choice | Why | Throughput Potential |
| :-- | :-- | :-- | :-- |
| **I/O-bound: API calls, DB queries, file I/O** | **Asyncio** (FastAPI `async def`)[^2] | Event loop hi·ªáu qu·∫£ nh·∫•t, √≠t overhead | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (10K-100K ops/s) |
| **I/O-bound: Legacy code kh√¥ng th·ªÉ async** | ThreadPoolExecutor | Wrapper sync code d·ªÖ d√†ng | ‚≠ê‚≠ê‚≠ê (100-1K ops/s) |
| **CPU-bound: T√≠nh to√°n, data processing** | **ProcessPoolExecutor**[^1] | Bypass GIL, true parallelism | ‚≠ê‚≠ê‚≠ê‚≠ê (= s·ªë cores √ó performance) |
| **CPU-bound: Nh·∫π (< 100ms)** | Sequential ho·∫∑c Threading | Overhead kh√¥ng ƒë√°ng | ‚≠ê‚≠ê |
| **Mix I/O + CPU** | asyncio + `run_in_executor()`[^4] | I/O tr√™n event loop, CPU tr√™n processes | ‚≠ê‚≠ê‚≠ê‚≠ê |

### B. Theo Infrastructure \& Constraints

| Constraint | Recommendation | Reason |
| :-- | :-- | :-- |
| **Single-core server** | Asyncio | Multiprocessing v√¥ √≠ch khi ch·ªâ 1 core |
| **Multi-core server (4+)** | Multiprocessing cho CPU-bound | T·∫≠n d·ª•ng parallelism |
| **RAM gi·ªõi h·∫°n** | Asyncio > Threading > Multiprocessing | Asyncio d√πng √≠t RAM nh·∫•t[^2] |
| **C·∫ßn shared state** | Asyncio ho·∫∑c Threading | Multiprocessing c·∫ßn IPC ph·ª©c t·∫°p |
| **Microservices (stateless)** | Asyncio (FastAPI) | Scalability t·ªët nh·∫•t |
| **Legacy codebase** | ThreadPoolExecutor | Wrap sync code m√† kh√¥ng refactor |


***

## VI. CODE EXAMPLES ƒê·∫¶Y ƒê·ª¶

### 1. Asyncio (FastAPI Style)

```python
import asyncio
import httpx

# ‚úÖ Best for: 1000s of concurrent I/O operations
async def fetch_many_urls(urls):
    async with httpx.AsyncClient() as client:
        tasks = [client.get(url) for url in urls]
        # All run concurrently on 1 thread!
        responses = await asyncio.gather(*tasks)
    return responses

# FastAPI endpoint
@app.get("/data")
async def get_data():
    urls = ["https://api1.com", "https://api2.com", ...]
    return await fetch_many_urls(urls)
```


### 2. ThreadPoolExecutor

```python
from concurrent.futures import ThreadPoolExecutor
import requests

# ‚úÖ Best for: I/O-bound batch jobs (legacy sync code)
def fetch_url(url):
    return requests.get(url).json()

urls = ["https://api1.com", "https://api2.com"] * 100

with ThreadPoolExecutor(max_workers=20) as executor:
    results = list(executor.map(fetch_url, urls))
```


### 3. ProcessPoolExecutor

```python
from concurrent.futures import ProcessPoolExecutor
import numpy as np

# ‚úÖ Best for: CPU-intensive calculations
def heavy_compute(n):
    # T√≠nh to√°n ph·ª©c t·∫°p
    return np.sum(np.random.rand(n, n) ** 2)

numbers = [5000, 10000, 15000, 20000]

with ProcessPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(heavy_compute, numbers))
    # Ch·∫°y th·∫≠t s·ª± song song tr√™n 4 cores!
```


### 4. Hybrid: Asyncio + Executor (Best of Both Worlds)

```python
import asyncio
from concurrent.futures import ProcessPoolExecutor

async def hybrid_workflow():
    loop = asyncio.get_event_loop()
    executor = ProcessPoolExecutor(max_workers=4)
    
    # I/O-bound: ch·∫°y async
    data = await fetch_data_from_api()
    
    # CPU-bound: offload sang process pool
    result = await loop.run_in_executor(
        executor, 
        heavy_computation, 
        data
    )
    
    return result
```


***

## VII. ANTI-PATTERNS \& COMMON MISTAKES

| ‚ùå Anti-Pattern | V·∫•n ƒê·ªÅ | ‚úÖ Fix |
| :-- | :-- | :-- |
| `async def` + `requests.get()` | Block event loop[^3] | D√πng `httpx.AsyncClient()` |
| Threading cho CPU-bound | GIL block, kh√¥ng c·∫£i thi·ªán[^1] | D√πng Multiprocessing |
| Multiprocessing cho I/O-bound | Overhead cao kh√¥ng c·∫ßn thi·∫øt[^1] | D√πng Asyncio |
| Shared state trong Multiprocessing | C·∫ßn Manager, Queue, Pipe ph·ª©c t·∫°p | D√πng Threading ho·∫∑c Asyncio |
| Threading kh√¥ng c√≥ locks | Race conditions, data corruption[^2] | D√πng `threading.Lock()` |
| Asyncio v·ªõi CPU-intensive tasks | Block event loop l√¢u | Offload sang `run_in_executor()` |


***

## VIII. PERFORMANCE SUMMARY TABLE

### I/O-Bound (1000 API calls)

| Method | Time | Speedup | Resource |
| :-- | :-- | :-- | :-- |
| Sequential | 1000s | 1x | 1 CPU, 10MB |
| **Asyncio** | **15s** | **67x** | 1 CPU, 50MB |
| Threading | 25s | 40x | 2 CPU, 200MB |
| Multiprocessing | 50s | 20x | 4 CPU, 500MB |

### CPU-Bound (Heavy computation)

| Method | Time | Speedup | CPU Usage |
| :-- | :-- | :-- | :-- |
| Sequential | 40s | 1x | 25% (1 core) |
| Asyncio | 39s | 1.03x ‚ùå | 25% |
| Threading | 38s | 1.05x ‚ùå | 30% |
| **Multiprocessing** | **10s** | **4x** | 100% (4 cores) |


***

## IX. K·∫æT LU·∫¨N \& BEST PRACTICES

### üéØ Top Picks

| Use Case | \#1 Choice | Alternative | Avoid |
| :-- | :-- | :-- | :-- |
| **Web APIs (FastAPI)** | Asyncio (`async def`) | Threading (`def`) | Multiprocessing |
| **Data processing** | ProcessPoolExecutor | - | Threading |
| **Web scraping** | Asyncio + httpx | ThreadPoolExecutor | Multiprocessing |
| **Batch jobs (I/O)** | ThreadPoolExecutor | Asyncio | - |
| **ML training** | Multiprocessing | GPU (if available) | Threading |
| **Legacy sync code** | ThreadPoolExecutor | Refactor to async | Force async |

### üìè Nguy√™n T·∫Øc Ch·ªçn L·ª±a

1. **I/O-bound ‚Üí Asyncio first** (FastAPI style)[^2][^6]
2. **CPU-bound ‚Üí Multiprocessing only**[^1]
3. **Can't refactor ‚Üí ThreadPoolExecutor** (wrapper cho sync code)
4. **Mix I/O + CPU ‚Üí Asyncio + run_in_executor**[^4]
5. **Always measure** - Premature optimization l√† g·ªëc r·ªÖ c·ªßa m·ªçi t·ªôi l·ªói

### ‚ö†Ô∏è Caveat v·ªÅ Langfuse SDK

Nh∆∞ ƒë√£ ƒë·ªÅ c·∫≠p trong b·∫£ng tr∆∞·ªõc:

- **@observe + ThreadPoolExecutor/ProcessPoolExecutor ‚Üí BROKEN**[^7]
- **Ph·∫£i d√πng Low-Level API v·ªõi manual trace_id propagation**

***

**Sources:**,,,,,[^5][^2][^4][^6][^1][^3]
<span style="display:none">[^10][^11][^8][^9]</span>

<div align="center">‚ÅÇ</div>

[^1]: https://stackoverflow.com/questions/27435284/multiprocessing-vs-multithreading-vs-asyncio/52498068

[^2]: https://www.geeksforgeeks.org/python/asyncio-vs-threading-in-python/

[^3]: https://github.com/Minibrams/fastapi-benchmark

[^4]: https://stackoverflow.com/questions/61351844/difference-between-multiprocessing-asyncio-threading-and-concurrency-futures-i

[^5]: https://leimao.github.io/blog/Python-Concurrency-High-Level/

[^6]: https://dev.to/kfir-g/unleash-the-power-of-fastapi-async-vs-blocking-io-4h0b

[^7]: https://langfuse.com/blog/2024-04-python-decorator

[^8]: https://www.reddit.com/r/learnpython/comments/1fhry6u/asyncio_vs_threading_vs_multiprocessing/

[^9]: https://www.linkedin.com/pulse/multithreading-vs-multiprocessing-asyncio-code-examples-kaushik-yxgjc

[^10]: https://dev.to/doziestar/concurrency-in-python-made-simple-51g3

[^11]: https://www.linkedin.com/pulse/python-concurrency-models-navigating-maze-concurrent-programming-srmoc

