# ğŸ¯ ÄÃšC Káº¾T CUá»I CÃ™NG: KHI NÃ€O TÄ‚NG WORKERS

Dá»±a trÃªn phÃ¢n tÃ­ch chi tiáº¿t vÃ  test thá»±c táº¿ vá»›i mem0-server, Ä‘Ã¢y lÃ  káº¿t luáº­n vá» **khi nÃ o nÃªn tÄƒng workers**:

---

## âš¡ QUY Táº®C VÃ€NG (Golden Rules)

### ğŸŸ¢ **DEFAULT: Báº¯t Ä‘áº§u vá»›i 1 WORKER**

Ãp dá»¥ng cho **Táº¤T Cáº¢** async I/O-bound applications:

- âœ… FastAPI/Flask/Django async
- âœ… Web APIs gá»i database/cache/external services
- âœ… Microservices vá»›i async HTTP clients
- âœ… Workload >70% I/O time

**LÃ½ do:**

- 1 worker + async event loop xá»­ lÃ½ Ä‘Æ°á»£c **hÃ ng nghÃ¬n concurrent connections**
- CPU idle tháº¥p nháº¥t (20-30%)
- Memory tiáº¿t kiá»‡m nháº¥t
- Debug Ä‘Æ¡n giáº£n nháº¥t

---

### ğŸ”´ **CHá»ˆ TÄ‚NG WORKERS KHI:**

Thá»a mÃ£n **ÃT NHáº¤T 1** trong cÃ¡c Ä‘iá»u kiá»‡n sau:

#### 1ï¸âƒ£ **CPU-bound Operations â‰¥30% Total Time**

```python
# âŒ Cáº§n workers: Heavy CPU computation
@app.post("/predict")
async def ml_inference(data):
    # 500ms CPU blocking
    result = model.predict(data)  # Sync blocking!
    return result

# Total: 500ms CPU (100% CPU-bound)
# â†’ Cáº§n 2-4 workers Ä‘á»ƒ táº­n dá»¥ng multi-core
```

**Dáº¥u hiá»‡u:**

- ML inference, image/video processing
- Data transformation (pandas, numpy)
- Cryptography operations
- CPU time â‰¥30% total response time

**Giáº£i phÃ¡p:**

```yaml
# Option 1: Scale workers
command: uvicorn main:app --workers 4

# Option 2: Offload to ProcessPoolExecutor (KHUYáº¾N NGHá»Š)
cpu_pool = ProcessPoolExecutor(max_workers=4)
result = await loop.run_in_executor(cpu_pool, heavy_compute)
```

---

#### 2ï¸âƒ£ **Legacy Blocking Code KhÃ´ng Thá»ƒ Refactor**

```python
# âŒ Cáº§n workers: Blocking sync library
import requests  # Not async!

@app.get("/external")
async def call_api():
    # Sync blocking worker!
    response = requests.get("https://api.example.com")
    return response.json()
```

**Giáº£i phÃ¡p tá»‘t:**

```python
# âœ… Migrate to async (KHUYáº¾N NGHá»Š)
import httpx

@app.get("/external")
async def call_api():
    async with httpx.AsyncClient() as client:
        response = await client.get("https://api.example.com")
    return response.json()
```

**Giáº£i phÃ¡p táº¡m:**

```yaml
# Náº¿u KHÃ”NG THá»‚ refactor â†’ dÃ¹ng workers
command: uvicorn main:app --workers 2
```

---

#### 3ï¸âƒ£ **High Traffic â‰¥10,000 Requests/Second**

**Load test chá»©ng minh bottleneck:**

```bash
# Test vá»›i 1 worker
wrk -t12 -c1000 -d30s http://localhost:8000/api

Results:
  Requests/sec:   8,500
  Latency p99:    800ms  âŒ (quÃ¡ cao)
  CPU usage:      95%    âŒ (saturated)
  
# Test vá»›i 4 workers
Results:
  Requests/sec:   15,000  âœ… (+76%)
  Latency p99:    250ms   âœ… (-69%)
  CPU usage:      85%     âœ… (balanced)
```

**Quy táº¯c:**

- 1 worker async thÆ°á»ng handle: **5K-10K reqs/s**
- Náº¿u traffic >10K reqs/s â†’ cáº§n 2-4 workers

---

#### 4ï¸âƒ£ **Load Test Cáº£i Thiá»‡n â‰¥30%**

**Decision matrix:**

|Workers|Req/s|Lat p99|CPU%|Memory|Verdict|
|---|---|---|---|---|---|
|1|8,500|800ms|95%|400MB|âš ï¸ Bottleneck|
|2|12,000|500ms|85%|700MB|âœ… Cáº£i thiá»‡n 41%|
|4|15,000|250ms|80%|1.3GB|âœ… Cáº£i thiá»‡n 76%|

**Quy táº¯c:** Scale náº¿u:

- Throughput tÄƒng â‰¥30%
- Latency p99 giáº£m â‰¥30%
- CPU < 90% under load
- Memory Ä‘á»§ (cÃ³ â‰¥2GB available)

---

## ğŸš« **KHÃ”NG TÄ‚NG WORKERS KHI:**

### âŒ Anti-patterns (trÃ¡nh)

#### 1. Code 100% Async + I/O-bound

```python
# âœ… Giá»¯ 1 worker: Pure async I/O
@app.get("/users")
async def get_users():
    users = await db.query()     # Async
    cache = await redis.get()    # Async
    result = await api.call()    # Async
    return users

# Total: 350ms
# CPU: 10ms (3%)
# I/O: 340ms (97%)
# â†’ 1 worker Äá»¦
```

#### 2. CPU Idle Cao (>50%)

```bash
# 4 workers: CPU idle 116% â†’ Overhead cao
# 1 worker:  CPU idle  54% â†’ Tá»‘i Æ°u
```

#### 3. ChÆ°a Fix Root Cause

```python
# âŒ SAI: Scale workers trÆ°á»›c khi fix N+1 queries
# N+1 query problem
for user in users:
    posts = await db.get_posts(user.id)  # N queries!

# âœ… ÄÃšNG: Fix N+1 trÆ°á»›c
post_map = await db.get_all_posts(user_ids)  # 1 query
```

#### 4. Memory Háº¡n Cháº¿

```
1 worker:  400MB
2 workers: 700MB
4 workers: 1.3GB

â†’ Náº¿u container limit 1GB â†’ chá»‰ dÃ¹ng 1-2 workers
```

---

## ğŸ“Š CASE STUDY: mem0-server

### Káº¿t quáº£ test thá»±c táº¿ cá»§a Báº N:

|Metric|4 Workers|2 Workers|1 Worker|
|---|---|---|---|
|**CPU Idle**|116.21% âš ï¸|54.96-61.55%|**53.70%** âœ…|
|**CPU Peak**|415.16% âŒ|130-415%|407-408% âš ï¸|
|**Memory**|819MB|622MB|**386MB** âœ…|
|**PIDS**|3,610|3,498|~2,500 âœ…|
|**30 CCU**|Treo|Treo|Treo (408% CPU) âŒ|
|**50 CCU**|Treo|?|Treo (407% CPU) âŒ|

### PhÃ¢n tÃ­ch workload:

```python
# mem0-server flow:
async def search(query):
    chunks = chunk_text(query)          # 50ms - CPU
    vectors = await embed(chunks)        # 200ms - I/O
    results = await milvus.search()      # 100ms - I/O
    summary = await llm.generate()       # 100ms - I/O
    await milvus.insert()                # 50ms - I/O
    return results

# Total: 500ms
# CPU: 50ms (10%)
# I/O: 450ms (90%)
# â†’ I/O-bound workload âœ…
```

### âœ… Káº¾T LUáº¬N: GIá»® 1 WORKER

**LÃ½ do:**

1. âœ… Code 100% async (FastAPI + async clients)
2. âœ… Workload 90% I/O-bound
3. âœ… CPU idle tháº¥p nháº¥t (53.70% vs 116%)
4. âœ… Memory tiáº¿t kiá»‡m 38-54%
5. âœ… Traffic vá»«a pháº£i (<5K reqs/s)

**ğŸ”´ Váº¥n Ä‘á» thá»±c sá»±:**

KhÃ´ng pháº£i sá»‘ workers, mÃ  lÃ :

1. **CPU limit quÃ¡ tháº¥p (4 cores)**
    
    - 30-50 CCU â†’ CPU 407-408% â†’ vÆ°á»£t limit â†’ treo
    - Cáº§n tÄƒng CPU limit lÃªn **6-8 cores**
2. **CPU idle cao (53.70%)**
    
    - Logging overhead
    - Background tasks
    - Cáº§n tá»‘i Æ°u xuá»‘ng <30%

---

## ğŸ¯ DECISION TREE THá»°C CHIáº¾N

```
Báº®T Äáº¦U: 1 WORKER (default)
   â†“
[1] Code cÃ³ 100% async/await?
   NO â†’ [2] CÃ³ thá»ƒ refactor?
          YES â†’ Refactor â†’ [3]
          NO â†’ DÃ™NG 2-4 WORKERS âš ï¸
   YES â†’ [3]

[3] Workload type?
   I/O-bound (â‰¥70% I/O) â†’ [4]
   CPU-bound (â‰¥70% CPU) â†’ DÃ™NG 2-4 WORKERS âš ï¸
   Mixed â†’ [5]

[4] Traffic level?
   Low (<5K reqs/s) â†’ GIá»® 1 WORKER âœ…
   Medium (5-10K) â†’ Load test â†’ [6]
   High (â‰¥10K) â†’ DÃ™NG 2-4 WORKERS âš ï¸

[5] CPU ratio?
   <30% CPU â†’ GIá»® 1 WORKER âœ…
   30-50% â†’ Load test â†’ [6]
   >50% â†’ DÃ™NG 2-4 WORKERS âš ï¸

[6] Load test results?
   CPU <80% & latency OK â†’ GIá»® 1 WORKER âœ…
   CPU >90% â†’ DÃ™NG 2 WORKERS âš ï¸
   CPU maxed + errors â†’ DÃ™NG 4 WORKERS âš ï¸
```

---

## ğŸ“‹ CHECKLIST THá»°C HÃ€NH

### TrÆ°á»›c khi scale workers:

- [ ] âœ… ÄÃ£ profile code (async/sync ratio >90%)
- [ ] âœ… ÄÃ£ fix N+1 queries
- [ ] âœ… ÄÃ£ migrate sync libraries â†’ async (requests â†’ httpx)
- [ ] âœ… ÄÃ£ cháº¡y load test vá»›i 1 worker
- [ ] âœ… CPU >90% under load OR latency p99 >1s
- [ ] âœ… Memory Ä‘á»§ (cÃ³ â‰¥2GB available)

### Khi scale workers:

- [ ] âœ… Load test cáº£i thiá»‡n â‰¥30%
- [ ] âœ… Monitor 1-2 tuáº§n: CPU, memory, latency
- [ ] âœ… So sÃ¡nh metrics before/after
- [ ] âœ… Rollback náº¿u khÃ´ng cáº£i thiá»‡n

---

## ğŸ”§ GIáº¢I PHÃP CHO mem0-server

### Ngáº¯n háº¡n (URGENT - TrÃ¡nh treo):

```yaml
# docker-compose-app.yml
deploy:
  resources:
    limits:
      cpus: '6.0'  # TÄƒng tá»« 4.0 â†’ 6.0 cores
      memory: 2G
command: uvicorn main:app --host 0.0.0.0 --port 8000 --workers 1 --loop uvloop
```

**Dá»± kiáº¿n:**

- 30 CCU: CPU ~65% (khÃ´ng vÆ°á»£t limit)
- 50 CCU: CPU ~85% (khÃ´ng vÆ°á»£t limit)
- 80 CCU: CPU ~100% (max capacity)

### Trung háº¡n (Tá»‘i Æ°u CPU idle):

```python
# main.py - Tá»‘i Æ°u logging
@app.middleware("http")
async def add_process_time_header(request, call_next):
    start = time.time()
    response = await call_next(request)
    process_time = time.time() - start
    
    # Chá»‰ log náº¿u cháº­m hoáº·c error
    if process_time > 0.5 or response.status_code >= 400:
        logging.warning(
            f"{request.method} {request.url.path} - {process_time:.4f}s"
        )
    
    return response
```

**Má»¥c tiÃªu:** CPU idle giáº£m tá»« 53.70% â†’ <30%

### DÃ i háº¡n (Scale):

```python
# Option 1: Auto-scaling CPU limit
if current_ccu > 50:
    cpus = min(8.0, current_ccu * 0.08)  # 8% CPU per CCU
else:
    cpus = 6.0

# Option 2: Scale ngang (nhiá»u containers)
replicas = ceil(current_ccu / 50)  # Má»—i container handle 50 CCU
```

---

## ğŸ“ CÃ”NG THá»¨C Tá»”NG QUÃT

### CÃ´ng thá»©c tÃ­nh workers tá»‘i Æ°u:

#### Cho I/O-bound (>70% I/O time):

```python
optimal_workers = 1  # Always!

# Trá»« khi:
if traffic > 10_000:  # reqs/s
    optimal_workers = 2
if traffic > 20_000:
    optimal_workers = 4
```

#### Cho CPU-bound (>70% CPU time):

```python
optimal_workers = min(
    CPU_cores,
    concurrent_requests / (cpu_time_per_req / target_response_time)
)

# VÃ­ dá»¥:
# - 4 CPU cores
# - 100 concurrent requests
# - 500ms CPU per request
# - Target: 200ms response time
optimal_workers = min(4, 100 / (500/200))
                = min(4, 40)
                = 4 workers
```

#### Cho Mixed workload:

```python
cpu_ratio = cpu_time / total_time

if cpu_ratio < 0.3:
    optimal_workers = 1
elif cpu_ratio < 0.5:
    optimal_workers = 2
else:
    optimal_workers = min(CPU_cores, 4)
```

---

## ğŸ’¡ Káº¾T LUáº¬N CUá»I CÃ™NG

### ğŸŸ¢ Quy táº¯c Ä‘Æ¡n giáº£n:

```
1. Báº®T Äáº¦U vá»›i 1 WORKER (async apps)

2. MONITOR 1-2 tuáº§n

3. CHá»ˆ SCALE khi:
   âœ… Load test chá»©ng minh cáº£i thiá»‡n â‰¥30%
   âœ… CPU â‰¥90% under production load
   âœ… CÃ³ CPU-bound khÃ´ng thá»ƒ offload
   âœ… Traffic â‰¥10K reqs/s

4. KHÃ”NG SCALE khi:
   âŒ Code 100% async + I/O-bound
   âŒ CPU idle >50% (overhead cao)
   âŒ ChÆ°a fix blocking code/N+1
   âŒ Memory háº¡n cháº¿
```

### ğŸ¯ Cho mem0-server:

```
âœ… GIáº¢I PHÃP: GIá»® 1 WORKER + TÄ‚NG CPU LIMIT

Config:
- Workers: 1 (async I/O-bound)
- CPU limit: 6.0 cores (tá»« 4.0)
- Memory: 2GB (giá»¯ nguyÃªn)

Expected:
- CPU idle: <30% (sau optimize logging)
- 30 CCU: ~65% CPU (khÃ´ng treo)
- 50 CCU: ~85% CPU (khÃ´ng treo)
- 80 CCU: ~100% CPU (max capacity)
```

---

**TL;DR:**

> **â€œVá»›i async I/O-bound apps: 1 worker + tÄƒng CPU limit > nhiá»u workersâ€**
> 
> TÄƒng workers chá»‰ khi: CPU-bound (â‰¥30%), high traffic (â‰¥10K reqs/s), hoáº·c load test chá»©ng minh cáº£i thiá»‡n â‰¥30%.