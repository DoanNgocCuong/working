# Tá»« váº¥n Ä‘á» gáº·p pháº£i cá»§a mem0-service cháº³ng lÃ m gÃ¬ mÃ  CPU idle Ä‘Ã£ 60-70%, khi cháº¡y thÃ¬ 30-50 CCU lÃºc Ä‘áº§u váº«n oke. CÃ²n lÃºc mÃ  cháº¡y láº§n test thá»© 2 lÃ  bá»‹ treo Ä‘Æ¡ 

# Khi nÃ o dÃ¹ng 1 worker, khi nÃ o dÃ¹ng scale worker 

# ğŸ“˜ HÆ¯á»šNG DáºªN: KHI NÃ€O DÃ™NG 1 WORKER, KHI NÃ€O SCALE WORKERS

---

## ğŸ¯ TÃ“M Táº®T NHANH (TL;DR)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TÃ¬nh huá»‘ng                          â”‚ Sá»‘ workers       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 100% async I/O (API, DB, cache)    â”‚ âœ… 1 worker      â”‚
â”‚ Mixed: 80% I/O + 20% CPU            â”‚ âœ… 1-2 workers   â”‚
â”‚ CPU-bound: ML, image processing     â”‚ âš ï¸ 2-4 workers  â”‚
â”‚ Legacy sync blocking code           â”‚ âš ï¸ 2-4 workers  â”‚
â”‚ High traffic (>10K req/s)           â”‚ âš ï¸ 2-4 workers  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Quy táº¯c vÃ ng:
- Default: Báº®T Äáº¦U vá»›i 1 worker
- Scale khi: Load test chá»©ng minh cáº§n thiáº¿t
- KhÃ´ng scale náº¿u: CPU < 80% under load
```

---

## 1ï¸âƒ£ KHI NÃ€O DÃ™NG 1 WORKER

### âœ… Äiá»u kiá»‡n lÃ½ tÆ°á»Ÿng (Táº¤T Cáº¢ pháº£i thá»a)

#### A. **Code 100% async/await**

```python
# âœ… GOOD: Async code
@app.get("/users")
async def get_users():
    users = await db.query("SELECT * FROM users")  # async
    cache = await redis.get("users")               # async
    result = await api.call("external")            # async
    return users

# âŒ BAD: Blocking code
@app.get("/users")
def get_users():  # Not async!
    users = db.query("SELECT * FROM users")  # Blocking!
    return users
```

**Kiá»ƒm tra:**

```bash
# TÃ¬m sync functions trong codebase:
grep -r "^def " --include="*.py" | grep -v "async def"

# Náº¿u tháº¥y nhiá»u sync functions â†’ cáº§n review
```

---

#### B. **Workload chá»§ yáº¿u I/O-bound**

**Äá»‹nh nghÄ©a I/O-bound:**

- > 70% thá»i gian chá» external services:
    
    - Database queries
    - API calls (HTTP/gRPC)
    - Cache operations (Redis/Memcached)
    - File I/O (Ä‘á»c/ghi disk)
    - Message queues

**VÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh:**

```python
# Typical web API flow (I/O-bound):
async def handle_request():
    user = await db.get_user(id)           # 50ms - I/O
    posts = await db.get_posts(user_id)    # 100ms - I/O
    await cache.set(f"user:{id}", user)    # 10ms - I/O
    await publish_event(user)              # 20ms - I/O
    return format_response(user, posts)    # 5ms - CPU
    
# Total: 185ms
# I/O: 180ms (97%)
# CPU: 5ms (3%)
# â†’ 1 worker Äá»¦
```

**Äo I/O ratio:**

```python
import time

async def profile_request():
    start = time.time()
    
    io_time = 0
    t0 = time.time()
    await db.query()
    io_time += (time.time() - t0)
    
    t0 = time.time()
    await api.call()
    io_time += (time.time() - t0)
    
    total_time = time.time() - start
    io_ratio = io_time / total_time * 100
    print(f"I/O ratio: {io_ratio:.1f}%")
    
# Náº¿u I/O ratio > 70% â†’ dÃ¹ng 1 worker
```

---

#### C. **Traffic vá»«a pháº£i (<5,000 req/s)**

**Benchmark thá»±c táº¿:**

```bash
# Test vá»›i 1 worker:
wrk -t12 -c400 -d30s http://localhost:8000/api/health

# Káº¿t quáº£ tá»‘t:
Requests/sec:   2,500
Latency avg:    160ms
Latency p99:    400ms
CPU usage:      60-80%

# â†’ 1 worker Äá»¦ náº¿u:
# - Throughput Ä‘Ã¡p á»©ng nhu cáº§u
# - Latency p99 < 1s
# - CPU < 80% (cÃ²n dÆ° Ä‘á»ƒ scale)
```

**CÃ´ng thá»©c Æ°á»›c tÃ­nh:**

```
Max throughput (1 worker) = 1000 / avg_latency_ms

VÃ­ dá»¥:
- Avg latency: 100ms â†’ Max: 10 req/s per connection
- Vá»›i 1000 concurrent connections â†’ 10,000 req/s

â†’ 1 worker + async cÃ³ thá»ƒ handle 5,000-10,000 req/s
```

---

#### D. **CPU idle <30% vÃ  CPU under load <80%**

```bash
# Monitoring metrics:
docker stats mem0-server

# Äiá»u kiá»‡n tá»‘t cho 1 worker:
CPU idle:        <30%   (khÃ´ng lÃ£ng phÃ­ resources)
CPU under load:  60-80% (cÃ²n headroom Ä‘á»ƒ scale)
Memory:          <50%   (khÃ´ng bá»‹ memory pressure)

# âŒ Cáº§n scale náº¿u:
CPU idle:        >50%   (quÃ¡ nhiá»u overhead)
CPU under load:  >90%   (saturated, cáº§n thÃªm workers)
```

**VÃ­ dá»¥ phÃ¢n tÃ­ch:**

```
Scenario A: 1 worker
â”œâ”€ CPU idle:  25%
â”œâ”€ CPU load:  70%
â””â”€ Verdict:   âœ… PERFECT (giá»¯ 1 worker)

Scenario B: 1 worker  
â”œâ”€ CPU idle:  55%
â”œâ”€ CPU load:  95%
â””â”€ Verdict:   âš ï¸ Cáº¦N SCALE (CPU maxed out under load)

Scenario C: 1 worker
â”œâ”€ CPU idle:  20%  
â”œâ”€ CPU load:  50%
â””â”€ Verdict:   âœ… OVER-PROVISIONED (cÃ³ thá»ƒ giáº£m resources)
```

---

### ğŸ Lá»£i Ã­ch cá»§a 1 worker

#### 1. **CPU idle tháº¥p**

```
1 worker:  ~25-30% CPU idle
2 workers: ~50-60% CPU idle
4 workers: ~100-120% CPU idle

â†’ Tiáº¿t kiá»‡m 50-75% CPU khi khÃ´ng cÃ³ traffic
```

#### 2. **Memory tiáº¿t kiá»‡m**

```
1 worker:  ~300-400MB
2 workers: ~600-800MB  
4 workers: ~1.2-1.6GB

â†’ Má»—i worker load toÃ n bá»™ app vÃ o memory
```

#### 3. **Debugging Ä‘Æ¡n giáº£n**

```python
# 1 worker: logs dá»… trace
[2026-01-08 10:00:00] Request ID: 123 - Start
[2026-01-08 10:00:01] Request ID: 123 - DB query
[2026-01-08 10:00:02] Request ID: 123 - Done

# 4 workers: logs bá»‹ xen káº½
[Worker 1] Request ID: 123 - Start
[Worker 3] Request ID: 456 - Start  
[Worker 2] Request ID: 789 - Start
[Worker 1] Request ID: 123 - DB query
[Worker 3] Request ID: 456 - Error!
# â†’ KhÃ³ trace!
```

#### 4. **Shared state Ä‘Æ¡n giáº£n**

```python
# 1 worker: in-memory cache works
cache = {}  # Simple dict

@app.get("/data")
async def get_data(key):
    if key in cache:
        return cache[key]
    
    data = await db.query(key)
    cache[key] = data  # Cache works!
    return data

# 4 workers: cache KHÃ”NG share giá»¯a workers
# â†’ Pháº£i dÃ¹ng Redis/Memcached
```

---

## 2ï¸âƒ£ KHI NÃ€O SCALE WORKERS (2-4 workers)

### âš ï¸ Äiá»u kiá»‡n báº¯t buá»™c (ÃT NHáº¤T 1 trong cÃ¡c Ä‘iá»u kiá»‡n)

#### A. **CÃ³ CPU-bound operations (>30% CPU time)**

**Äá»‹nh nghÄ©a CPU-bound:**

- Operations tÃ­nh toÃ¡n náº·ng, khÃ´ng chá» I/O:
    - ML inference (model.predict)
    - Image/video processing
    - Data transformation (pandas, numpy)
    - Cryptography (hash, encrypt)
    - Text processing (regex, parsing)

**VÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh:**

```python
# âŒ CPU-bound: ML inference
@app.post("/predict")
async def predict(data: ImageData):
    # Preprocessing: 50ms CPU
    image = preprocess_image(data)
    
    # ML inference: 500ms CPU (BLOCKING!)
    prediction = model.predict(image)  # Sync call!
    
    # Postprocessing: 50ms CPU
    result = format_prediction(prediction)
    
    return result

# Total: 600ms
# CPU: 600ms (100%)
# I/O: 0ms (0%)
# â†’ Cáº¦N 4 workers Ä‘á»ƒ táº­n dá»¥ng 4 cores
```

**Giáº£i phÃ¡p:**

**Option 1: Scale workers**

```yaml
# docker-compose.yml
command: uvicorn main:app --workers 4

# 4 workers Ã— 500ms = 2000ms capacity/s
# â†’ Throughput: 4 req/s â†’ 8 req/s (2x improvement)
```

**Option 2: Offload to ProcessPoolExecutor (KHUYáº¾N NGHá»Š)**

```python
from concurrent.futures import ProcessPoolExecutor

# Pool riÃªng cho CPU-bound tasks
cpu_pool = ProcessPoolExecutor(max_workers=4)

@app.post("/predict")
async def predict(data: ImageData):
    loop = asyncio.get_event_loop()
    
    # Cháº¡y trong process riÃªng, khÃ´ng block worker
    prediction = await loop.run_in_executor(
        cpu_pool,
        model.predict,
        data
    )
    
    return prediction

# â†’ Giá»¯ 1 worker cho I/O, 4 processes cho CPU
# â†’ Best of both worlds!
```

---

#### B. **Legacy blocking code khÃ´ng thá»ƒ refactor**

```python
# âŒ Blocking sync library
import requests  # Not async!

@app.get("/external")
async def call_external():
    # Sync call blocking worker!
    response = requests.get("https://api.example.com")
    return response.json()

# Giáº£i phÃ¡p táº¡m thá»i: Scale workers
# workers=4 â†’ má»—i worker block 100ms â†’ 40 req/s capacity

# Giáº£i phÃ¡p lÃ¢u dÃ i: Migrate to httpx
import httpx

@app.get("/external")
async def call_external():
    async with httpx.AsyncClient() as client:
        response = await client.get("https://api.example.com")
    return response.json()
```

**Kiá»ƒm tra blocking code:**

```bash
# TÃ¬m sync HTTP clients:
grep -r "import requests" --include="*.py"
grep -r "urllib.request" --include="*.py"

# TÃ¬m sync DB clients:
grep -r "psycopg2" --include="*.py"  # Should be asyncpg
grep -r "pymongo" --include="*.py"   # Should be motor

# Náº¿u tháº¥y nhiá»u â†’ cáº§n workers hoáº·c refactor
```

---

#### C. **High traffic (>10,000 req/s)**

**Load test chá»©ng minh bottleneck:**

```bash
# Test 1: 1 worker
wrk -t12 -c1000 -d30s http://localhost:8000/api/endpoint

Results:
  Requests/sec:   8,500
  Latency avg:    120ms
  Latency p99:    800ms  âŒ (quÃ¡ cao!)
  CPU usage:      95%    âŒ (saturated!)

# Test 2: 4 workers  
wrk -t12 -c1000 -d30s http://localhost:8000/api/endpoint

Results:
  Requests/sec:   15,000  âœ… (tÄƒng 76%)
  Latency avg:    65ms    âœ… (giáº£m 46%)
  Latency p99:    250ms   âœ… (giáº£m 69%)
  CPU usage:      85%     âœ… (balanced)

# â†’ Scale to 4 workers JUSTIFIED
```

**Quy táº¯c:**

```
Náº¿u 1 worker:
- CPU > 90% under load
- Latency p99 > 1s
- Error rate > 1%

â†’ Scale to 2-4 workers
```

---

#### D. **Load test cho tháº¥y cáº£i thiá»‡n rÃµ rá»‡t**

**Decision matrix:**

```bash
# Benchmark 1 worker vs 2 workers vs 4 workers:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Workers  â”‚ Req/s   â”‚ Lat p99  â”‚ CPU %    â”‚ Memory   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1        â”‚ 8,500   â”‚ 800ms    â”‚ 95%      â”‚ 400MB    â”‚
â”‚ 2        â”‚ 12,000  â”‚ 500ms    â”‚ 85%      â”‚ 700MB    â”‚
â”‚ 4        â”‚ 15,000  â”‚ 250ms    â”‚ 80%      â”‚ 1.3GB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PhÃ¢n tÃ­ch:
- 1â†’2 workers: +41% throughput, -37% latency âœ… Tá»‘t
- 2â†’4 workers: +25% throughput, -50% latency âœ… Tá»‘t
- Trade-off: Memory tÄƒng 3.25x

Decision: DÃ¹ng 4 workers náº¿u:
  âœ… Cáº§n latency tháº¥p (<300ms p99)
  âœ… CÃ³ Ä‘á»§ memory (>2GB available)
  âœ… Traffic justify cost
```

---

### âš™ï¸ CÃ´ng thá»©c tÃ­nh sá»‘ workers tá»‘i Æ°u

#### **Cho CPU-bound tasks:**

```python
optimal_workers = min(
    CPU_cores,
    concurrent_requests_avg / (cpu_time_per_request / response_time_target)
)

VÃ­ dá»¥:
- CPU cores: 4
- Concurrent requests: 100
- CPU time per request: 500ms
- Target response time: 200ms

optimal_workers = min(4, 100 / (500/200))
                = min(4, 40)
                = 4 workers

â†’ DÃ¹ng 4 workers (max CPU cores)
```

#### **Cho I/O-bound tasks:**

```python
optimal_workers = 1  # Always!

# Trá»« khi:
if (traffic > 10000 req/s) or (cpu_usage > 90%):
    optimal_workers = 2

# Async + 1 worker cÃ³ thá»ƒ handle hÃ ng nghÃ¬n concurrent connections
```

#### **Mixed workload:**

```python
cpu_ratio = cpu_time / total_time

if cpu_ratio < 0.3:
    optimal_workers = 1
elif cpu_ratio < 0.5:
    optimal_workers = 2
else:
    optimal_workers = min(CPU_cores, 4)
```

---

## 3ï¸âƒ£ DECISION TREE THá»°C CHIáº¾N

```
Báº®T Äáº¦U
   â†“
[1] Code cÃ³ 100% async/await?
   â”œâ”€ NO â†’ [2] CÃ³ thá»ƒ refactor to async?
   â”‚         â”œâ”€ YES â†’ Refactor â†’ [3]
   â”‚         â””â”€ NO â†’ DÃ™NG 2-4 WORKERS âš ï¸
   â”‚
   â””â”€ YES â†’ [3]

[3] Workload type?
   â”œâ”€ I/O-bound (>70% I/O time) â†’ [4]
   â”œâ”€ CPU-bound (>70% CPU time) â†’ DÃ™NG 2-4 WORKERS âš ï¸
   â””â”€ Mixed â†’ [5]

[4] Traffic level?
   â”œâ”€ Low (<5K req/s) â†’ DÃ™NG 1 WORKER âœ…
   â”œâ”€ Medium (5-10K) â†’ Load test â†’ [6]
   â””â”€ High (>10K) â†’ DÃ™NG 2-4 WORKERS âš ï¸

[5] CPU ratio?
   â”œâ”€ <30% CPU â†’ DÃ™NG 1 WORKER âœ…
   â”œâ”€ 30-50% â†’ DÃ™NG 1-2 WORKERS âš ï¸
   â””â”€ >50% â†’ DÃ™NG 2-4 WORKERS âš ï¸

[6] Load test results?
   â”œâ”€ CPU < 80%, latency OK â†’ DÃ™NG 1 WORKER âœ…
   â”œâ”€ CPU > 90% â†’ DÃ™NG 2 WORKERS âš ï¸
   â””â”€ CPU maxed + errors â†’ DÃ™NG 4 WORKERS âš ï¸
```

---

## 4ï¸âƒ£ ÃP Dá»¤NG CHO mem0-server

### PhÃ¢n tÃ­ch hiá»‡n tráº¡ng

```python
# mem0-server workload:
async def add_memory(text: str):
    # 1. Preprocessing (CPU)
    chunks = chunk_text(text)              # 50ms - CPU
    
    # 2. Embedding (I/O - external service)
    vectors = await embed(chunks)          # 200ms - I/O
    
    # 3. Vector search (I/O - Milvus)
    results = await milvus.search(vectors) # 100ms - I/O
    
    # 4. LLM generation (I/O - external)
    summary = await llm.generate(text)     # 100ms - I/O
    
    # 5. Store (I/O - Milvus)
    await milvus.insert(vectors)           # 50ms - I/O
    
    return {"status": "ok"}

# Total: 500ms
# CPU: 50ms (10%)
# I/O: 450ms (90%)
# â†’ I/O-bound workload âœ…
```

### Káº¿t luáº­n cho mem0-server

**âœ… NÃŠN DÃ™NG 1 WORKER vÃ¬:**

1. âœ… **Code 100% async**
    
    - FastAPI vá»›i async/await
    - Embedding service: httpx (async)
    - Milvus: pymilvus async client
    - LLM: async client
2. âœ… **Workload I/O-bound (90%)**
    
    - Chá»§ yáº¿u chá» external services
    - CPU chá»‰ 10% (preprocessing)
3. âœ… **Traffic vá»«a pháº£i**
    
    - KhÃ´ng cÃ³ nhu cáº§u >10K req/s
    - Latency target: <1s
4. âœ… **Test results support**
    
    - CPU idle giáº£m tá»« 116% â†’ 54%
    - Memory giáº£m tá»« 833MB â†’ 386MB
    - CPU peak váº«n á»•n (407%)

**âš ï¸ Cáº¦N MONITOR:**

- Throughput dÃ i háº¡n
- Latency p99 khi cÃ³ load
- Error rate

**ğŸ”„ KHI NÃ€O SCALE:**

- Khi traffic > 5,000 req/s
- Khi CPU > 90% under load
- Khi latency p99 > 1s

---

## 5ï¸âƒ£ BEST PRACTICES

### âœ… Quy trÃ¬nh Ä‘Ãºng

```
Step 1: Báº®T Äáº¦U Vá»šI 1 WORKER
â”œâ”€ Default cho má»i async app
â”œâ”€ Deploy to production
â””â”€ Monitor metrics

Step 2: MONITOR 1-2 TUáº¦N
â”œâ”€ CPU usage (idle & load)
â”œâ”€ Memory usage
â”œâ”€ Latency (p50, p95, p99)
â”œâ”€ Throughput (req/s)
â””â”€ Error rate

Step 3: ÄÃNH GIÃ
â”œâ”€ Náº¿u CPU < 80% & latency OK
â”‚   â†’ GIá»® 1 WORKER âœ…
â”‚
â”œâ”€ Náº¿u CPU > 90% hoáº·c latency cao
â”‚   â†’ CHáº Y LOAD TEST vá»›i 2 workers
â”‚
â””â”€ Náº¿u load test cáº£i thiá»‡n >30%
    â†’ SCALE TO 2 WORKERS

Step 4: ITERATE
â”œâ”€ Monitor sau khi scale
â”œâ”€ So sÃ¡nh metrics before/after
â””â”€ Adjust náº¿u cáº§n
```

### âŒ Anti-patterns (trÃ¡nh)

```
âŒ "CÃ ng nhiá»u workers cÃ ng tá»‘t"
   â†’ Overhead cao, lÃ£ng phÃ­ resources

âŒ "DÃ¹ng workers = CPU cores"
   â†’ Chá»‰ Ä‘Ãºng cho CPU-bound, sai cho I/O-bound

âŒ "Scale workers trÆ°á»›c khi profile"
   â†’ Fix root cause trÆ°á»›c (blocking code, N+1 queries)

âŒ "Set workers má»™t láº§n rá»“i quÃªn"
   â†’ Pháº£i monitor & adjust theo workload thá»±c táº¿

âŒ "Tin benchmark synthetic"
   â†’ Pháº£i test vá»›i production traffic pattern
```

---

## 6ï¸âƒ£ MONITORING & ALERTING

### Metrics cáº§n theo dÃµi

```yaml
# Prometheus metrics:
http_requests_total
http_request_duration_seconds
http_requests_in_flight

# CPU metrics:
container_cpu_usage_seconds_total
process_cpu_seconds_total

# Alerts:
- alert: HighCPUUsage
  expr: rate(container_cpu_usage_seconds_total[5m]) > 0.9
  for: 5m
  annotations:
    summary: "CPU > 90% for 5 minutes"
    
- alert: HighLatency
  expr: histogram_quantile(0.99, http_request_duration_seconds) > 1
  for: 5m
  annotations:
    summary: "P99 latency > 1s"
```

### Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Workers: 1                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CPU Usage (5m avg)                          â”‚
â”‚ Idle:  28% â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘        â”‚
â”‚ Load:  75% â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Latency                                     â”‚
â”‚ P50:   80ms                                 â”‚
â”‚ P95:  200ms                                 â”‚
â”‚ P99:  450ms âœ…                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Throughput                                  â”‚
â”‚ Current: 2,500 req/s                        â”‚
â”‚ Errors:  0.1%      âœ…                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Decision: âœ… Giá»¯ 1 worker (metrics healthy)
```

---

## ğŸ“‹ CHEAT SHEET

```bash
# 1. Kiá»ƒm tra code cÃ³ async khÃ´ng
grep -r "^def " app/ | wc -l  # Sync functions
grep -r "^async def " app/ | wc -l  # Async functions
# Ratio async/(async+sync) > 90% â†’ OK

# 2. Profile workload
python -m cProfile -o profile.stats main.py
python -c "import pstats; pstats.Stats('profile.stats').sort_stats('cumtime').print_stats(20)"
# TÃ¬m top 20 functions tá»‘n CPU

# 3. Load test vá»›i 1 worker
docker-compose up -d
wrk -t12 -c400 -d30s http://localhost:8000/health
# Note: req/s, latency p99, CPU%

# 4. Load test vá»›i 2 workers
# Sá»­a workers: 2 trong docker-compose.yml
docker-compose restart
wrk -t12 -c400 -d30s http://localhost:8000/health
# So sÃ¡nh vá»›i 1 worker

# 5. Decision
if [ improvement > 30% ]; then
    echo "Scale to 2 workers"
else
    echo "Keep 1 worker"
fi
```

---

## ğŸ¯ Káº¾T LUáº¬N

### Quy táº¯c vÃ ng

```
DEFAULT: 1 worker cho async apps

SCALE KHI:
âœ… Load test chá»©ng minh cáº£i thiá»‡n >30%
âœ… CPU > 90% under production load
âœ… CÃ³ CPU-bound operations khÃ´ng thá»ƒ offload
âœ… Traffic > 10,000 req/s

KHÃ”NG SCALE KHI:
âŒ Chá»‰ dá»±a vÃ o benchmark synthetic
âŒ CPU idle cao (overhead tá»« workers)
âŒ ChÆ°a fix blocking code/N+1 queries
âŒ Memory khÃ´ng Ä‘á»§ (workers tá»‘n RAM)
```

### Cho mem0-server

```
âœ… KHUYáº¾N NGHá»Š: GIá»® 1 WORKER

LÃ½ do:
1. Code 100% async
2. Workload 90% I/O-bound
3. Traffic vá»«a pháº£i (<5K req/s)
4. Metrics healthy (CPU 54% idle, 407% peak)
5. Memory tiáº¿t kiá»‡m (386MB vs 833MB)

Next steps:
1. Monitor 1-2 tuáº§n
2. Profile CPU peak (407%)
3. Fix blocking I/O náº¿u cÃ³
4. Reeval khi traffic tÄƒng
```