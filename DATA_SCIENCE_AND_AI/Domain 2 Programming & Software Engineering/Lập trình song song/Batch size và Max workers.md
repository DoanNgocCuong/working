```python 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Parallel Processing Module for Main Pipeline
===========================================

Features:
- Adaptive batch sizing based on data complexity
- Smart worker management with performance monitoring
- Thread-safe result collection
- Progress tracking and ETA calculation
- Error handling and retry mechanisms
- Memory usage optimization
"""

import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from typing import List, Dict, Any, Callable, Optional
import psutil
import math

class ParallelProcessor:
    """
    Advanced parallel processor with adaptive optimization
    """
    
    def __init__(self, 
                 max_workers: Optional[int] = None,
                 batch_size: Optional[int] = None,
                 memory_limit_mb: int = 1024,
                 enable_adaptive: bool = True,
                 timeout_seconds: int = 300):
        """
        Initialize parallel processor
        
        Args:
            max_workers: Maximum number of worker threads (None = auto-detect)
            batch_size: Items per batch (None = auto-calculate)
            memory_limit_mb: Memory limit in MB
            enable_adaptive: Enable adaptive optimization
        """
        self.max_workers = max_workers or self._auto_detect_workers()
        self.batch_size = batch_size
        self.memory_limit_mb = memory_limit_mb
        self.enable_adaptive = enable_adaptive
        self.timeout_seconds = timeout_seconds
        
        # Performance tracking
        self.start_time = None
        self.completed_items = 0
        self.total_items = 0
        self.error_count = 0
        self.retry_count = 0
        
        # Thread safety
        self.lock = threading.Lock()
        self.results = []
        
        # Memory monitoring
        self.process = psutil.Process()
        
    def _auto_detect_workers(self) -> int:
        """Auto-detect optimal number of workers based on system"""
        cpu_count = psutil.cpu_count(logical=True)
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # Conservative approach: don't overwhelm the system
        if memory_gb < 4:
            return min(2, cpu_count)
        elif memory_gb < 8:
            return min(4, cpu_count)
        else:
            return min(8, cpu_count)
    
    def _calculate_optimal_batch_size(self, total_items: int, avg_time_per_item: float = None) -> int:
        """Calculate optimal batch size based on data characteristics"""
        if self.batch_size:
            return self.batch_size
            
        # Adaptive batch sizing
        if self.enable_adaptive:
            # Estimate based on available memory
            available_memory_mb = psutil.virtual_memory().available / (1024**2)
            estimated_memory_per_item = 50  # MB per item (conservative estimate)
            
            memory_based_batch = max(1, int(available_memory_mb / estimated_memory_per_item))
            
            # Time-based optimization
            if avg_time_per_item:
                # Aim for batches that take 30-60 seconds to process
                target_batch_time = 45  # seconds
                time_based_batch = max(1, int(target_batch_time / avg_time_per_item))
            else:
                time_based_batch = 10
            
            # Worker-based optimization
            worker_based_batch = max(1, total_items // (self.max_workers * 2))
            
            # Take the minimum to be conservative
            optimal_batch = min(memory_based_batch, time_based_batch, worker_based_batch)
            
            return max(1, min(optimal_batch, 50))  # Between 1 and 50
        
        return 10  # Default fallback
    
    def _create_batches(self, items: List[Any], batch_size: int) -> List[List[Any]]:
        """Create batches from items list"""
        return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]
    
    def _monitor_memory(self) -> float:
        """Monitor current memory usage"""
        memory_info = self.process.memory_info()
        memory_mb = memory_info.rss / (1024**2)
        return memory_mb
    
    def _should_pause_for_memory(self) -> bool:
        """Check if we should pause for memory management"""
        current_memory = self._monitor_memory()
        return current_memory > self.memory_limit_mb
    
    def process_parallel(self, 
                        items: List[Any], 
                        process_func: Callable,
                        progress_callback: Optional[Callable] = None,
                        **kwargs) -> List[Any]:
        """
        Process items in parallel with advanced optimization
        
        Args:
            items: List of items to process
            process_func: Function to process each batch
            progress_callback: Optional callback for progress updates
            **kwargs: Additional arguments for process_func
            
        Returns:
            List of results from all batches
        """
        self.total_items = len(items)
        self.completed_items = 0
        self.start_time = datetime.now()
        self.results = []
        
        if not items:
            return []
        
        # Calculate optimal batch size
        batch_size = self._calculate_optimal_batch_size(len(items))
        batches = self._create_batches(items, batch_size)
        
        print(f"üöÄ Parallel Processing Setup:")
        print(f"   üìä Total items: {self.total_items}")
        print(f"   üì¶ Batch size: {batch_size}")
        print(f"   üßµ Max workers: {self.max_workers}")
        print(f"   üìã Total batches: {len(batches)}")
        print(f"   üíæ Memory limit: {self.memory_limit_mb}MB")
        print(f"   üîß Adaptive mode: {'‚úÖ' if self.enable_adaptive else '‚ùå'}")
        
        # Process batches in parallel
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all batches - filter out timeout_seconds from kwargs
            process_kwargs = {k: v for k, v in kwargs.items() if k != 'timeout_seconds'}
            future_to_batch = {
                executor.submit(self._process_batch_with_retry, batch, process_func, **process_kwargs): batch
                for batch in batches
            }
            
            # Process completed batches with timeout
            for future in as_completed(future_to_batch):
                try:
                    # Add timeout to prevent hanging
                    batch_result = future.result(timeout=self.timeout_seconds)
                    with self.lock:
                        self.results.extend(batch_result)
                        self.completed_items += len(batch_result)
                    
                    # Progress update
                    if progress_callback:
                        progress_callback(self.completed_items, self.total_items)
                    
                    # Memory management
                    if self._should_pause_for_memory():
                        print(f"‚ö†Ô∏è  High memory usage ({self._monitor_memory():.1f}MB), pausing...")
                        time.sleep(1)  # Brief pause
                    
                    # Performance monitoring
                    self._log_progress()
                    
                except TimeoutError:
                    self.error_count += 1
                    print(f"‚è∞ Batch processing timeout after {self.timeout_seconds}s, skipping...")
                    continue
                except Exception as e:
                    self.error_count += 1
                    print(f"‚ùå Batch processing error: {e}")
                    continue
        
        # Final summary
        self._log_final_summary()
        return self.results
    
    def _process_batch_with_retry(self, 
                                 batch: List[Any], 
                                 process_func: Callable,
                                 max_retries: int = 2,
                                 **kwargs) -> List[Any]:
        """Process a batch with retry mechanism"""
        for attempt in range(max_retries + 1):
            try:
                return process_func(batch, **kwargs)
            except Exception as e:
                if attempt < max_retries:
                    self.retry_count += 1
                    print(f"üîÑ Retry {attempt + 1}/{max_retries} for batch due to: {e}")
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    print(f"üí• Batch failed after {max_retries} retries: {e}")
                    return []  # Return empty result for failed batch
    
    def _log_progress(self):
        """Log current progress with ETA"""
        if self.completed_items == 0:
            return
            
        elapsed_time = (datetime.now() - self.start_time).total_seconds()
        avg_time_per_item = elapsed_time / self.completed_items
        remaining_items = self.total_items - self.completed_items
        estimated_remaining_time = avg_time_per_item * remaining_items
        
        progress_percent = (self.completed_items / self.total_items) * 100
        eta = datetime.now() + timedelta(seconds=estimated_remaining_time)
        
        memory_usage = self._monitor_memory()
        
        print(f"üìä Progress: {self.completed_items}/{self.total_items} ({progress_percent:.1f}%)")
        print(f"   ‚è±Ô∏è  Elapsed: {elapsed_time/60:.1f}min | ETA: {eta.strftime('%H:%M:%S')}")
        print(f"   üöÄ Speed: {avg_time_per_item:.1f}s/item | üíæ Memory: {memory_usage:.1f}MB")
        print(f"   ‚ùå Errors: {self.error_count} | üîÑ Retries: {self.retry_count}")
    
    def _log_final_summary(self):
        """Log final processing summary"""
        total_time = (datetime.now() - self.start_time).total_seconds()
        success_rate = ((self.total_items - self.error_count) / self.total_items) * 100
        
        print(f"\nüéâ Parallel Processing Complete!")
        print(f"   ‚è±Ô∏è  Total time: {total_time/60:.1f} minutes")
        print(f"   üìä Success rate: {success_rate:.1f}%")
        print(f"   ‚ùå Errors: {self.error_count}")
        print(f"   üîÑ Retries: {self.retry_count}")
        print(f"   üíæ Peak memory: {self._monitor_memory():.1f}MB")
        
        if self.total_items > 0:
            throughput = self.total_items / total_time
            print(f"   üöÄ Throughput: {throughput:.2f} items/second")


# Convenience functions for easy integration
def process_situations_parallel(situations: List[Dict], 
                               process_func: Callable,
                               max_workers: int = None,
                               batch_size: int = None,
                               timeout_seconds: int = 300,
                               **kwargs) -> List[Dict]:
    """
    Convenience function to process situations in parallel
    
    Args:
        situations: List of situation dictionaries
        process_func: Function to process each situation
        max_workers: Maximum worker threads
        batch_size: Items per batch
        **kwargs: Additional arguments for process_func
        
    Returns:
        List of processed results
    """
    processor = ParallelProcessor(
        max_workers=max_workers,
        batch_size=batch_size,
        enable_adaptive=True
    )
    
    def progress_callback(completed, total):
        if completed % 10 == 0:  # Update every 10 items
            print(f"üîÑ Processed {completed}/{total} situations...")
    
    return processor.process_parallel(
        items=situations,
        process_func=process_func,
        progress_callback=progress_callback,
        timeout_seconds=timeout_seconds,
        **kwargs
    )


if __name__ == "__main__":
    # Demo usage
    def demo_process_func(batch, **kwargs):
        """Demo processing function"""
        time.sleep(0.1)  # Simulate work
        return [f"Processed: {item}" for item in batch]
    
    # Test data
    test_items = list(range(100))
    
    print("üß™ Testing Parallel Processor...")
    processor = ParallelProcessor(max_workers=4, batch_size=10)
    results = processor.process_parallel(test_items, demo_process_func)
    
    print(f"‚úÖ Demo completed with {len(results)} results")
```


|Ti√™u ch√≠|max_workers|batch_size|
|---|---|---|
|ƒê·ªãnh nghƒ©a|S·ªë l∆∞·ª£ng lu·ªìng (thread) ho·∫∑c ti·∫øn tr√¨nh (process) ch·∫°y song song c√πng l√∫c|S·ªë l∆∞·ª£ng d·ªØ li·ªáu (item) x·ª≠ l√Ω trong m·ªói l∆∞·ª£t c·ªßa m·ªôt worker|
|Vai tr√≤|TƒÉng ƒë·ªô song song (parallelism c·∫•p h·ªá th·ªëng)|T·ªëi ∆∞u hi·ªáu nƒÉng, memory trong t·ª´ng l∆∞·ª£t x·ª≠ l√Ω c·ªßa worker|
|T√°c ƒë·ªông|·∫¢nh h∆∞·ªüng t·ªëc ƒë·ªô to√†n pipeline v√† m·ª©c ti√™u th·ª• CPU/RAM|·∫¢nh h∆∞·ªüng hi·ªáu qu·∫£ t·ª´ng worker, m·ª©c ti√™u th·ª• RAM t·ª´ng batch|
|Gi·ªõi h·∫°n c·∫•u h√¨nh|B·ªã gi·ªõi h·∫°n b·ªüi s·ªë nh√¢n CPU v√† t·ªïng RAM h·ªá th·ªëng|B·ªã gi·ªõi h·∫°n b·ªüi dung l∆∞·ª£ng RAM t·ª´ng worker v√† kh·∫£ nƒÉng x·ª≠ l√Ω l·ªói|
|V√≠ d·ª• gi√° tr·ªã|4, 8, 16, 32|10, 20, 50, 100|
|·∫¢nh h∆∞·ªüng khi tƒÉng|X·ª≠ l√Ω nhanh h∆°n, chi·∫øm nhi·ªÅu t√†i nguy√™n h·ªá th·ªëng h∆°n, d·ªÖ qu√° t·∫£i n·∫øu qu√° cao|M·ªói worker x·ª≠ l√Ω nhanh h∆°n/l·ªõn h∆°n m·ªói l∆∞·ª£t, nh∆∞ng d·ªÖ l·ªói, timeout, ng·ªën nhi·ªÅu RAM h∆°n|
|·ª®ng d·ª•ng th·ª±c t·∫ø|Thi·∫øt l·∫≠p song song t·ªëi ∆∞u cho server m·∫°nh ho·∫∑c d·ªØ li·ªáu l·ªõn l√¢u|Chia batch nh·ªè d·ªÖ retry, ng·ªën √≠t RAM (ph√π h·ª£p d·ªØ li·ªáu si√™u l·ªõn ho·∫∑c nhi·ªÅu l·ªói)|
|T·ªëi ∆∞u h√≥a c√πng nhau|Ch·ªçn max_workers d·ª±a tr√™n CPU/RAM t·ªïng, t√πy ch·ªânh batch_size nh·ªè/l·ªõn t·ªëi ∆∞u memory, t·ªëc ƒë·ªô|TƒÉng c·∫£ hai gi√∫p h·ªá th·ªëng x·ª≠ l√Ω c·ª±c nhanh n·∫øu c√¢n ƒë·ªëi t√†i nguy√™n|
|H·ªá qu·∫£ khi sai c·∫•u h√¨nh|max_workers qu√° l·ªõn khi·∫øn RAM/cpu c·∫°n ki·ªát, h·ªá th·ªëng treo|batch_size qu√° l·ªõn khi·∫øn worker d·ªÖ b·ªã l·ªói out-of-memory, timeout|
|Th√¥ng s·ªë trong module|max_workers:ThreadPoolExecutor(max_workers=...)|batch_size: chia nh·ªè d·ªØ li·ªáu truy·ªÅn v√†o m·ªói worker|