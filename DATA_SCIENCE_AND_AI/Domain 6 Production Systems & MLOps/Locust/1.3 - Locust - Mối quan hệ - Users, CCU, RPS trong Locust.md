## Mối quan hệ: Users, CCU, RPS trong Locust

### Công thức cốt lõi

```
RPS = Users / (Response Time + Wait Time)
```

### Giải thích

| Thuật ngữ          | Ý nghĩa                                           |
| ------------------ | ------------------------------------------------- |
| **Users (Locust)** | Số virtual users đang chạy đồng thời = CCU        |
| **CCU**            | Concurrent Users = chính là số Users trong Locust |
| **RPS**            | Số requests server nhận được mỗi giây             |

### Ví dụ thực tế

```
Users = 100
Response Time = 0.1s (100ms)
Wait Time = 2.0s (trung bình của 1-3s)

RPS = 100 / (0.1 + 2.0) = 100 / 2.1 ≈ 48 RPS
```

### Tại sao 100 users không phải 100 RPS?

Vì mỗi user phải **đợi** giữa các request:

```
User 1: [Request 0.1s] → [Wait 2s] → [Request 0.1s] → [Wait 2s] → ...
User 2: [Request 0.1s] → [Wait 2s] → [Request 0.1s] → [Wait 2s] → ...
...
```

Mỗi user chỉ gửi được **1 request / 2.1 giây**, không phải 1 request/giây.

### Muốn tăng RPS?

|Cách|Hệ quả|
|---|---|
|Tăng Users|Tăng tải server|
|Giảm Wait Time|Tăng tần suất request/user|
|Giảm Response Time|Server xử lý nhanh hơn|


# Ơ, nhưng nếu đặt wait là 1-3 giây thì khi để 100 user dùng thì RPS có 48 ??? THẾ ĐÃ PHẢN ÁNH ĐÚNG cái CCU=100 chưa ?

## RPS 48 Có Phản Ánh Đúng CCU=100 Không?

**Có, hoàn toàn đúng.** Với 100 user, response time 0.1s + wait 1-3s (trung bình 2s), mỗi user gửi ~1 request / 2.1s, nên tổng RPS ≈ 100 / 2.1 = 48. Đây chính là CCU=100 thực tế, vì Locust Users = số người dùng đồng thời đang hoạt động song song.[github+1](https://github.com/locustio/locust/issues/646)​

## Tại Sao RPS < Users Là Bình Thường?

- **Mỗi user có chu kỳ riêng**: User1 gửi request → chờ 2s → gửi tiếp; User2 lệch pha một chút → tổng 48 request/giây từ 100 user cùng lúc.[github](https://github.com/locustio/locust/issues/646)​
    
- **Công thức chuẩn**: RPS = Users / (Response Time + Avg Wait Time), khớp chính xác ví dụ của bạn.[stackoverflow+1](https://stackoverflow.com/questions/27713104/locust-io-controlling-the-request-per-second-parameter)​
    
- **CCU server cũng =100**: Server thấy ~100 kết nối đồng thời, xử lý 48 req/s - phản ánh đúng tải thực tế.[stackoverflow](https://stackoverflow.com/questions/68466767/locust-take-response-time-out-of-rps-equation)​
    

## Kiểm Tra Thực Tế

Chạy test và xem server logs: số concurrent connections ≈100, RPS≈48 → đã đúng. Nếu muốn RPS=100, giảm wait_time xuống ~0s (nhưng không thực tế cho web app) hoặc dùng `constant_throughput(1)`. Stress test mục tiêu là CCU thực, không phải RPS max.[locust+1](https://docs.locust.io/en/stable/writing-a-locustfile.html)​

1. [https://github.com/locustio/locust/issues/646](https://github.com/locustio/locust/issues/646)
2. [https://docs.locust.io/en/stable/writing-a-locustfile.html](https://docs.locust.io/en/stable/writing-a-locustfile.html)
3. [https://github.com/locustio/locust/issues/2399](https://github.com/locustio/locust/issues/2399)
4. [https://stackoverflow.com/questions/27713104/locust-io-controlling-the-request-per-second-parameter](https://stackoverflow.com/questions/27713104/locust-io-controlling-the-request-per-second-parameter)
5. [https://learn.microsoft.com/en-us/azure/app-testing/load-testing/concept-load-testing-concepts](https://learn.microsoft.com/en-us/azure/app-testing/load-testing/concept-load-testing-concepts)
6. [https://docs.locust.io/en/stable/api.html](https://docs.locust.io/en/stable/api.html)
7. [https://coffeebytes.dev/en/python/how-to-measure-requests-per-second-with-locust-in-python/](https://coffeebytes.dev/en/python/how-to-measure-requests-per-second-with-locust-in-python/)
8. [https://community.cratedb.com/t/load-testing-cratedb-using-locust/1686](https://community.cratedb.com/t/load-testing-cratedb-using-locust/1686)
9. [https://www.diva-portal.org/smash/get/diva2:685934/FULLTEXT01.pdf](https://www.diva-portal.org/smash/get/diva2:685934/FULLTEXT01.pdf)
10. [https://stackoverflow.com/questions/68466767/locust-take-response-time-out-of-rps-equation](https://stackoverflow.com/questions/68466767/locust-take-response-time-out-of-rps-equation)




---

Đang kiểm tra các file Locust/stress test để xác định CCU.


[4 tools called]


Đang đọc thêm chi tiết từ stress test report:


[2 tools called]


## Trả lời: Locust "Users" = CCU

Trong Locust:
- "Users" = Concurrent Users (CCU)
- 100 users = 100 CCU
- 200 users = 200 CCU

---

## Tính toán graceful shutdown timeout dựa trên CCU thực tế

### Từ stress test report của bạn:

| Metric | 100 CCU | 200 CCU |
|--------|---------|---------|
| **Concurrent Users** | 100 | 200 |
| **Response Time** | 200-500ms (max 554ms) | 200-500ms (max 2,271ms) |
| **Wait Time** | 1-3 giây giữa requests | 1-3 giây giữa requests |
| **RPS** | 48.8 req/s | 57.6 req/s |

---

## Tính toán requests đồng thời khi shutdown

### Scenario: Shutdown khi có 200 CCU

```
Phân tích:
├─ 200 CCU đang chạy
├─ Wait time: 1-3s → Requests được spread ra
├─ Không phải tất cả users đều gửi request cùng lúc
└─ Thường chỉ có 20-40% users đang xử lý request tại một thời điểm

Tính toán:
├─ 200 CCU × 30% = 60 requests đồng thời (realistic)
├─ Worst case: 200 CCU × 100% = 200 requests đồng thời (unlikely)
└─ Response time: 200-500ms (max 554ms)
```

### Tính timeout:

```
Realistic case (30% active):
├─ 60 requests × 500ms = 30 giây
└─ Với buffer 1.5x: 30s × 1.5 = 45 giây

Worst case (100% active):
├─ 200 requests × 554ms = 110.8 giây
└─ Với buffer 1.5x: 110.8s × 1.5 = 166 giây (quá cao!)
```

---

## Khuyến nghị timeout dựa trên CCU

### Option 1: Conservative (khuyến nghị)

```dockerfile
CMD ["uvicorn", "app.main_app:app", \
     "--host", "0.0.0.0", \
     "--port", "30020", \
     "--timeout-graceful-shutdown", "60", \
     "--no-reload"]
```

Lý do:
- Đủ cho 200 CCU với 30% active (60 requests × 500ms = 30s)
- Buffer 2x để an toàn (60s)
- Cover worst case khi có requests dài hơn

### Option 2: Balanced (nếu production thường < 100 CCU)

```dockerfile
CMD ["uvicorn", "app.main_app:app", \
     "--host", "0.0.0.0", \
     "--port", "30020", \
     "--timeout-graceful-shutdown", "30", \
     "--no-reload"]
```

Lý do:
- Đủ cho 100 CCU với 30% active (30 requests × 500ms = 15s)
- Buffer 2x (30s)
- Deploy nhanh hơn

### Option 3: Very conservative (nếu muốn cover worst case)

```dockerfile
CMD ["uvicorn", "app.main_app:app", \
     "--host", "0.0.0.0", \
     "--port", "30020", \
     "--timeout-graceful-shutdown", "90", \
     "--no-reload"]
```

Lý do:
- Cover 200 CCU worst case (200 requests × 500ms = 100s)
- Rất an toàn, nhưng deploy chậm hơn

---

## So sánh

| CCU | Active Requests | Timeout cần | Khuyến nghị |
|-----|----------------|-------------|-------------|
| 100 CCU | ~30 requests | 15s | 30s (buffer 2x) |
| 200 CCU | ~60 requests | 30s | 60s (buffer 2x) |
| Worst case | 200 requests | 110s | 90s (cover 80%) |

---

## Kết luận

- Locust "Users" = CCU
- Với 200 CCU: nên set `--timeout-graceful-shutdown 60`
- Với 100 CCU: có thể set 30s
- Khuyến nghị: 60s để cover cả 100 và 200 CCU

Bạn muốn dùng timeout bao nhiêu? 30s, 60s, hay 90s?


---


## CCU là gì?

CCU = Concurrent Users = Số người dùng đồng thời

---

## Giải thích chi tiết

### CCU (Concurrent Users)

- Concurrent = đồng thời, cùng lúc
- Users = người dùng (trong context này là virtual users trong Locust)
- CCU = số lượng users đang hoạt động cùng lúc

---

## Ví dụ

### Scenario: Locust test với 100 users

```
Timeline:

T=0s:    Locust spawn 100 users
         ├─ User 1: Gửi request → Chờ response
         ├─ User 2: Gửi request → Chờ response
         ├─ User 3: Gửi request → Chờ response
         ├─ ...
         └─ User 100: Gửi request → Chờ response

T=0.3s:  User 1 nhận response → Chờ 1-3s → Gửi request tiếp
T=0.4s:  User 2 nhận response → Chờ 1-3s → Gửi request tiếp
...

→ 100 users đang chạy đồng thời = 100 CCU
```

---

## So sánh với các khái niệm khác

| Thuật ngữ | Ý nghĩa | Ví dụ |
|-----------|---------|-------|
| **CCU** | Số users đang hoạt động cùng lúc | 100 CCU = 100 users đang chạy |
| **RPS** | Requests per second (số requests/giây) | 48.8 RPS = 48.8 requests/giây |
| **TPS** | Transactions per second | Tương tự RPS |
| **Active Users** | Users đang có session/connection | Tương tự CCU |

---

## Trong context của bạn

### Từ Locust test:

```
100 Users = 100 CCU
├─ 100 virtual users đang chạy đồng thời
├─ Mỗi user gửi request, chờ response, rồi gửi request tiếp
└─ Tổng: 100 users × requests = Total load

200 Users = 200 CCU
├─ 200 virtual users đang chạy đồng thời
└─ Load cao gấp đôi so với 100 CCU
```

---

## Tại sao quan trọng với graceful shutdown?

### Khi shutdown với 200 CCU:

```
Scenario:
├─ 200 CCU đang chạy
├─ Mỗi user có thể đang xử lý 1 request
├─ Khi shutdown: Chờ tất cả requests đang xử lý xong
└─ Có thể có 200 requests đang xử lý đồng thời

Tính toán:
├─ 200 requests × 500ms = 100 giây (worst case)
└─ Cần timeout đủ lớn để cover
```

---

## Kết luận

- CCU = Concurrent Users = Số người dùng đồng thời
- 100 CCU = 100 users đang chạy cùng lúc
- 200 CCU = 200 users đang chạy cùng lúc
- Quan trọng cho graceful shutdown: càng nhiều CCU, càng cần timeout lớn hơn

Bạn hiểu rồi chứ?


---

## Sự khác biệt: CCU vs Concurrent Requests

### CCU (Concurrent Users) = Số người đang chạy
- 100 CCU = 100 users đang chạy
- Mỗi user có thể gửi nhiều requests theo thời gian

### Concurrent Requests = Số requests đang xử lý tại một thời điểm
- 50 concurrent requests = 50 requests đang được server xử lý ngay bây giờ
- Không phải tất cả users đều đang gửi request cùng lúc

---

## Ví dụ

### Scenario: 100 CCU với wait time 1-3s

```
Timeline:

T=0.0s:  User 1 gửi Request A → Server xử lý (mất 0.3s)
T=0.1s:  User 2 gửi Request B → Server xử lý (mất 0.4s)
T=0.2s:  User 3 gửi Request C → Server xử lý (mất 0.5s)
T=0.3s:  Request A xong → User 1 chờ 1-3s (không gửi request)
T=0.4s:  Request B xong → User 2 chờ 1-3s (không gửi request)
T=0.5s:  Request C xong → User 3 chờ 1-3s (không gửi request)

T=1.0s:  User 1 chờ xong → Gửi Request D
T=1.5s:  User 2 chờ xong → Gửi Request E
...

Phân tích:
├─ CCU = 100 users (luôn có 100 users đang chạy)
├─ Concurrent requests tại T=0.0s = 3 requests (chỉ 3 users đang gửi)
├─ Concurrent requests tại T=0.5s = 0 requests (tất cả đang chờ)
└─ Concurrent requests tại T=1.0s = 1 request (chỉ 1 user gửi)
```

---

## So sánh trực quan

| Thời điểm | CCU | Concurrent Requests | Giải thích |
|-----------|-----|---------------------|------------|
| T=0.0s | 100 | 3 | 3 users vừa gửi request |
| T=0.3s | 100 | 2 | Request A xong, còn 2 đang xử lý |
| T=0.5s | 100 | 0 | Tất cả requests xong, users đang chờ |
| T=1.0s | 100 | 1 | User 1 chờ xong, gửi request mới |
| T=2.0s | 100 | 5 | Nhiều users chờ xong, gửi requests |

---

## Tại sao khác nhau?

### 1. Wait time giữa requests

```
User behavior:
├─ Gửi request → Chờ response (0.3s)
├─ Nhận response → Chờ 1-3s (wait_time)
└─ Gửi request tiếp

→ Không phải lúc nào user cũng đang gửi request
→ Chỉ có một phần users đang có request đang xử lý
```

### 2. Response time

```
Nếu response time = 0.3s và wait_time = 1-3s:
├─ User active time = 0.3s (gửi + nhận)
├─ User idle time = 1-3s (chờ)
└─ Active ratio = 0.3s / (0.3s + 2s) = ~13%

→ Chỉ ~13% users đang có request đang xử lý tại một thời điểm
```

---

## Tính toán thực tế

### Từ stress test của bạn:

```
100 CCU:
├─ Response time: 200-500ms (avg 85.75ms)
├─ Wait time: 1-3s (avg 2s)
├─ Active ratio = 0.086s / (0.086s + 2s) = ~4%
└─ Concurrent requests = 100 CCU × 4% = ~4 requests

200 CCU:
├─ Response time: 200-500ms (avg 731.62ms)
├─ Wait time: 1-3s (avg 2s)
├─ Active ratio = 0.73s / (0.73s + 2s) = ~27%
└─ Concurrent requests = 200 CCU × 27% = ~54 requests
```

---

## Tại sao quan trọng với graceful shutdown?

### Khi shutdown:

```
Scenario với 200 CCU:
├─ 200 users đang chạy
├─ Active ratio = 27% (từ tính toán trên)
├─ Concurrent requests khi shutdown = 200 × 27% = ~54 requests
├─ Mỗi request mất 200-500ms
└─ Timeout cần = 54 requests × 500ms = 27 giây

Nhưng worst case:
├─ Tất cả 200 users vừa gửi request cùng lúc
├─ Concurrent requests = 200 requests
└─ Timeout cần = 200 requests × 500ms = 100 giây
```

---

## Kết luận

| Khái niệm | Ý nghĩa | Ví dụ |
|-----------|---------|-------|
| **CCU** | Số users đang chạy | 200 CCU = 200 users |
| **Concurrent Requests** | Số requests đang xử lý | 54 requests (từ 200 CCU) |
| **Mối quan hệ** | Concurrent requests = CCU × Active ratio | 200 × 27% = 54 |

### Quan trọng cho graceful shutdown:

- CCU cho biết tổng số users
- Concurrent requests cho biết số requests đang xử lý khi shutdown
- Tính timeout dựa trên concurrent requests, không phải CCU

### Khuyến nghị:

```
Với 200 CCU:
├─ Realistic: 54 concurrent requests × 500ms = 27s → Set 60s (buffer 2x)
└─ Worst case: 200 concurrent requests × 500ms = 100s → Set 90s (cover 90%)
```

Bạn hiểu sự khác biệt rồi chứ?