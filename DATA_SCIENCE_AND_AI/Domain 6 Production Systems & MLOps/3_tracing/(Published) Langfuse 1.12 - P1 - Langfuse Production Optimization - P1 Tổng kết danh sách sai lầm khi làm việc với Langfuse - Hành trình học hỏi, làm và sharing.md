> Bài viết chia sẻ kinh nghiệm sử dụng Langfuse cho việc Tracing services trên Production, nếu có sai sót ở đâu mong được mọi người góp ý để mình có thể điều chỉnh. Mình đã trải qua gần 5 tháng (từ tháng 10, năm 2025 đến hiện tại 14/02/2026 để vừa đủ kiến thức, trải nghiệm để viết bài viết này).


Đôi lời giới thiệu:

1. Langfuse hỗ trợ cơ chế tracing cực tốt giúp theo dõi luồng data lồng nhau trên 1 UI rất trực quan, cộng với đo đạc hiệu năng p95, P99 đầy đủ => Đáp ứng đủ tiêu chí về: Log, Đo Đạc
    
2. Trong quá trình sử dụng, mình từng mắc phải những sai lầm khiến overhead lên đến 100ms-500ms, mình xin phép chia sẻ trong phần 1 này: "**Các sai lầm mình từng mắc phải khi sử dụng Langfuse"**
    
3. Sau này, khi có kinh nghiệm hơn, mình đã triển khai 1 service với response time < 150ms cho 100 CCU và mình dùng Langfuse để tracing. Mình nhận thấy 99% overhead ~ 10ms. Tuy nhiên 1% có những cú vút lên overhead lên đến 1s (so với bình thường chỉ 30ms), điều đó buộc mình đào sâu hơn cơ chế vận hành của Langfuse => Mình xin chia sẻ nó trong phần 2: **Vấn đề tranh chấp khoá và tranh chấp tài nguyên GIL. (Do cơ chế Langfusk SDK v3 hoàn toàn tương tự cách OpenTelemetry hoạt động, đều đẩy xuống 1 thread riêng: BatchSpanProcessor => gây ra vấn đề đề tranh chấp khoá và tranh chấp tài nguyên GIL)**
# P1. Các sai lầm mình từng mắc phải khi làm việc với Langfuse

## 1.1 SAI LẦM 1: Khởi tạo mới Langfuse mỗi lần dùng => Gây overhead 0.1s  (Khởi tạo trước Langfuse 1 lần các lần sau chỉ việc dùng giúp giảm response time xuống 0.002s - 0.01s)

### 1.1.1 Kiểm chứng độc lập

1. test_trace_no_create_langfuse_client.py

```test_trace_no_create_langfuse_client.py
import os
import time
from pathlib import Path
from dotenv import load_dotenv

# Load .env file from project root before importing langfuse
# Find project root (go up from app/common/langfuse to root)
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent
env_path = project_root / ".env"
load_dotenv(env_path)

from langfuse import observe

# @observe(name="test_trace_with_observe_no_create_langfuse_client")
def test_trace_with_observe():
    time.sleep(1)

def test_trace():
    time.sleep(1)

def __main__():
    start_time = time.time()
    test_trace_with_observe()
    end_time = time.time()
    duration_with_observe = end_time - start_time
    print(f"Duration with observe: {duration_with_observe:.6f} seconds")
    print("--------------------------------")
    start_time = time.time()
    test_trace()
    end_time = time.time()
    duration_without_observe = end_time - start_time
    print(f"Duration without observe: {duration_without_observe:.6f} seconds")
    print("--------------------------------")
    print(f"Difference: {duration_with_observe - duration_without_observe:.6f} seconds")

if __name__ == "__main__":
    __main__()


```

→ Overhead do observe (và ngầm tạo client) ≈ 1.683371 - 1.002006 ≈ 0.681s.

2. test_trace_create_langfuse_client_first.py

```test_trace_create_langfuse_client_first.py
import os
import time
from pathlib import Path
from dotenv import load_dotenv

# Load .env file from project root before importing langfuse
# Find project root (go up from app/common/langfuse to root)
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent
env_path = project_root / ".env"
load_dotenv(env_path)

# Import langfuse_client from app.common.langfuse module
# This client is already initialized in __init__.py at module level
# which is more efficient than creating a new client each time
import sys
sys.path.insert(0, str(project_root))
from app.common.langfuse import langfuse_client

# Now import observe - it will use the client from __init__.py
from langfuse import observe

# @observe(name="test_trace_create_langfuse_client_first")
def test_trace_create_langfuse_client_first():
    time.sleep(1)

def test_trace():
    time.sleep(1)

def __main__():
    start_time = time.time()
    test_trace_create_langfuse_client_first()
    end_time = time.time()
    duration_with_observe = end_time - start_time
    print(f"Duration with observe: {duration_with_observe:.6f} seconds")
    start_time = time.time()
    test_trace()
    end_time = time.time()
    duration_without_observe = end_time - start_time
    print(f"Duration without observe: {duration_without_observe:.6f} seconds")
    print(f"Difference: {duration_with_observe - duration_without_observe:.6f} seconds")

if __name__ == "__main__":
    __main__()


```

→ Overhead chỉ ≈ 1.002282 - 1.002002 ≈ 0.00028s (≈ 0.3ms), tức là nhỏ hơn trước đó khoảng 2–3 bậc.


=> Nguyên nhân lớn khiến ban đầu chậm là do không khởi tạo langfuse_client trước, để decorator tự lo client mỗi lần.


### 1.1.2 Các bài test thực tế từ Langfuse
### 1.1.3 Kiểm chứng thực tế : Mình đã tự kiểm nghiệm thực tế

### 1.1.4 Kết luận: 

```
Việc khởi tạo langfuse mỗi lần gây ra overhead lên đến 100ms-300ms
```

---


## 1.2. SAI LẦM 2: TRACE QUÁ NHIỀU NẤC KHÔNG CẦN THIẾT (nấc lồng nhau)


Link chi tiết: D:\GIT\robot-lesson-workflow\utils\docs\Stage1_OverheadOfLangFuse\log_trace_image\log2_Conclusion_0.01s_0.02s_overhead.md  + D:\GIT\robot-lesson-workflow\utils\docs\Stage1_OverheadOfLangFuse\log_trace_image\log3_Deployv1_OverheadLangfuse_20112025.md

![](CKP/image/Pasted%20image%2020260206175911.png)

![](CKP/image/Pasted%20image%2020260206175920.png)

> Nhìn vào 2 ảnh, hãy so sánh step 1, step 3 khi bật @observe và khi không bật => Ta thấy ngay bị overhead từ 3-10ms-300ms
### KẾT LUẬN: Kiểm tra các nấc lồng nhau ta đưa ra kết luận: 

```
1. Là trace ở hàm con được trace mỗi hàm dôi lên 0.002s - 0.01s 
2. Là việc trace ở hàm cha sẽ bị dôi 0.02s so với tổng của việc cộng time của các thành phần con (kể cả con được trace hay không được trace) 
```


## 1.3 SAI LẦM 3: Để capture_input=True, capture_output=True với JSON quá dài. 

```
1. Là trace ở hàm con được trace mỗi hàm dôi lên 0.002s - 0.01s 
2. Là việc trace ở hàm cha sẽ bị dôi 0.02s so với tổng của việc cộng time của các thành phần con (kể cả con được trace hay không được trace) 
3. Chỉ load data cần thiết bằng việc set: capture_input hoặc capture_output = False 
   +, Sau đó chỉ load data cần thiết vào metadata thông qua 
   @observe + update_current_trace, update_current_span, update_current_generation
   +, Mẹo: có thể dùng thêm orjson cho việc tối ưu xử lý JSON (đặc biệt hiệu quả với json dài)
   
```

### 1.3.1 Ví dụ: Trong `robot_v2_services.webhook_service`
- Ở đầu hàm `webhook_service`, ta sẽ đặt capture_input=False, **ghi metadata theo conversation** => điều này giúp làm giảm kích thước của JSON load vào Queue và sau đó bắn lên Langfuse (nói cách khác: chỉ đính kèm những trường “nhẹ” nhưng đủ để debug (ID + message tóm tắt + thông tin audio), không nhét cả payload to vào trace để tránh overhead & rò rỉ dữ liệu)

```1242:1251:app/api/services/robot_v2_services.py
@observe(name="robot-v2.webhook-service", capture_input=False, capture_output=True)
async def webhook(self, payload: Dict[str, Any]) -> Dict[str, Any]:
...
if langfuse_client:
    metadata = {
        "conversation_id": conversation_id,
        "message": message[:200] if message else "",
        "has_audio_url": bool(audio_url),
        "audio_url": audio_url[:100] if audio_url else ""
    }
    langfuse_client.update_current_span(metadata=metadata)
```

### 1.3.2 Sự khác nhau giữa các methods: update_current_trace, update_current_span, update_current_generation

|                | `update_current_trace`                    | `update_current_span`                        | `update_current_generation`              |
| -------------- | ----------------------------------------- | -------------------------------------------- | ---------------------------------------- |
| **Cấp**        | Trace (root)                              | Span/Observation (bước con)                  | Generation (LLM call trong span)         |
| **Hiển thị**   | Metadata của trace trên UI                | Metadata của span/observation                | Model, usage, cost trên Generation       |
| **Filter**     | Filter theo trace (vd: `conversation_id`) | Xem chi tiết từng bước                       | Xem cost, token theo từng LLM call       |
| **Vị trí gọi** | Thường ở entry point (API routes)         | Bên trong các hàm `@observe`                 | Trong LLM call (vd: OpenRouter client)   |
| **Ví dụ**      | `conversation_id`, `bot_id`, `user_id`    | `message`, `next_action`, `messages_summary` | `model`, `usage_details`, `cost_details` |
| **Cấu trúc**   | Trace (container gốc)                     | Trace → Span                                 | Trace → Span → Generation                |

```
Trace
  └── Span (intent.llm)
        └── Generation (LLM call)  → update_current_generation(...)
```

#### 1. `update_current_trace` — cập nhật metadata ở mức **trace** (cả request)

Dùng ở **route** (trước khi gọi service), vì lúc đó đang ở trace gốc:

**File:** `app/api/routes/robot_v2_routes.py` (khoảng dòng 63–96)

```python
@router.post("/webhook")
# @observe(name="robot-v2.webhook-route", ...)
async def webhook(inputs: RobotV2Input, service: RobotV2Service = Depends(get_robot_v2_service)):
    conversation_id = inputs.conversation_id
    user_id = inputs.user_id

    if conversation_id or user_id:
        try:
            langfuse = get_client()  # từ langfuse import get_client
            if langfuse:
                metadata = {}
                if conversation_id:
                    metadata["conversation_id"] = conversation_id
                if user_id:
                    metadata["user_id"] = user_id
                # Update trace metadata - hiển thị trên UI Langfuse
                langfuse.update_current_trace(metadata=metadata)
        except Exception as e:
            pass

    return await service.webhook(inputs.dict())
```

Tương tự cho `init_conversation`: cũng dùng `get_client()` rồi `langfuse.update_current_trace(metadata=...)` (với `conversation_id`, `bot_id`).

---

#### 2. `update_current_span` — cập nhật metadata cho **span** hiện tại (hàm được @observe)

Dùng **bên trong** các hàm đã được `@observe`, để gắn thêm thông tin cho đúng span đó.

**Ví dụ 1 – trong webhook service (span `robot-v2.webhook-service`):**  
**File:** `app/api/services/robot_v2_services.py` (khoảng 1343–1354)

```python
@observe(name="robot-v2.webhook-service", capture_input=False, capture_output=True)
async def webhook(self, payload: Dict[str, Any]) -> Dict[str, Any]:
    # ...
    try:
        if langfuse_client:
            metadata = {
                "conversation_id": conversation_id,
                "message": message[:200] if message else "",
                "has_audio_url": bool(audio_url),
                "audio_url": audio_url[:100] if audio_url else ""
            }
            langfuse_client.update_current_span(metadata=metadata)
    except Exception:
        pass
```

**Ví dụ 2 – trong intent phoneme (span `robot-v2.intent.phoneme`):**  
**File:** `app/module/workflows/v2_robot_workflow/chatbot/policies/utils_conversation_workflow_orchestrator.py` (khoảng 333–341)

```python
@observe(name="robot-v2.intent.phoneme", capture_input=False, capture_output=True)
async def classify_by_phoneme(message: str, scenario: dict, next_action: int):
    try:
        if langfuse_client:
            langfuse_client.update_current_span(metadata={"message": message, "next_action": next_action})
    except Exception:
        pass
    return await RegexIntentClassifier().phoneme_classifier(...)
```

**Ví dụ 3 – trong intent LLM (span `robot-v2.intent.llm`):**  
Cùng file, khoảng 361–408:

```python
@observe(name="robot-v2.intent.llm", capture_input=False, capture_output=True)
async def classify_by_llm(messages: list, llm_base, params: dict, conversation_id: str, **kwargs) -> str:
    try:
        if langfuse_client:
            metadata = {
                "conversation_id": conversation_id,
                "messages_count": len(messages) if messages else 0,
                "model": params.get("model", "unknown") if params else "unknown",
                "provider_name": getattr(llm_base, 'provider_name', 'unknown'),
                # ... temperature, top_p, max_tokens, messages summary ...
            }
            langfuse_client.update_current_span(metadata=metadata)
    except Exception:
        pass
    return await llm_base.predict(messages=messages, params=params, ...)
```

---

#### 3. `update_current_generation` — cập nhật **generation** (LLM call: model, usage, cost)

Dùng khi bạn tự tạo/gọi LLM và muốn ghi lại model, token usage và cost lên generation hiện tại của Langfuse.

**File:** `app/common/openrouter_llm/client.py` (khoảng 269–277)

```python
# Update current generation; if none exists, this call is a no-op
self.langfuse_client.update_current_generation(
    model=plain_model,
    usage_details=usage_details,
    cost_details=cost_details,
)
```

Trước đó code đã build `usage_details` (input/output tokens) và `cost_details` (input/output/total cost). Đây là chỗ bổ sung metadata cho **generation** (một lần gọi LLM), không phải trace hay span chung.

---

## SAI LẦM 1.4: 12/02/2026 Sử dụng Langfuse() của version cũ mà không chịu update lên version mới sử dụng get_client()
> https://langfuse.com/changelog/2025-06-05-python-sdk-v3-generally-available

### So sánh: `get_client()` vs `Langfuse()`

Đây là hai phương thức khởi tạo client của Langfuse Python SDK, với sự khác biệt quan trọng giữa các phiên bản SDK:

#### **1. Phiên bản SDK**

**`Langfuse()`**
- Sử dụng trong cả **SDK v2** (legacy) và **SDK v3** (hiện tại)
- Trong v2: Là phương thức chính để khởi tạo client
- Trong v3: Vẫn có thể dùng để tạo client instance mới với cấu hình tùy chỉnh

**`get_client()`**
- **Chỉ có trong SDK v3** (OpenTelemetry-based)
- Ra mắt từ tháng 6/2025 khi v3 trở thành Generally Available

[Langfuse](https://langfuse.com/changelog/2025-06-05-python-sdk-v3-generally-available)

https://langfuse.com/docs/observability/sdk/overview

![](CKP/image/Pasted%20image%2020260212145214.png)

**Quản lý instance**

| Đặc điểm     | `Langfuse()`                                                                                                                                                                                      | `get_client()`                                                                                                                                |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| **Pattern**  | Tạo instance mới mỗi lần gọi                                                                                                                                                                      | Singleton - trả về cùng 1 instance                                                                                                            |
| **Khởi tạo** | Multiple instances có thể tồn tại                                                                                                                                                                 | Chỉ 1 instance duy nhất (khởi tạo lần đầu)                                                                                                    |
| **Cấu hình** | Qua constructor parameters                                                                                                                                                                        | Qua environment variables                                                                                                                     |
| **Use case** | Cần nhiều client với cấu hình khác nhau                                                                                                                                                           | Sử dụng chung 1 cấu hình trong toàn app                                                                                                       |
|              | "If you create multiple Langfuse instances with the same public_key, the singleton instance is REUSED and new arguments are IGNORED."<br><br>https://langfuse.com/docs/observability/sdk/overview | - Tích hợp tốt với OpenTelemetry context propagation<br>- Tránh duplicate configuration<br>- Quản lý resource tốt hơn (singleton pattern)<br> |
###### OpenTelemetry Foundation[](https://langfuse.com/changelog/2025-06-05-python-sdk-v3-generally-available#opentelemetry-foundation)

The foundation of the v3 SDK is OpenTelemetry, which brings several practical advantages:

- **Standardized Context Propagation**: OTEL automatically handles the propagation of trace and span context. This means when you create a new span or generation, it correctly nests under the currently active operation.
- **Third-Party Library Compatibility**: Libraries already instrumented with OpenTelemetry will integrate with the Langfuse SDK, with their spans being captured and correctly nested within your Langfuse traces.

---

#### **2. Code 

##### 2.1 Cách khởi tạo và cấu hình**

	**`Langfuse()`** - Khởi tạo instance mới

```python
from langfuse import Langfuse

# Cấu hình qua constructor
langfuse = Langfuse(
    public_key="your-public-key",
    secret_key="your-secret-key",
    base_url="https://cloud.langfuse.com",
    debug=True
)
```

**`get_client()`** - Truy cập singleton instance

```python
from langfuse import get_client

# Tự động sử dụng environment variables
langfuse = get_client()

# Environment variables cần thiết:
# LANGFUSE_PUBLIC_KEY="your-public-key"
# LANGFUSE_SECRET_KEY="your-secret-key"
# LANGFUSE_BASE_URL="https://cloud.langfuse.com"
```


```python
# Load environment variables và sử dụng

from dotenv import load_dotenv
from langfuse import get_client

# Load .env file vào environment variables
load_dotenv()

# get_client() sẽ tự động đọc từ env vars đã được load
langfuse = get_client()

# Sử dụng bình thường
@observe()
def my_function():
    # ...
    pass
```

Thông thường: Các dự án sẽ có 1 file config môi trường riêng - chuẩn best practices folder structure 

```
my_project/
├── .env                    # Environment variables
├── .env.example            # Template
├── config/
│   ├── __init__.py
│   └── settings.py         # ⭐ Config centralized
├── app/
│   ├── main.py
│   ├── api/
│   └── services/
└── requirements.txt
```

[Langfuse](https://langfuse.com/docs/observability/sdk/overview)

##### **2.2. Ví dụ sử dụng trong SDK v3**

**Sử dụng `get_client()` - Recommended approach**

```python
from langfuse import get_client, observe

# Module A
@observe()
def process_data():
    langfuse = get_client()  # Lấy global client
    langfuse.update_current_trace(tags=["processing"])
    # ... xử lý

# Module B
@observe()
def analyze_data():
    langfuse = get_client()  # Cùng 1 client instance
    langfuse.update_current_trace(user_id="user-123")
    # ... phân tích
```

**Sử dụng `Langfuse()` - Multiple clients**

```python
from langfuse import Langfuse

# Production client - sample 5% traces
langfuse_prod = Langfuse(
    sample_rate=0.05,
    public_key="prod-key"
)

# Beta client - sample 100% traces
langfuse_beta = Langfuse(
    sample_rate=1.0,
    public_key="beta-key"
)
```

[Langfuse](https://langfuse.com/changelog/2025-06-05-python-sdk-v3-generally-available)


---

 #### 3. Recommend nên dùng: `get_client()` - BEST PRACTICE dù cả 2 đều singleton?

```
Lý do 1: INTENT RÕ RÀNG
─────────────────────────
  langfuse = get_client()       → "Tôi muốn LẤY client hiện có"  ✅ Rõ
  langfuse_client = Langfuse()  → "Tôi muốn TẠO client mới"      ❌ Misleading
                                   (thực tế không tạo mới)

Lý do 2: OVERHEAD NHỎ NHƯNG THẬT
──────────────────────────────────
  get_client()    → return _singleton          → ~0.001ms
  Langfuse()      → check key → lookup → return → ~0.01ms
                     ↑
                     10x chậm hơn (dù vẫn rất nhỏ)
                     × 1000 requests/s = 10ms/s unnecessary

Lý do 3: TRÁNH BẪY "NEW ARGUMENTS ARE IGNORED"
────────────────────────────────────────────────
  # File A (load trước)
  client = Langfuse(host="http://server-1:3009")     # Instance tạo

  # File B (load sau)  
  client = Langfuse(host="http://server-2:3009")     # ⚠️ BỊ IGNORED!
                                                       # Vẫn dùng server-1
  
  # Với get_client() → không ai bị lừa vì không pass args
```


---
## SAI LẦM 1.6: VẤN ĐỀ GIL CONTENTION - 

- Tình trạng: ĐANG P95, P99 30ms, tự nhiên có những khoảnh khắc bị vụt lên 1.5 s ???  
- NGUYÊN NHÂN GỐC ĐƯỢC TÌM THẤY: Do cơ chế auto-flush của Langfuse Python SDK v3 (OpenTelemetry BatchSpanProcessor) chạy trong cùng process với FastAPI/vLLM, gây GIL contention giữa background flush thread và asyncio event loop. Dùng `@observe` hay context manager đều đi qua cùng đường SDK này, nên bản chất overhead GIL **vẫn tồn tại**, chỉ khác là mỗi pattern làm tần suất flush và số spans khác nhau.
- Mình sẽ có 1 bài viết riêng cho phần này : P2 - Langfuse

---



