# üéØ Best Practices Tri·ªÉn Khai Langfuse Python SDK

## 1. Client Initialization Pattern

### ‚úÖ **Recommended: `get_client()` Pattern**

```python
# app/core/langfuse_client.py
from langfuse import get_client

# Singleton instance - initialized once, reused everywhere
langfuse_client = get_client()

# Export for use across the application
__all__ = ["langfuse_client"]
```

**T·∫°i sao?**

- Clear intent: ‚ÄúGet existing client‚Äù thay v√¨ ‚ÄúCreate new instance‚Äù
- Tr√°nh confusion v·ªõi ‚Äúnew arguments are ignored‚Äù behavior
- Thread-safe v√† context-aware
- [Source](https://langfuse.com/docs/observability/sdk/overview)

---

## 2. Project Structure Best Practice

```
your_project/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ langfuse_client.py      # Single initialization point
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm_service.py          # Import from core
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py               # Import from core
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ tracing_utils.py        # Helper functions
‚îú‚îÄ‚îÄ main.py                          # Application entry
‚îî‚îÄ‚îÄ .env                            # Environment variables
```

### ‚úÖ **Single Import Pattern**

```python
# app/core/langfuse_client.py
from langfuse import get_client, observe
from langfuse._client import Langfuse

# Initialize once at startup
langfuse_client: Langfuse = get_client()

def get_langfuse():
    """Get the singleton Langfuse client."""
    return langfuse_client

__all__ = ["langfuse_client", "get_langfuse", "observe"]
```

---

## 3. Environment Configuration

### ‚úÖ **Environment Variables (Best Practice)**

```bash
# .env
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_BASE_URL=https://cloud.langfuse.com  # or your self-hosted URL
LANGFUSE_TRACING_ENVIRONMENT=production
LANGFUSE_SAMPLE_RATE=1.0  # 1.0 = 100%, 0.1 = 10%
LANGFUSE_TRACING_ENABLED=true
LANGFUSE_FLUSH_INTERVAL=10.0  # seconds
```

**∆Øu ƒëi·ªÉm:**

- Kh√¥ng c·∫ßn hardcode credentials
- D·ªÖ d√†ng chuy·ªÉn ƒë·ªïi gi·ªØa environments
- Consistent v·ªõi [Langfuse docs](https://langfuse.com/docs/observability/features/environments)

---

## 4. Instrumentation Patterns

### ‚úÖ **Pattern 1: `@observe` Decorator (Recommended)**

```python
from app.core.langfuse_client import observe

@observe(as_type="generation")
async def call_llm(prompt: str, model: str = "gpt-4"):
    """Automatically traced LLM call."""
    response = await openai_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# Update metadata if needed
@observe(as_type="generation")
async def call_llm_with_metadata(prompt: str):
    response = await call_llm(prompt)
    
    # Get client to update current span
    from app.core.langfuse_client import get_langfuse
    langfuse = get_langfuse()
    
    langfuse.update_current_generation(
        metadata={"custom_key": "value"},
        usage={
            "input": 100,
            "output": 50,
            "total": 150,
            "unit": "TOKENS",
            "input_cost": 0.001,
            "output_cost": 0.002
        }
    )
    return response
```

### ‚úÖ **Pattern 2: Context Manager**

```python
from app.core.langfuse_client import get_langfuse

async def process_workflow(user_input: str):
    langfuse = get_langfuse()
    
    # Create trace context
    with langfuse.start_as_current_observation(
        as_type="span",
        name="workflow_processing"
    ) as span:
        # All nested calls automatically become children
        result = await step1(user_input)
        result = await step2(result)
        
        # Update span before exiting
        span.update(
            metadata={"workflow_version": "1.0.0"},
            output=result
        )
        return result
```

### ‚úÖ **Pattern 3: Manual Observations (Advanced)**

```python
from app.core.langfuse_client import get_langfuse

async def background_task(data: dict):
    """For non-blocking, parallel operations."""
    langfuse = get_langfuse()
    
    # Create manual observation (no context shift)
    observation = langfuse.start_observation(
        as_type="span",
        name="background_processing"
    )
    
    try:
        result = await process_data(data)
        observation.end(
            output=result,
            status="success"
        )
    except Exception as e:
        observation.end(
            error=str(e),
            status="error"
        )
        raise
```

---

## 5. Async/Await Best Practices

### ‚úÖ **Trong async code, rely on Langfuse helpers**

```python
# ‚úÖ Good: Use decorator
@observe(as_type="generation")
async def async_llm_call():
    return await openai_client.chat.completions.create(...)

# ‚úÖ Good: Use context manager with async
async def async_workflow():
    langfuse = get_langfuse()
    
    with langfuse.start_as_current_observation(as_type="span") as span:
        # await boundaries are handled correctly
        result1 = await step1()
        result2 = await step2(result1)
        return result2

# ‚ùå Avoid: Manual context management across await boundaries
async def bad_pattern():
    trace = langfuse.start_trace(name="bad")
    await step1()  # Context might be lost here!
    trace.end()  # Might not work correctly
```

[Source](https://langfuse.com/docs/observability/sdk/troubleshooting-and-faq)

---

## 6. Multi-Project Setup (Advanced)

### ‚úÖ **Contextvars for Multi-Project**

```python
from langfuse._client.get_client import _set_current_public_key
from app.core.langfuse_client import get_langfuse

async def handle_multi_project_request(project_id: str):
    # Set context for this request
    with _set_current_public_key(project_id):
        langfuse = get_langfuse()
        # All traces go to correct project
        await process_request()
```

**L∆∞u √Ω:** Multi-project support l√† experimental. [Source](https://langfuse.com/docs/observability/sdk/advanced-features)

---

## 7. Error Handling & Resilience

### ‚úÖ **Graceful Degradation**

```python
from app.core.langfuse_client import get_langfuse

async def safe_llm_call(prompt: str):
    try:
        langfuse = get_langfuse()
        
        with langfuse.start_as_current_observation(
            as_type="generation",
            name="llm_call"
        ) as span:
            response = await openai_client.chat.completions.create(...)
            span.update(output=response.choices[0].message.content)
            return response
            
    except Exception as e:
        # Langfuse won't break your app
        # Log error but don't crash
        logger.error(f"LLM call failed: {e}")
        # Return fallback or re-raise
        raise
```

---

## 8. Short-Lived Processes (Scripts/Serverless)

### ‚úÖ **Always Flush/Shutdown**

```python
# scripts/batch_processing.py
from app.core.langfuse_client import get_langfuse

async def main():
    langfuse = get_langfuse()
    
    try:
        # Process data
        for item in dataset:
            await process_item(item)
    finally:
        # Critical: Flush before exit
        langfuse.flush()  # or langfuse.shutdown()
        # In async: await langfuse.async_shutdown()

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

**T·∫°i sao?** SDK l√† asynchronous v√† buffers spans trong background. [Source](https://langfuse.com/docs/sdk/python/decorators)

---

## 9. Sampling Strategy

### ‚úÖ **Production Setup**

```python
# .env.production
LANGFUSE_SAMPLE_RATE=0.1  # 10% sampling in production

# .env.development
LANGFUSE_SAMPLE_RATE=1.0   # 100% in development
```

```python
# For critical paths, force sampling
@observe(as_type="generation")
async def critical_path():
    # This will respect global sample_rate
    pass

# Or use manual sampling override (if supported)
```

**L∆∞u √Ω:** V·ªõi SDK v3, sampling ƒë∆∞·ª£c qu·∫£n l√Ω ·ªü global OTEL TracerProvider level, kh√¥ng th·ªÉ c√≥ different sampling rates cho c√πng public_key. [Source](https://github.com/orgs/langfuse/discussions/7571)

---

## 10. Testing & Development

### ‚úÖ **Mock Langfuse for Testing**

```python
# tests/conftest.py
import pytest
from unittest.mock import MagicMock

@pytest.fixture
def mock_langfuse():
    """Mock Langfuse client for unit tests."""
    mock = MagicMock()
    mock.start_as_current_observation.return_value.__enter__ = MagicMock(
        return_value=MagicMock()
    )
    mock.start_as_current_observation.return_value.__exit__ = MagicMock(
        return_value=False
    )
    return mock

# tests/test_service.py
async def test_llm_service(mock_langfuse, monkeypatch):
    monkeypatch.setattr(
        "app.services.llm_service.get_langfuse",
        lambda: mock_langfuse
    )
    
    result = await llm_service.call_llm("test prompt")
    assert result is not None
```

---

## 11. Monitoring & Debugging

### ‚úÖ **Enable Debug Logging (Development Only)**

```python
# .env.development
LANGFUSE_DEBUG=True
```

```python
# Programmatic check
from app.core.langfuse_client import get_langfuse

langfuse = get_langfuse()

# Verify connectivity (don't use in production)
try:
    langfuse.auth_check()
    print("‚úÖ Langfuse connected")
except Exception as e:
    print(f"‚ùå Langfuse connection failed: {e}")
```

---

## 12. Summary Checklist

|Aspect|Best Practice|Priority|
|---|---|---|
|**Client Init**|`get_client()` in single module|üî¥ High|
|**Import Pattern**|Import from centralized module|üî¥ High|
|**Config**|Environment variables|üî¥ High|
|**Tracing**|`@observe` decorator|üü° Medium|
|**Async**|Use decorators/context managers|üü° Medium|
|**Shutdown**|`flush()`/`shutdown()` in scripts|üî¥ High|
|**Sampling**|Environment-based rates|üü¢ Low|
|**Multi-project**|Contextvars with caution|üü¢ Low|
|**Testing**|Mock client|üü° Medium|

---

## 13. Common Anti-Patterns to Avoid

### ‚ùå **Don‚Äôt: Create multiple Langfuse() instances expecting different configs**

```python
# ‚ùå Bad: This won't work as expected!
client1 = Langfuse(sample_rate=0.1)   # First call "wins"
client2 = Langfuse(sample_rate=1.0)   # Ignored! Same public_key
```

### ‚ùå **Don‚Äôt: Mix `get_client()` and `Langfuse()` without understanding**

```python
# ‚ùå Confusing and potentially problematic
from langfuse import Langfuse, get_client

# In one file
client_a = Langfuse()

# In another file
client_b = get_client()  # Might return different instance!
```

### ‚ùå **Don‚Äôt: Forget to flush in short-lived processes**

```python
# ‚ùå Data might be lost!
def lambda_handler(event, context):
    langfuse = get_langfuse()
    process(event)
    # Missing flush() or shutdown()!
```

---

## 14. Recommended Project Template

```python
# app/core/langfuse_client.py
"""
Centralized Langfuse client configuration.
All Langfuse interactions should go through this module.
"""
from langfuse import get_client, observe
from langfuse._client import Langfuse

# Initialize singleton client
_langfuse_client: Langfuse = get_client()

def get_langfuse() -> Langfuse:
    """Get the singleton Langfuse client."""
    return _langfuse_client

__all__ = ["get_langfuse", "observe"]
```

```python
# app/services/llm_service.py
from app.core.langfuse_client import get_langfuse, observe

@observe(as_type="generation")
async def call_llm(prompt: str, model: str = "gpt-4"):
    """LLM call with automatic tracing."""
    langfuse = get_langfuse()
    
    # Update metadata if needed
    langfuse.update_current_generation(
        metadata={"model": model, "prompt_version": "1.0"}
    )
    
    # Your LLM call here
    response = await openai_client.chat.completions.create(...)
    return response.choices[0].message.content
```

```python
# main.py
from app.core.langfuse_client import get_langfuse

async def main():
    langfuse = get_langfuse()
    
    try:
        # Your application logic
        pass
    finally:
        # Critical for clean shutdown
        await langfuse.async_shutdown()
```

---

B·∫°n mu·ªën t√¥i ƒëi s√¢u v√†o ph·∫ßn n√†o c·ª• th·ªÉ h∆°n kh√¥ng? V√≠ d·ª•:

- Multi-project setup chi ti·∫øt
- Self-hosted Langfuse configuration
- Integration v·ªõi FastAPI/FastAPI
- Prompt management best practices