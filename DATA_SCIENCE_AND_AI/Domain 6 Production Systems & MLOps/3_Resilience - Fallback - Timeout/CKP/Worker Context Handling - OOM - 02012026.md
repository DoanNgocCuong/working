## 1. V·∫§N ƒê·ªÄ, HI·ªÜN TR·∫†NG

### 1.1. S·ª± ki·ªán OOM Kill

**Th·ªùi gian**: 11:42:04 AM ng√†y 1/1/2026
**Exit code**: 137 (OOMKilled)
**Memory limit**: 3000 MiB (3 GB)
**S·ªë l·∫ßn restart**: 2 l·∫ßn

### 1.2. Memory Usage Pattern

T·ª´ Datadog Metrics v√† Rancher:

```
08:00-11:00 AM: Memory ·ªïn ƒë·ªãnh ~250MB ‚úÖ
11:42-11:46 AM: Memory tƒÉng ƒë·ªôt bi·∫øn ‚Üí 3GB (spike!) ‚ö†Ô∏è
11:42:04 AM: OOM Kill (exit code 137) üí•
12:00+: Memory reset v·ªÅ ~192MB (sau restart)
```

### 1.3. APM Traces Analysis

T·ª´ Datadog APM:

- Endpoint `/extract_facts` g√¢y 99.5% errors
- Service: `pika-mem0:6699` (Memory API)
- Latency Distribution:
  - p50: 19.1s
  - p75: 26.2s
  - p90: 60.8s (TIMEOUT)
  - p95: 60.8s (TIMEOUT)
  - Max: 60.9s

**Pattern errors**:

- 11:27:08 ‚Üí 60s timeout ‚Üí HTTP 500 ‚ùå
- 11:43:16 ‚Üí 60s timeout ‚Üí HTTP 500 ‚ùå
- Nhi·ªÅu requests timeout sau 60 gi√¢y

### 1.4. Architecture hi·ªán t·∫°i

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FastAPI API Server (Uvicorn)      ‚îÇ
‚îÇ  ‚îî‚îÄ Event Loop (1 thread)          ‚îÇ
‚îÇ      ‚îî‚îÄ X·ª≠ l√Ω HTTP requests        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì Publish message
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RabbitMQ Queue                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì Consume message
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RabbitMQ Worker (Separate Process) ‚îÇ
‚îÇ  ‚îî‚îÄ ThreadPoolExecutor (10 threads)‚îÇ ‚Üê ƒê√¢y m·ªõi c√≥ threads!
‚îÇ      ‚îú‚îÄ Thread 1: Process message  ‚îÇ
‚îÇ      ‚îú‚îÄ Thread 2: Process message  ‚îÇ
‚îÇ      ‚îî‚îÄ Thread 3: Process message  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

```

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1 WORKER PROCESS (python src/worker.py)                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ RabbitMQ Connection                                 ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚îú‚îÄ Host: RabbitMQ server                          ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚îú‚îÄ Queue: conversation_events_processing         ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚îî‚îÄ Prefetch: 10 messages                          ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ ThreadPoolExecutor                                  ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚îú‚îÄ max_workers: 10                                ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚îú‚îÄ Queue: UNBOUNDED (kh√¥ng gi·ªõi h·∫°n) ‚ö†Ô∏è          ‚îÇ ‚îÇ
‚îÇ ‚îÇ   ‚îî‚îÄ 10 threads x·ª≠ l√Ω messages ƒë·ªìng th·ªùi           ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ Processing Flow                                     ‚îÇ ‚îÇ
‚îÇ ‚îÇ   1. Parse message (conversation_log ~3MB)         ‚îÇ ‚îÇ
‚îÇ ‚îÇ   2. LLM Analysis (n·∫øu enabled)                    ‚îÇ ‚îÇ
‚îÇ ‚îÇ   3. Memory API call (pika-mem0:6699)              ‚îÇ ‚îÇ
‚îÇ ‚îÇ      ‚îî‚îÄ Timeout: 240s ‚ö†Ô∏è QU√Å CAO                   ‚îÇ ‚îÇ
‚îÇ ‚îÇ   4. Calculate friendship score                    ‚îÇ ‚îÇ
‚îÇ ‚îÇ   5. Update DB                                      ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.5. V·∫•n ƒë·ªÅ c·ª• th·ªÉ

1. Memory API timeout = 240s (qu√° cao)

   - M·ªói thread block t·ªëi ƒëa 240s
   - Throughput: 10 messages / 240s = 0.04 msg/s
2. ThreadPoolExecutor queue kh√¥ng gi·ªõi h·∫°n

   - Messages t√≠ch l≈©y v√¥ h·∫°n trong queue
   - M·ªói message gi·ªØ ~3MB (body bytes)
3. Retry v√¥ h·∫°n khi timeout

   - NACK v·ªõi `requeue=True` ‚Üí RE-DELIVER
   - Messages retry li√™n t·ª•c ‚Üí Throughput = 0
4. Memory t√≠ch l≈©y khi timeout

   - Exception stack traces gi·ªØ references
   - Python GC delay ‚Üí Memory kh√¥ng ƒë∆∞·ª£c gi·∫£i ph√≥ng ngay
   - 50-100 messages timeout ‚Üí 550-1100MB exception objects

---

## 2. NGUY√äN NH√ÇN CH√çNH

### 2.1. Primary Root Cause: Memory API Timeout Qu√° Cao

**D·∫´n ch·ª©ng code**:

```143:143:src/app/core/config_settings.py
MEMORY_API_TIMEOUT_SECONDS: int = 240  # 4 ph√∫t!
```

**V·∫•n ƒë·ªÅ**:

- Timeout 240s qu√° cao so v·ªõi th·ª±c t·∫ø (pika-mem0 timeout sau 60s)
- M·ªói thread block 240s ‚Üí kh√¥ng th·ªÉ x·ª≠ l√Ω messages kh√°c
- 10 threads √ó 240s = Memory gi·ªØ l√¢u

### 2.2. Secondary Root Cause: pika-mem0 Service Kh√¥ng Response

**B·∫±ng ch·ª©ng t·ª´ APM traces**:

- Requests timeout sau 60 gi√¢y
- HTTP 500 errors v·ªõi message "Missing error message and stack trace"
- 99.5% errors ƒë·∫øn t·ª´ endpoint `/extract_facts`

**C∆° ch·∫ø**:

```
pika-mem0 kh√¥ng response (timeout 60s)
    ‚Üì
Worker threads block 240s (ch·ªù timeout)
    ‚Üì
Memory t√≠ch l≈©y (conversation_log + formatted_conversation + payload)
    ‚Üì
Exception stack traces gi·ªØ references
    ‚Üì
Python GC delay ‚Üí Memory kh√¥ng ƒë∆∞·ª£c gi·∫£i ph√≥ng ngay
    ‚Üì
Memory spike ƒë·ªôt ng·ªôt ‚Üí OOM! üí•
```

### 2.3. Architecture Flaws

#### 2.3.1. ThreadPoolExecutor Queue Kh√¥ng Gi·ªõi H·∫°n

**D·∫´n ch·ª©ng code**:

```113:113:src/app/background/rabbitmq_consumer.py
self.executor = ThreadPoolExecutor(max_workers=max_workers)
# ‚Üí Queue m·∫∑c ƒë·ªãnh l√† unbounded Queue()
```

**V·∫•n ƒë·ªÅ**:

- Queue kh√¥ng gi·ªõi h·∫°n ‚Üí Messages t√≠ch l≈©y v√¥ h·∫°n
- M·ªói message gi·ªØ ~3MB (body bytes)
- 500-1000 messages = 1500-3000MB

#### 2.3.2. NACK v·ªõi requeue=True ‚Üí Retry V√¥ H·∫°n

**D·∫´n ch·ª©ng code**:

```576:576:src/app/background/rabbitmq_consumer.py
self.channel.basic_nack(delivery_tag=delivery_tag, requeue=True)
```

**V·∫•n ƒë·ªÅ**:

- Timeout ‚Üí NACK ‚Üí RE-DELIVER ‚Üí Timeout l·∫°i ‚Üí Cycle l·∫∑p l·∫°i
- Messages kh√¥ng ƒë∆∞·ª£c x·ª≠ l√Ω (retry v√¥ h·∫°n)
- Throughput = 0

#### 2.3.3. Exception Stack Traces Gi·ªØ References

**D·∫´n ch·ª©ng code**:

```934:961:src/app/services/utils/llm_analysis_utils.py
except httpx.TimeoutException as e:
    # ...
    raise  # ‚ö†Ô∏è RAISE EXCEPTION - Memory v·∫´n gi·ªØ trong stack!
```

**V·∫•n ƒë·ªÅ**:

- Exception object gi·ªØ references ƒë·∫øn:
  - `conversation_log` (~3MB)
  - `formatted_conversation` (~3MB)
  - `payload` (~3MB)
  - `client` buffers (~1MB)
- Stack trace gi·ªØ references cho ƒë·∫øn khi exception ƒë∆∞·ª£c handle
- 50-100 messages timeout ‚Üí 550-1100MB exception objects

#### 2.3.4 Python GC delay ‚Üí Memory kh√¥ng ƒë∆∞·ª£c gi·∫£i ph√≥ng ngay - T√≥m l·∫°i: Python GC delay l√† th·ªùi gian ch·ªù GC ch·∫°y ƒë·ªÉ gi·∫£i ph√≥ng memory. Trong tr∆∞·ªùng h·ª£p OOM, nhi·ªÅu exception objects t√≠ch l≈©y tr∆∞·ªõc khi GC ch·∫°y, g√¢y memory spike ƒë·ªôt ng·ªôt.

###### Python GC Delay l√† g√¨?

- GC kh√¥ng ch·∫°y li√™n t·ª•c, m√† ch·∫°y khi ƒë·∫°t threshold (700 objects cho gen0)
- C√≥ delay gi·ªØa l√∫c object kh√¥ng c√≤n reference v√† l√∫c GC gi·∫£i ph√≥ng
- Exception stack traces gi·ªØ references ƒë·∫øn local variables ‚Üí memory kh√¥ng ƒë∆∞·ª£c gi·∫£i ph√≥ng ngay

###### T·∫°i sao g√¢y v·∫•n ƒë·ªÅ trong OOM?

- Nhi·ªÅu messages timeout c√πng l√∫c ‚Üí nhi·ªÅu exception objects t√≠ch l≈©y
- GC ch∆∞a ch·∫°y ‚Üí memory kh√¥ng ƒë∆∞·ª£c gi·∫£i ph√≥ng
- Memory t√≠ch l≈©y nhanh ‚Üí v∆∞·ª£t 3GB limit ‚Üí OOM

##### 1. PYTHON GARBAGE COLLECTION L√Ä G√å?

###### 1.1. C∆° ch·∫ø c∆° b·∫£n

Python d√πng Garbage Collector (GC) ƒë·ªÉ t·ª± ƒë·ªông gi·∫£i ph√≥ng memory khi objects kh√¥ng c√≤n ƒë∆∞·ª£c s·ª≠ d·ª•ng.

```python
# V√≠ d·ª•:
def process_message():
    conversation_log = [{"message": "..."} for _ in range(1000)]  # ~3MB
    # ... x·ª≠ l√Ω ...
    return result

# Khi function k·∫øt th√∫c:
# - conversation_log kh√¥ng c√≤n ƒë∆∞·ª£c reference
# - Python GC s·∫Ω gi·∫£i ph√≥ng memory
# - NH∆ØNG: Kh√¥ng ph·∫£i ngay l·∫≠p t·ª©c!
```

###### 1.2. Reference Counting vs Generational GC

Python d√πng 2 c∆° ch·∫ø:

1. Reference Counting (t·ª©c th√¨)

   - ƒê·∫øm s·ªë references ƒë·∫øn object
   - Khi count = 0 ‚Üí gi·∫£i ph√≥ng ngay
   - Nh∆∞ng kh√¥ng x·ª≠ l√Ω circular references
2. Generational GC (c√≥ delay)

   - X·ª≠ l√Ω circular references
   - Ch·∫°y theo chu k·ª≥ (kh√¥ng li√™n t·ª•c)
   - C√≥ delay tr∆∞·ªõc khi ch·∫°y

---

##### 2. T·∫†I SAO C√ì DELAY?

###### 2.1. GC kh√¥ng ch·∫°y li√™n t·ª•c

```python
# Python GC ch·∫°y khi:
# - gen0 ƒë·∫°t 700 objects (generation 0)
# - gen1 ƒë·∫°t 10 objects (generation 1)  
# - gen2 ƒë·∫°t 10 objects (generation 2)

# KH√îNG ch·∫°y ngay khi object kh√¥ng c√≤n reference!
```

L√Ω do:

- GC t·ªën CPU
- Ch·∫°y li√™n t·ª•c s·∫Ω l√†m ch·∫≠m ·ª©ng d·ª•ng
- Python ch·∫°y GC khi c·∫ßn (threshold-based)

###### 2.2. Generational GC Thresholds

```python
import gc

# M·∫∑c ƒë·ªãnh thresholds:
gc.get_threshold()
# Output: (700, 10, 10)
# - gen0: 700 objects
# - gen1: 10 collections
# - gen2: 10 collections
```

C∆° ch·∫ø:

- M·ªói l·∫ßn t·∫°o object ‚Üí gen0 count++
- Khi gen0 = 700 ‚Üí ch·∫°y GC gen0
- N·∫øu object s·ªëng s√≥t ‚Üí chuy·ªÉn sang gen1
- Sau 10 l·∫ßn GC gen0 ‚Üí ch·∫°y GC gen1
- Sau 10 l·∫ßn GC gen1 ‚Üí ch·∫°y GC gen2

###### 3.1. Scenario: 50 messages timeout trong 10 gi√¢y

```python
# Timeline:

T=0s:   Message 1 timeout
        ‚îú‚îÄ Exception ƒë∆∞·ª£c raise
        ‚îú‚îÄ Exception object gi·ªØ references:
        ‚îÇ  ‚îú‚îÄ conversation_log: ~3MB
        ‚îÇ  ‚îú‚îÄ formatted_conversation: ~3MB
        ‚îÇ  ‚îî‚îÄ payload: ~3MB
        ‚îú‚îÄ Total: ~9MB per exception
        ‚îî‚îÄ GC count: 0 (ch∆∞a ƒë·∫°t threshold 700)

T=1s:   Message 2 timeout
        ‚îú‚îÄ Exception object: +9MB
        ‚îú‚îÄ Total: 18MB
        ‚îî‚îÄ GC count: 0 (ch∆∞a ƒë·∫°t threshold)

T=2s:   Message 3 timeout
        ‚îú‚îÄ Exception object: +9MB
        ‚îú‚îÄ Total: 27MB
        ‚îî‚îÄ GC count: 0 (ch∆∞a ƒë·∫°t threshold)

... (ti·∫øp t·ª•c) ...

T=10s:  Message 50 timeout
        ‚îú‚îÄ Exception objects: 50 √ó 9MB = 450MB
        ‚îú‚îÄ GC count: 50 (v·∫´n ch∆∞a ƒë·∫°t threshold 700!)
        ‚îî‚îÄ Memory: 450MB (CH∆ØA ƒê∆Ø·ª¢C GI·∫¢I PH√ìNG!)

T=11s:  GC ch·∫°y (threshold ƒë·∫°t ho·∫∑c manual trigger)
        ‚îú‚îÄ Gi·∫£i ph√≥ng exception objects
        ‚îú‚îÄ Memory: 450MB ‚Üí ~50MB (sau GC)
        ‚îî‚îÄ Delay: 1 gi√¢y (ho·∫∑c l√¢u h∆°n!)
```

---

### 2.4. C∆° Ch·∫ø G√¢y Memory Spike ƒê·ªôt Ng·ªôt

**Timeline th·ª±c t·∫ø (11:42 AM)**:

```
11:40:00 AM: Memory: ~400MB
‚îú‚îÄ 10 threads ƒëang block (timeout 60s)
‚îú‚îÄ Queue: 20 messages
‚îî‚îÄ Exception objects: 10 √ó 11MB = 110MB

11:41:00 AM: Memory: ~600MB
‚îú‚îÄ 10 threads v·∫´n block
‚îú‚îÄ Queue: 40 messages
‚îî‚îÄ Exception objects: 20 √ó 11MB = 220MB

11:42:00 AM: ‚ö†Ô∏è SPIKE B·∫ÆT ƒê·∫¶U!
‚îú‚îÄ 10 messages timeout c√πng l√∫c
‚îú‚îÄ Exception objects: +110MB
‚îú‚îÄ Queue: +10 messages (retry)
‚îî‚îÄ Memory: 600MB + 110MB + 30MB = 740MB

11:42:01 AM: ‚ö†Ô∏è SPIKE TI·∫æP T·ª§C!
‚îú‚îÄ 10 messages timeout
‚îú‚îÄ Exception objects: +110MB
‚îú‚îÄ Queue: +10 messages
‚îî‚îÄ Memory: 740MB + 110MB + 30MB = 880MB

... (ti·∫øp t·ª•c) ...

11:42:04 AM: üí• OOM KILL!
‚îú‚îÄ Memory: 1160MB + overhead + GC delay = 2.5-3GB
‚îú‚îÄ V∆∞·ª£t qu√° 3GB limit
‚îî‚îÄ Kubernetes OOMKill (exit code 137)
```

**Compound Effect**:

- Nhi·ªÅu messages timeout c√πng l√∫c (50-100 messages trong 10 gi√¢y)
- Exception stack traces t√≠ch l≈©y: 550-1100MB
- ThreadPoolExecutor queue t√≠ch l≈©y: 300-600MB
- Python GC delay: 200-400MB (Python GC delay l√† th·ªùi gian ch·ªù GC ch·∫°y ƒë·ªÉ gi·∫£i ph√≥ng memory. Trong tr∆∞·ªùng h·ª£p OOM, nhi·ªÅu exception objects t√≠ch l≈©y tr∆∞·ªõc khi GC ch·∫°y, g√¢y memory spike ƒë·ªôt ng·ªôt.)
- Total: 1050-2100MB ‚Üí V∆∞·ª£t 3GB limit! üí•

---

## 3. GI·∫¢I PH√ÅP

### 3.1. T·ªïng Quan Gi·∫£i Ph√°p

**M·ª•c ti√™u**:

1. Gi·∫£m blocking time t·ª´ 240s xu·ªëng 60s (gi·∫£m 75%)
2. T·∫Øt LLM calls ho√†n to√†n (kh√¥ng ·∫£nh h∆∞·ªüng time)
3. Fail fast ‚Üí Gi·∫£i ph√≥ng memory ngay khi timeout
4. Kh√¥ng RE-queue ‚Üí Tr√°nh retry v√¥ h·∫°n
5. Mark FAILED trong DB ‚Üí Cron job retry v·ªõi exponential backoff + jitter
6. Context manager cleanup ‚Üí Guaranteed memory release
7. Bounded queue v·ªõi backpressure ‚Üí Fail-fast khi qu√° t·∫£i (preventive)

**Gi·∫£i ph√°p ch√≠nh (Critical)**:

1. Timeout 60s (fail-fast)
2. Timeout handling - Mark FAILED v√† ACK (kh√¥ng RE-queue)
3. Context manager cleanup (gi·∫£i ph√≥ng memory ngay)
4. Disable LLM ho√†n to√†n
5. Exponential backoff v·ªõi jitter

**Gi·∫£i ph√°p ph√≤ng ng·ª´a (Preventive)**:

1. Bounded queue v·ªõi backpressure (nice-to-have)
### 3.2. Implementation Details

#### 3.2.1. Gi·∫£m Memory API Timeout: 240s ‚Üí 60s

**File**: `src/app/core/config_settings.py`

```python
# Tr∆∞·ªõc:
MEMORY_API_TIMEOUT_SECONDS: int = 240  # 4 ph√∫t

# Sau:
MEMORY_API_TIMEOUT_SECONDS: int = 60  # 1 ph√∫t
```

**Impact**:

- Blocking time gi·∫£m 75% (240s ‚Üí 60s)
- Throughput tƒÉng ~4x (0.04 msg/s ‚Üí 0.17 msg/s)
- Memory gi·ªØ ng·∫Øn h∆°n
**Th√™m MAX_RETRY_ATTEMPTS**:

```python
MAX_RETRY_ATTEMPTS: int = 5  # Max retry attempts
```

**Rationale**: Gi·ªõi h·∫°n retry ƒë·ªÉ tr√°nh infinite loop.

#### 3.2.2. T·∫Øt LLM Calls Ho√†n To√†n

**File**: `src/app/core/config_settings.py`

```python
# Set trong .env ho·∫∑c config
LLM_ANALYSIS_ENABLED: bool = False
GROQ_API_KEY: Optional[str] = None  # Ho·∫∑c kh√¥ng set
```

**Code ƒë√£ c√≥ check**:

```1038:1044:src/app/services/utils/llm_analysis_utils.py
llm_enabled = llm_client.is_enabled()
if not llm_enabled:
    logger.warning(
        f"‚ö†Ô∏è  LLM analysis disabled | "
        f"LLM_ANALYSIS_ENABLED={settings.LLM_ANALYSIS_ENABLED} | "
        f"GROQ_API_KEY={'set' if settings.GROQ_API_KEY else 'not set'}"
    )
```

**Impact**:

- LLM calls return ngay (0s) n·∫øu disabled
- Kh√¥ng block worker threads
- Kh√¥ng ·∫£nh h∆∞·ªüng processing time

#### 3.2.3. Timeout ‚Üí Mark FAILED v√†o DB v√† ACK, Kh√¥ng RE-queue

**File**: `src/app/background/rabbitmq_consumer.py`

**S·ª≠a exception handling**:

**File**: `src/app/background/rabbitmq_consumer.py`

```python
except httpx.TimeoutException as e:
    # Memory API timeout ‚Üí Mark FAILED, kh√¥ng RE-queue
    error_msg = f"Memory API timeout after 60s: {str(e)}"
    logger.error(
        f"‚ùå Memory API timeout | "
        f"conversation_id={conversation_id} | "
        f"error={error_msg}"
    )
  
    # Alert khi Memory API timeout (rate limit: 1 l·∫ßn m·ªói 5 ph√∫t)
    current_time = time.time()
    if not hasattr(self, '_last_timeout_alert_time'):
        self._last_timeout_alert_time = 0
  
    if current_time - self._last_timeout_alert_time > 300:  # 5 ph√∫t
        send_alert_safe(
            alert_type=AlertType.EXTERNAL_API_TIMEOUT,
            level=AlertLevel.HIGH,
            message=(
                f"Worker: Memory API timeout after 60s | "
                f"conversation_id={conversation_id} | "
                f"Event marked as FAILED, will retry via cron job"
            ),
            context={
                "timeout_seconds": 60,
                "conversation_id": conversation_id,
                "error_code": "MEMORY_API_TIMEOUT",
                "component": "worker",
                "action": "marked_failed_no_requeue",
                "retry_mechanism": "cron_job_6h"
            },
            component="worker",
            conversation_id=conversation_id
        )
        self._last_timeout_alert_time = current_time
  
    # Mark FAILED trong DB
    if event:
        try:
            self.repository.mark_failed(
                event=event,
                error_code="MEMORY_API_TIMEOUT",
                error_details=error_msg
            )
            db.commit()
        except Exception as db_error:
            logger.error(f"‚ùå Failed to mark event as FAILED: {db_error}")
            db.rollback()
  
    # ACK message (kh√¥ng RE-queue)
    should_ack = True
  
    # Gi·∫£i ph√≥ng memory ngay
    if conversation_log:
        del conversation_log
    if 'formatted_conversation' in locals():
        del formatted_conversation
    if 'payload' in locals():
        del payload
    gc.collect()  # Force GC

except Exception as e:
    # C√°c l·ªói kh√°c v·∫´n NACK (retry)
    error_msg = str(e)
    logger.error(...)
    should_nack = True
```

**Impact**:

- Fail fast ‚Üí Gi·∫£i ph√≥ng memory ngay
- Kh√¥ng RE-queue ‚Üí Tr√°nh retry v√¥ h·∫°n
- Mark FAILED ‚Üí Cron job retry v·ªõi exponential backoff
- Alert ƒë·ªÉ track timeout events
#### 3.2.4. Gi·∫£i Ph√≥ng Memory Ngay Khi Timeout - Context Manager cho Guaranteed Cleanup

**File**: `src/app/background/rabbitmq_consumer.py`

**Rationale**: Gi·∫£i ph√≥ng memory ngay sau khi x·ª≠ l√Ω xong, tr√°nh Python GC delay g√¢y memory spike.

**T·∫°o `conversation_log_context()` Context Manager**:

```python
from contextlib import contextmanager
import gc

@contextmanager
def conversation_log_context(conversation_log, formatted_conversation=None, payload=None):
    """
    Context manager ƒë·ªÉ guaranteed cleanup c·ªßa large objects.
  
    Usage:
        with conversation_log_context(conversation_log, formatted_conversation, payload):
            # Process...
    """
    try:
        yield
    finally:
        # Cleanup
        if conversation_log:
            del conversation_log
        if formatted_conversation:
            del formatted_conversation
        if payload:
            del payload
        gc.collect()  # Force GC ngay
```

**S·ª≠ d·ª•ng trong `_process_message()`**:

```python
def _process_message(self, delivery_tag: int, body: bytes):
    conversation_log = None
    formatted_conversation = None
    payload = None
  
    try:
        # Parse message
        message = json.loads(body)
        conversation_log = message.get("conversation_log", [])
      
        # ... process ...
      
        # S·ª≠ d·ª•ng context manager
        with conversation_log_context(conversation_log, formatted_conversation, payload):
            # Process event...
            result = processor.process_single_event_with_log(...)
          
    except httpx.TimeoutException:
        # ... timeout handling ...
    except Exception:
        # ... other errors ...
    finally:
        # Context manager ƒë√£ cleanup, nh∆∞ng ƒë·∫£m b·∫£o th√™m
        if conversation_log:
            del conversation_log
        # ... DB cleanup ...
```

**Impact**:

- Memory ƒë∆∞·ª£c gi·∫£i ph√≥ng ngay khi x·ª≠ l√Ω xong (guaranteed)
- Kh√¥ng ch·ªù GC ‚Üí Gi·∫£m memory spike
- Context manager ƒë·∫£m b·∫£o cleanup ngay c·∫£ khi c√≥ exception

#### 3.2.5. Exponential Backoff v·ªõi Jitter (Critical)

**File**: `src/app/repositories/conversation_event_repository.py`

**Rationale**: Ph√¢n t√°n retry time ƒë·ªÉ tr√°nh thundering herd khi cron job ch·∫°y.

**S·ª≠a `mark_failed()` ƒë·ªÉ d√πng exponential backoff**:

```python
import random
from datetime import datetime, timedelta, timezone

def mark_failed(
    self,
    event: ConversationEvent,
    error_code: str,
    error_details: str,
    retry_attempt: Optional[int] = None
) -> ConversationEvent:
    """Set status to FAILED v√† schedule retry v·ªõi exponential backoff + jitter."""
    event.status = ConversationEventStatus.FAILED.value
    event.error_code = error_code
    event.error_details = error_details
  
    # Calculate retry attempt
    if retry_attempt is None:
        retry_attempt = 0
  
    # Exponential backoff: base_hours * (2 ^ retry_attempt)
    base_hours = 6  # CONVERSATION_EVENT_RETRY_HOURS
    backoff_hours = base_hours * (2 ** retry_attempt)
  
    # Jitter: ¬±20% random
    jitter_percent = random.uniform(-0.2, 0.2)
    jitter_hours = backoff_hours * jitter_percent
    total_hours = backoff_hours + jitter_hours
  
    # Max 48 hours
    total_hours = min(total_hours, 48)
  
    retry_at = datetime.now(timezone.utc) + timedelta(hours=total_hours)
    event.next_attempt_at = retry_at
    event.updated_at = datetime.now(timezone.utc)
  
    self.db.commit()
    self.db.refresh(event)
    return event
```

**V√≠ d·ª• exponential backoff v·ªõi jitter**:

```
Retry attempt 0: 6h √ó (2^0) = 6h ¬± 20% = 4.8h - 7.2h
Retry attempt 1: 6h √ó (2^1) = 12h ¬± 20% = 9.6h - 14.4h
Retry attempt 2: 6h √ó (2^2) = 24h ¬± 20% = 19.2h - 28.8h
Retry attempt 3: 6h √ó (2^3) = 48h (max)
```

**Impact**:

- Ph√¢n t√°n retry time ‚Üí Tr√°nh thundering herd
- TƒÉng d·∫ßn th·ªùi gian ch·ªù ‚Üí Gi·∫£m t·∫£i server
- Jitter ¬±20% ‚Üí Retry r·∫£i r√°c trong kho·∫£ng th·ªùi gian

**File**: `src/app/services/conversation_event_processing_service.py`

**Th√™m `should_retry()` v√† `calculate_next_attempt_time()`**:

```python
MAX_RETRY_ATTEMPTS = 5

def should_retry(event: ConversationEvent) -> bool:
    """Check xem c√≥ n√™n retry kh√¥ng."""
    retry_count = getattr(event, 'retry_count', 0)
    if retry_count >= MAX_RETRY_ATTEMPTS:
        return False
  
    if event.next_attempt_at and event.next_attempt_at > datetime.now(timezone.utc):
        return False
  
    return True
```

**Update `process_failed_events()` ƒë·ªÉ mark PERMANENTLY_FAILED**:

```python
def process_failed_events(self) -> Dict[str, int]:
    """Process failed events v·ªõi exponential backoff."""
    stats = {"total": 0, "processed": 0, "failed": 0, "skipped": 0}
  
    while True:
        events = self.repository.fetch_due_events(batch_size=25)
        if not events:
            break
      
        stats["total"] += len(events)
      
        for event in events:
            # Check should retry
            if not self.should_retry(event):
                # Mark PERMANENTLY_FAILED
                event.status = ConversationEventStatus.PERMANENTLY_FAILED.value
                self.repository.db.commit()
              
                # Alert CRITICAL khi permanently failed
                send_alert_safe(
                    alert_type=AlertType.WORKFLOW_EXECUTION_FAILURE,
                    level=AlertLevel.CRITICAL,
                    message=(
                        f"Event permanently failed after {retry_count} retry attempts | "
                        f"conversation_id={event.conversation_id} | "
                        f"Manual intervention may be required"
                    ),
                    context={
                        "conversation_id": event.conversation_id,
                        "retry_count": retry_count,
                        "error_code": event.error_code,
                        "max_retry_attempts": MAX_RETRY_ATTEMPTS,
                        "component": "worker",
                        "action": "permanently_failed",
                        "requires_manual_intervention": True
                    },
                    component="worker",
                    conversation_id=event.conversation_id
                )
              
                stats["skipped"] += 1
                continue
          
            # Process event...
            # ...
  
    return stats
```

**Impact**:

- Gi·ªõi h·∫°n retry attempts (5 l·∫ßn) ‚Üí Tr√°nh infinite loop
- Mark PERMANENTLY_FAILED ‚Üí C·∫ßn manual intervention
- Alert CRITICAL ‚Üí C·∫£nh b√°o data loss risk
### 3.3. Database Schema

**C·ªôt status trong `conversation_events`**:

```sql
status VARCHAR(50) NOT NULL DEFAULT 'PENDING'
    CHECK (status IN ('PENDING', 'PROCESSING', 'PROCESSED', 'FAILED', 'SKIPPED'))
```

**C√°c c·ªôt li√™n quan**:

- `error_code`: L∆∞u "MEMORY_API_TIMEOUT"
- `error_details`: L∆∞u chi ti·∫øt l·ªói
- `next_attempt_at`: Set = now + 6 hours (cho cron job)

### 3.4. Cron Job Retry (ƒê√£ Code S·∫µn)

**Cron job ƒë√£ c√≥ s·∫µn** s·∫Ω:

1. Query events v·ªõi `status='FAILED'` v√† `next_attempt_at <= now`
2. Retry processing
3. N·∫øu v·∫´n fail ‚Üí `next_attempt_at += 6 hours`
### 3.4. Cron Job Retry v·ªõi Exponential Backoff

**Cron job ƒë√£ c√≥ s·∫µn** s·∫Ω:

1. Query events v·ªõi `status='FAILED'` v√† `next_attempt_at <= now`
2. Check `should_retry()` ‚Üí N·∫øu kh√¥ng ‚Üí Mark PERMANENTLY_FAILED
3. Retry processing
4. N·∫øu v·∫´n fail ‚Üí Increment `retry_count` v√† t√≠nh `next_attempt_at` m·ªõi v·ªõi exponential backoff + jitter

### 3.5. Bounded Queue v·ªõi Backpressure (Preventive)

**File**: `src/app/background/rabbitmq_consumer.py`

**Rationale**: Backpressure mechanism ƒë·ªÉ tr√°nh qu√° t·∫£i khi system kh√¥ng th·ªÉ x·ª≠ l√Ω k·ªãp. L∆∞u √Ω: Queue ch·ªâ gi·ªØ bytes nh·ªè (~280 bytes/message), kh√¥ng ph·∫£i nguy√™n nh√¢n ch√≠nh g√¢y OOM, nh∆∞ng v·∫´n h·ªØu √≠ch ƒë·ªÉ fail-fast khi qu√° t·∫£i.

**Configuration**:

```python
QUEUE_MAX_SIZE: int = 100  # Max queue size
QUEUE_BACKPRESSURE_THRESHOLD: float = 0.8  # 80% threshold
```

**Implement `_check_queue_and_backpressure()`**:

```python
def _check_queue_and_backpressure(
    self, 
    delivery_tag: int, 
    message_body: bytes
) -> bool:
    """Check queue size v√† apply backpressure."""
    queue_size = self.executor._work_queue.qsize()
    max_size = settings.QUEUE_MAX_SIZE
    threshold = int(max_size * settings.QUEUE_BACKPRESSURE_THRESHOLD)
  
    if queue_size >= threshold:
        # Alert khi v∆∞·ª£t threshold (rate limit: 1 l·∫ßn m·ªói 5 ph√∫t)
        send_alert_safe(
            alert_type=AlertType.WORKFLOW_EXECUTION_FAILURE,
            level=AlertLevel.HIGH,
            message=(
                f"Worker queue size exceeded threshold: "
                f"{queue_size}/{max_size} ({queue_percent:.1f}%)"
            ),
            context={
                "queue_size": queue_size,
                "max_size": max_size,
                "threshold": threshold,
                "component": "worker",
                "action": "backpressure_triggered"
            },
            component="worker"
        )
      
        # Parse message ƒë·ªÉ l·∫•y conversation_id
        message = json.loads(message_body)
        conversation_id = message.get('conversation_id')
      
        # Mark FAILED trong DB TR∆Ø·ªöC
        db = SessionLocal()
        try:
            repo = ConversationEventRepository(db)
            event = repo.get_by_conversation_id(conversation_id)
            if event:
                repo.mark_failed(
                    event=event,
                    error_code="QUEUE_FULL",
                    error_details=f"Worker queue full: {queue_size}/{max_size}"
                )
                db.commit()
        finally:
            db.close()
      
        # Sau ƒë√≥ m·ªõi NACK v·ªõi requeue=False
        self._do_nack_no_requeue(delivery_tag)
        return False  # Reject message
  
    return True  # Queue OK, c√≥ th·ªÉ submit
```

**Update `callback()` ƒë·ªÉ s·ª≠ d·ª•ng backpressure**:

```python
def callback(self, ch, method, properties, body):
    delivery_tag = method.delivery_tag
    message_body = body
  
    # Check backpressure TR∆Ø·ªöC khi submit
    if not self._check_queue_and_backpressure(delivery_tag, message_body):
        # Message ƒë√£ ƒë∆∞·ª£c reject v√† mark FAILED
        return
  
    # Submit v√†o thread pool
    self.executor.submit(self._process_message, delivery_tag, message_body)
```

**Impact**:

- Fail-fast khi queue ƒë·∫ßy ‚Üí Tr√°nh qu√° t·∫£i
- Mark FAILED trong DB tr∆∞·ªõc khi NACK ‚Üí Kh√¥ng m·∫•t message
- Alert ƒë·ªÉ c·∫£nh b√°o s·ªõm

### 3.6. So S√°nh Tr∆∞·ªõc/Sau

| Metric             | Tr∆∞·ªõc               | Sau                                         |
| ------------------ | --------------------- | ------------------------------------------- |
| Memory API timeout | 240s                  | 60s                                         |
| LLM calls          | Ch·∫°y (n·∫øu enabled)  | T·∫Øt ho√†n to√†n                            |
| Blocking time      | 240s+                 | 60s                                         |
| Retry behavior     | NACK ‚Üí RE-queue ngay | Mark FAILED ‚Üí Exponential backoff + jitter |
| Memory cleanup     | Ch·∫≠m (sau timeout)   | Ngay (context manager)                      |
| Throughput         | 0.04 msg/s            | 0.17 msg/s                                  |
| Memory spike risk  | Cao                   | Th·∫•p                                       |
| Max retry attempts | V√¥ h·∫°n              | 5 l·∫ßn ‚Üí PERMANENTLY_FAILED                |
| Queue management   | Unbounded             | Bounded v·ªõi backpressure                   |

### 3.7. Alerts

**3 lo·∫°i alerts ƒë∆∞·ª£c implement**:

1. **Queue Size Alert** (HIGH):

   - Trigger: Queue size >= 80% threshold
   - Rate limit: 1 l·∫ßn m·ªói 5 ph√∫t
   - Action: Mark FAILED trong DB, NACK v·ªõi `requeue=False`
2. **Memory API Timeout Alert** (HIGH):

   - Trigger: Memory API timeout sau 60s
   - Rate limit: 1 l·∫ßn m·ªói 5 ph√∫t
   - Action: Mark FAILED trong DB, ACK message (kh√¥ng RE-queue)
3. **Permanently Failed Alert** (CRITICAL):

   - Trigger: Event retry h·∫øt 5 l·∫ßn
   - Rate limit: Kh√¥ng (m·ªói event l√† critical)
   - Action: Mark PERMANENTLY_FAILED, c·∫ßn manual intervention
### 3.6. K·∫øt Qu·∫£ Mong ƒê·ª£i

1. Gi·∫£m blocking time 75% (240s ‚Üí 60s)
2. Fail fast ‚Üí Gi·∫£i ph√≥ng memory ngay
3. Kh√¥ng RE-queue ‚Üí Tr√°nh loop retry
4. Cron job retry ‚Üí V·∫´n c√≥ c∆° h·ªôi x·ª≠ l√Ω l·∫°i
5. Memory spike gi·∫£m ƒë√°ng k·ªÉ

---

### 3.8. K·∫øt Qu·∫£ Mong ƒê·ª£i

1. Gi·∫£m blocking time 75% (240s ‚Üí 60s)
2. Fail fast ‚Üí Gi·∫£i ph√≥ng memory ngay (context manager)
3. Kh√¥ng RE-queue ‚Üí Tr√°nh loop retry
4. Exponential backoff + jitter ‚Üí Ph√¢n t√°n retry, tr√°nh thundering herd
5. Max retry attempts (5 l·∫ßn) ‚Üí Tr√°nh infinite loop
6. Bounded queue v·ªõi backpressure ‚Üí Fail-fast khi qu√° t·∫£i
7. Memory spike gi·∫£m ƒë√°ng k·ªÉ
8. Alerts ƒë·ªÉ track v√† c·∫£nh b√°o s·ªõm

---

## 4. T√ìM T·∫ÆT

### V·∫•n ƒë·ªÅ:

- Worker b·ªã OOM kill (exit code 137) t·∫°i 11:42:04 AM
- Memory tƒÉng ƒë·ªôt bi·∫øn t·ª´ 250MB ‚Üí 3GB
- Memory API timeout 240s qu√° cao
- pika-mem0 service kh√¥ng response (timeout 60s)

### Nguy√™n nh√¢n ch√≠nh:

1. Memory API timeout = 240s (qu√° cao)
2. ThreadPoolExecutor queue kh√¥ng gi·ªõi h·∫°n
3. NACK v·ªõi requeue=True ‚Üí Retry v√¥ h·∫°n
4. Exception stack traces gi·ªØ references
5. Python GC delay ‚Üí Memory kh√¥ng ƒë∆∞·ª£c gi·∫£i ph√≥ng ngay

### Gi·∫£i ph√°p:

**Gi·∫£i ph√°p ch√≠nh (Critical)**:

1. Gi·∫£m Memory API timeout: 240s ‚Üí 60s
2. T·∫Øt LLM calls ho√†n to√†n
3. Timeout ‚Üí Mark FAILED, kh√¥ng RE-queue (v·ªõi alerts)
4. Context manager cleanup ‚Üí Gi·∫£i ph√≥ng memory ngay (guaranteed)
5. Exponential backoff v·ªõi jitter ‚Üí Ph√¢n t√°n retry
6. Max retry attempts (5 l·∫ßn) ‚Üí Mark PERMANENTLY_FAILED

**Gi·∫£i ph√°p ph√≤ng ng·ª´a (Preventive)**:
7. Bounded queue v·ªõi backpressure ‚Üí Fail-fast khi qu√° t·∫£i

---

**Document n√†y t·ªïng h·ª£p t·ª´ c√°c ph√¢n t√≠ch chi ti·∫øt trong folder `docs/6_OMM_worker/`**