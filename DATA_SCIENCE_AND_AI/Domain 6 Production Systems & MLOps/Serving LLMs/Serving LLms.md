
Ráº¥t sáºµn lÃ²ng! ÄÃ¢y lÃ  báº£ng "bÃ­ kÃ­p" cÃ¡c chá»‰ sá»‘ cáº¥u hÃ¬nh vLLM, Ä‘Æ°á»£c giáº£i thÃ­ch siÃªu Ä‘Æ¡n giáº£n nhÆ° Ä‘ang hÆ°á»›ng dáº«n xÃ¢y dá»±ng má»™t nhÃ  mÃ¡y LEGO.

# 1. ğŸ­ Báº£ng "BÃ­ KÃ­p" Cáº¥u HÃ¬nh NhÃ  MÃ¡y AI (vLLM)

TÆ°á»Ÿng tÆ°á»£ng GPU cá»§a báº¡n lÃ  má»™t **NhÃ  mÃ¡y láº¯p rÃ¡p LEGO (AI)**, vÃ  cÃ¡c chá»‰ sá»‘ cáº¥u hÃ¬nh lÃ  báº£ng Ä‘iá»u khiá»ƒn cá»§a GiÃ¡m Ä‘á»‘c nhÃ  mÃ¡y (báº¡n).

| TÃªn Chá»‰ Sá»‘ (Config Name)       | Ã NghÄ©a (Giáº£i thÃ­ch cho há»c sinh)                                                                                                | VÃ­ dá»¥ Thá»±c táº¿                                                                                                                                   | GiÃ¡ trá»‹ nÃªn Ä‘áº·t (Cho Pika)                                             | CÃ³ áº£nh hÆ°á»Ÿng Ä‘áº¿n response time nhÆ° nÃ o ? |
| ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------- |
| **`--gpu-memory-utilization`** | **Diá»‡n tÃ­ch Kho chá»©a hÃ ng**  <br>Báº¡n dÃ nh bao nhiÃªu pháº§n trÄƒm sÃ¢n nhÃ  mÃ¡y Ä‘á»ƒ chá»©a nguyÃªn liá»‡u (KV Cache).                        | Náº¿u nhÃ  mÃ¡y rá»™ng 100mÂ², báº¡n dÃ nh 60mÂ² lÃ m kho thÃ¬ chá»‰ sá»‘ lÃ  0.6.  <br>_Kho cÃ ng rá»™ng â†’ Chá»©a Ä‘Æ°á»£c cÃ ng nhiá»u Ä‘Æ¡n hÃ ng cÃ¹ng lÃºc._                 | `0.3` - `0.4`  <br>_(Model nhá» nÃªn kho khÃ´ng cáº§n quÃ¡ to, vá»«a Ä‘á»§ dÃ¹ng)_ |                                          |
| **`--max-model-len`**          | **Äá»™ dÃ i bÄƒng chuyá»n tá»‘i Ä‘a**  <br>Chiá»u dÃ i tá»‘i Ä‘a cá»§a má»™t sáº£n pháº©m (cÃ¢u há»i + cÃ¢u tráº£ lá»i) mÃ  dÃ¢y chuyá»n cÃ³ thá»ƒ xá»­ lÃ½.         | Náº¿u bÄƒng chuyá»n dÃ i 2 mÃ©t, báº¡n khÃ´ng thá»ƒ láº¯p con rá»“ng dÃ i 3 mÃ©t. NÃ³ sáº½ bá»‹ cáº¯t cá»¥t Ä‘uÃ´i.  <br>_DÃ i quÃ¡ â†’ Tá»‘n chá»—. Ngáº¯n quÃ¡ â†’ KhÃ´ng Ä‘á»§ viáº¿t vÄƒn._ | `512` - `1024`  <br>_(Pika nÃ³i ngáº¯n gá»n, khÃ´ng viáº¿t tiá»ƒu thuyáº¿t)_      |                                          |
| **`--max-num-seqs`**           | **Sá»‘ lÃ n cháº¡y song song**  <br>NhÃ  mÃ¡y cÃ³ thá»ƒ phá»¥c vá»¥ tá»‘i Ä‘a bao nhiÃªu khÃ¡ch hÃ ng cÃ¹ng má»™t lÃºc.                                  | Giá»‘ng nhÆ° quáº§y thu ngÃ¢n siÃªu thá»‹. Chá»‰ sá»‘ nÃ y lÃ  sá»‘ lÆ°á»£ng quáº§y má»Ÿ cá»­a.  <br>_CÃ ng nhiá»u â†’ Phá»¥c vá»¥ cÃ ng Ä‘Ã´ng, nhÆ°ng cáº§n kho (VRAM) to._           | `256` - `512`  <br>_(Model nhá» nÃªn má»Ÿ nhiá»u lÃ n thoáº£i mÃ¡i)_            |                                          |
| **`--enable-prefix-caching`**  | **Cháº¿ Ä‘á»™ "Copy bÃ i máº«u"**  <br>Ghi nhá»› pháº§n má»Ÿ Ä‘áº§u giá»‘ng nhau (vÃ­ dá»¥: luáº­t chÆ¡i) Ä‘á»ƒ khÃ´ng pháº£i Ä‘á»c láº¡i tá»« Ä‘áº§u vá»›i má»—i khÃ¡ch má»›i. | Giá»‘ng nhÆ° giÃ¡o viÃªn chÃ©p Ä‘á» bÃ i lÃªn báº£ng. Há»c sinh chá»‰ cáº§n chÃ©p lá»i giáº£i, khÃ´ng pháº£i chÃ©p láº¡i Ä‘á».  <br>_Tiáº¿t kiá»‡m ráº¥t nhiá»u thá»i gian!_         | `True` (Báº­t)  <br>_(Ráº¥t quan trá»ng vÃ¬ Pika dÃ¹ng chung system prompt)_  |                                          |
| **`--dtype`**                  | **Äá»™ tinh xáº£o cá»§a gáº¡ch LEGO**  <br>KÃ­ch thÆ°á»›c vÃ  Ä‘á»™ chÃ­nh xÃ¡c cá»§a tá»«ng máº£nh dá»¯ liá»‡u.                                             | - `float16`: Gáº¡ch tiÃªu chuáº©n (nháº¹, nhanh).  <br>- `float32`: Gáº¡ch cao cáº¥p (náº·ng gáº¥p Ä‘Ã´i, chÃ­nh xÃ¡c hÆ¡n xÃ­u).                                    | `float16` hoáº·c `auto`  <br>_(Nhanh vÃ  nháº¹ lÃ  Æ°u tiÃªn)_                 |                                          |
| **`--enforce-eager`**          | **Cháº¿ Ä‘á»™ "Cáº§m tay chá»‰ viá»‡c"**  <br>CPU ra lá»‡nh tá»«ng bÆ°á»›c nhá» cho GPU lÃ m. (Táº¯t cháº¿ Ä‘á»™ nÃ y = Báº­t CUDA Graphs).                    | - Báº­t (`True`): Sáº¿p Ä‘á»©ng kÃ¨ kÃ¨ chá»‰ tá»«ng bÆ°á»›c (cháº­m).  <br>- Táº¯t (`False`): Sáº¿p Ä‘Æ°a cáº£ báº£n váº½, thá»£ tá»± lÃ m má»™t máº¡ch (nhanh).                      | `False` (Táº¯t)  <br>_(Äá»ƒ kÃ­ch hoáº¡t CUDA Graphs siÃªu tá»‘c)_               |                                          |
| **`--max-num-batched-tokens`** | **Sá»©c chá»©a tá»‘i Ä‘a cá»§a xe Ä‘áº©y**  <br>Tá»•ng sá»‘ máº£nh LEGO tá»‘i Ä‘a mÃ  nhÃ  mÃ¡y cÃ³ thá»ƒ xá»­ lÃ½ trong má»™t nhá»‹p.                             | Xe Ä‘áº©y chá»‰ chá»Ÿ Ä‘Æ°á»£c 2000 máº£nh. DÃ¹ cÃ³ 100 Ä‘Æ¡n hÃ ng nhá» hay 1 Ä‘Æ¡n hÃ ng to, tá»•ng láº¡i khÃ´ng Ä‘Æ°á»£c quÃ¡ sá»‘ nÃ y.                                        | `2048`  <br>_(Äá»§ sá»©c cÃ¢n cáº£ trÄƒm cÃ¢u tráº£ lá»i ngáº¯n)_                    |                                          |

|                                |                                                                                                                                               |                                                                                                                                                                |                                                                |     |
| ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- | --- |
| **`--max-num-batched-tokens`** | **Sá»©c táº£i cá»§a xe nÃ¢ng hÃ ng**  <br>Tá»•ng sá»‘ lÆ°á»£ng máº£nh ghÃ©p tá»‘i Ä‘a mÃ  má»™t láº§n xe nÃ¢ng cÃ³ thá»ƒ bá»‘c lÃªn Ä‘á»ƒ Ä‘Æ°a vÃ o mÃ¡y lÃ m.                        | Xe nÃ¢ng chá»‰ chá»Ÿ Ä‘Æ°á»£c 2048 máº£nh. DÃ¹ lÃ  1 con rá»“ng to (2000 máº£nh) hay 10 con gÃ  nhá» (200 máº£nh/con), tá»•ng cá»™ng khÃ´ng Ä‘Æ°á»£c quÃ¡ táº£i trá»ng xe.                       | `2048` - `4096`  <br>_(Äá»§ lá»›n Ä‘á»ƒ xá»­ lÃ½ nhiá»u yÃªu cáº§u gá»™p láº¡i)_ |     |
| **`--kv-cache-dtype`**         | **Cháº¥t liá»‡u khay Ä‘á»±ng nguyÃªn liá»‡u**  <br>Báº¡n chá»n loáº¡i khay nhá»±a thÆ°á»ng (fp16) hay khay nhá»±a nÃ©n (fp8) Ä‘á»ƒ Ä‘á»±ng cÃ¡c khá»‘i nhá»› táº¡m thá»i.         | - `auto`: MÃ¡y tá»± chá»n loáº¡i khay tá»‘t nháº¥t.  <br>- `fp8`: Khay má»ng hÆ¡n, chá»©a Ä‘Æ°á»£c nhiá»u gáº¥p Ä‘Ã´i nhÆ°ng hÆ¡i khÃ³ xáº¿p.  <br>_GiÃºp tiáº¿t kiá»‡m diá»‡n tÃ­ch kho (VRAM)._  | `auto`  <br>_(Äá»ƒ vLLM tá»± quyáº¿t Ä‘á»‹nh cÃ¡i nÃ o an toÃ n nháº¥t)_     |     |
| **`--disable-log-requests`**   | **Táº¯t loa thÃ´ng bÃ¡o tá»«ng Ä‘Æ¡n hÃ ng**  <br>BÃ¬nh thÆ°á»ng má»—i khi cÃ³ khÃ¡ch gá»i mÃ³n, loa sáº½ "Alo! CÃ³ Ä‘Æ¡n má»›i!". Táº¯t Ä‘i Ä‘á»ƒ Ä‘á»¡ á»“n vÃ  Ä‘á»¡ tá»‘n Ä‘iá»‡n.     | Náº¿u nhÃ  mÃ¡y quÃ¡ Ä‘Ã´ng khÃ¡ch, loa kÃªu liÃªn tá»¥c sáº½ lÃ m Ä‘iáº¿c tai vÃ  cháº­m tiáº¿n Ä‘á»™. Táº¯t Ä‘i, chá»‰ lÃ m thÃ´i, khÃ´ng nÃ³i nhiá»u.  <br>_GiÃºp mÃ¡y cháº¡y nhanh hÆ¡n xÃ­u xiu._   | `True` (CÃ³ cá» nÃ y)  <br>_(Táº¯t log Ä‘á»ƒ táº­p trung tá»‘c Ä‘á»™)_        |     |
| **`--trust-remote-code`**      | **ChÃ¬a khÃ³a váº¡n nÄƒng**  <br>Cho phÃ©p nhÃ  mÃ¡y má»Ÿ cÃ¡c báº£n váº½ thiáº¿t káº¿ láº¡ tá»« bÃªn ngoÃ i (HuggingFace) mÃ  khÃ´ng cáº§n kiá»ƒm duyá»‡t gáº¯t gao.            | Báº¡n táº£i báº£n váº½ mod cá»§a má»™t phÃ¡p sÆ° trÃªn máº¡ng vá». MÃ¡y há»i: "Tin Ã´ng nÃ y khÃ´ng?". Báº¡n báº£o "Tin!" thÃ¬ mÃ¡y má»›i chá»‹u cháº¡y.  <br>_Báº¯t buá»™c vá»›i má»™t sá»‘ model má»›i/láº¡._ | `True` (CÃ³ cá» nÃ y)  <br>_(Cáº§n thiáº¿t cho SmolLM2/Phi-3)_        |     |
| **`--host 0.0.0.0`**           | **Má»Ÿ cá»­a chÃ­nh vÃ  cá»­a sá»•**  <br>Cho phÃ©p nháº­n Ä‘Æ¡n Ä‘áº·t hÃ ng tá»« báº¥t ká»³ ai, á»Ÿ báº¥t ká»³ Ä‘Ã¢u (trong máº¡ng LAN/Internet), khÃ´ng chá»‰ tá»« ná»™i bá»™ nhÃ  mÃ¡y. | - `127.0.0.1`: Chá»‰ ngÆ°á»i trong nhÃ  má»›i Ä‘Æ°á»£c Ä‘áº·t hÃ ng.  <br>- `0.0.0.0`: HÃ ng xÃ³m, ngÆ°á»i Ä‘i Ä‘Æ°á»ng Ä‘á»u cÃ³ thá»ƒ ghÃ© vÃ o Ä‘áº·t (náº¿u khÃ´ng cÃ³ báº£o vá»‡ cháº·n).            | `0.0.0.0`  <br>_(Äá»ƒ robot Pika tá»« mÃ¡y khÃ¡c gá»i Ä‘Æ°á»£c)_          |     |

---
# 2. BÃ¡o CÃ¡o ChuyÃªn SÃ¢u: CÃ¡c Ká»¹ Thuáº­t Tá»‘i Æ¯u HÃ³a Hiá»‡u Suáº¥t Serving cho MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLM)

**TÃ¡c giáº£**: Manus AI
**NgÃ y**: 08 thÃ¡ng 12 nÄƒm 2025

## Giá»›i thiá»‡u

Viá»‡c triá»ƒn khai vÃ  phá»¥c vá»¥ (serving) cÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯ Lá»›n (LLM) trong mÃ´i trÆ°á»ng sáº£n xuáº¥t Ä‘áº·t ra nhá»¯ng thÃ¡ch thá»©c Ä‘Ã¡ng ká»ƒ vá» hiá»‡u suáº¥t vÃ  chi phÃ­. Khi cÃ¡c mÃ´ hÃ¬nh ngÃ y cÃ ng trá»Ÿ nÃªn phá»©c táº¡p, viá»‡c Ä‘áº£m báº£o Ä‘á»™ trá»… (latency) tháº¥p vÃ  thÃ´ng lÆ°á»£ng (throughput) cao trá»Ÿ thÃ nh yáº¿u tá»‘ then chá»‘t Ä‘á»ƒ mang láº¡i tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng tá»‘t vÃ  tá»‘i Æ°u hÃ³a chi phÃ­ váº­n hÃ nh. BÃ¡o cÃ¡o nÃ y sáº½ Ä‘i sÃ¢u vÃ o cÃ¡c ká»¹ thuáº­t tá»‘i Æ°u hÃ³a serving hiá»‡n Ä‘áº¡i, phÃ¢n tÃ­ch cÃ¡c chá»‰ sá»‘ hiá»‡u suáº¥t quan trá»ng, vÃ  cung cáº¥p má»™t quy trÃ¬nh cÃ³ há»‡ thá»‘ng Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t phá»¥c vá»¥ tá»‘i Æ°u, dá»±a trÃªn cÃ¡c tÃ i liá»‡u nghiÃªn cá»©u vÃ  vÃ­ dá»¥ thá»±c tiá»…n.

## 1. CÃ¡c Chá»‰ Sá»‘ Hiá»‡u Suáº¥t Then Chá»‘t trong LLM Serving

Äá»ƒ Ä‘Ã¡nh giÃ¡ vÃ  tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t cá»§a má»™t há»‡ thá»‘ng phá»¥c vá»¥ LLM, cáº§n pháº£i hiá»ƒu rÃµ cÃ¡c chá»‰ sá»‘ Ä‘o lÆ°á»ng chÃ­nh. CÃ¡c chá»‰ sá»‘ nÃ y khÃ´ng chá»‰ pháº£n Ã¡nh tá»‘c Ä‘á»™ cá»§a há»‡ thá»‘ng mÃ  cÃ²n cho tháº¥y kháº£ nÄƒng xá»­ lÃ½ Ä‘á»“ng thá»i nhiá»u yÃªu cáº§u vÃ  hiá»‡u quáº£ sá»­ dá»¥ng tÃ i nguyÃªn pháº§n cá»©ng.

| Chá»‰ Sá»‘ | TÃªn Tiáº¿ng Anh | MÃ´ Táº£ | Táº§m Quan Trá»ng | 
| :--- | :--- | :--- | :--- | 
| **Äá»™ trá»… token Ä‘áº§u tiÃªn** | Time To First Token (TTFT) | Thá»i gian tá»« khi ngÆ°á»i dÃ¹ng gá»­i yÃªu cáº§u cho Ä‘áº¿n khi nháº­n Ä‘Æ°á»£c token Ä‘áº§u tiÃªn cá»§a cÃ¢u tráº£ lá»i. | Pháº£n Ã¡nh kháº£ nÄƒng pháº£n há»“i tá»©c thÃ¬ cá»§a há»‡ thá»‘ng, ráº¥t quan trá»ng cho cÃ¡c á»©ng dá»¥ng tÆ°Æ¡ng tÃ¡c thá»i gian thá»±c nhÆ° chatbot. | 
| **Äá»™ trá»… giá»¯a cÃ¡c token** | Inter-Token Latency (ITL) | Thá»i gian trung bÃ¬nh Ä‘á»ƒ sinh ra má»—i token tiáº¿p theo sau token Ä‘áº§u tiÃªn. | áº¢nh hÆ°á»Ÿng Ä‘áº¿n tá»‘c Ä‘á»™ Ä‘á»c vÃ  cáº£m nháº­n vá» sá»± "trÃ´i cháº£y" cá»§a cÃ¢u tráº£ lá»i. | 
| **ThÃ´ng lÆ°á»£ng** | Throughput | Sá»‘ lÆ°á»£ng token hoáº·c yÃªu cáº§u Ä‘Æ°á»£c há»‡ thá»‘ng xá»­ lÃ½ trong má»™t Ä‘Æ¡n vá»‹ thá»i gian (thÆ°á»ng lÃ  tokens/giÃ¢y hoáº·c requests/giÃ¢y). | Äo lÆ°á»ng kháº£ nÄƒng xá»­ lÃ½ táº£i cá»§a há»‡ thá»‘ng, quyáº¿t Ä‘á»‹nh kháº£ nÄƒng má»Ÿ rá»™ng vÃ  chi phÃ­ trÃªn má»—i yÃªu cáº§u. | 
| **Sá»­ dá»¥ng bá»™ nhá»› GPU** | GPU Memory Utilization | Tá»· lá»‡ pháº§n trÄƒm bá»™ nhá»› GPU Ä‘Æ°á»£c cáº¥p phÃ¡t vÃ  sá»­ dá»¥ng cho viá»‡c lÆ°u trá»¯ trá»ng sá»‘ mÃ´ hÃ¬nh vÃ  KV Cache. | Tá»‘i Æ°u hÃ³a chá»‰ sá»‘ nÃ y giÃºp tÄƒng kÃ­ch thÆ°á»›c batch, tá»« Ä‘Ã³ tÄƒng thÃ´ng lÆ°á»£ng mÃ  khÃ´ng gÃ¢y ra lá»—i háº¿t bá»™ nhá»› (Out-of-Memory). | 

Viá»‡c tá»‘i Æ°u hÃ³a thÆ°á»ng lÃ  má»™t sá»± Ä‘Ã¡nh Ä‘á»•i giá»¯a cÃ¡c chá»‰ sá»‘ nÃ y. VÃ­ dá»¥, viá»‡c tÄƒng kÃ­ch thÆ°á»›c batch (batch size) Ä‘á»ƒ cáº£i thiá»‡n thÃ´ng lÆ°á»£ng thÆ°á»ng sáº½ lÃ m tÄƒng Ä‘á»™ trá»… cá»§a tá»«ng yÃªu cáº§u riÃªng láº». Do Ä‘Ã³, má»¥c tiÃªu lÃ  tÃ¬m ra Ä‘iá»ƒm cÃ¢n báº±ng tá»‘i Æ°u phÃ¹ há»£p vá»›i yÃªu cáº§u cá»¥ thá»ƒ cá»§a tá»«ng á»©ng dá»¥ng.

## 2. CÃ¡c Ká»¹ Thuáº­t Tá»‘i Æ¯u HÃ³a Cá»‘t LÃµi

QuÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a serving bao gá»“m nhiá»u lá»›p ká»¹ thuáº­t, tá»« nÃ©n mÃ´ hÃ¬nh á»Ÿ cáº¥p Ä‘á»™ cao Ä‘áº¿n tá»‘i Æ°u hÃ³a kernel á»Ÿ cáº¥p Ä‘á»™ tháº¥p. 

### 2.1. LÆ°á»£ng Tá»­ HÃ³a (Quantization)

LÆ°á»£ng tá»­ hÃ³a lÃ  quÃ¡ trÃ¬nh giáº£m Ä‘á»™ chÃ­nh xÃ¡c sá»‘ há»c cá»§a cÃ¡c trá»ng sá»‘ mÃ´ hÃ¬nh, thÆ°á»ng tá»« kiá»ƒu dá»¯ liá»‡u 32-bit (FP32) xuá»‘ng 16-bit (FP16/BF16) hoáº·c tháº­m chÃ­ 8-bit vÃ  4-bit. Ká»¹ thuáº­t nÃ y giÃºp giáº£m Ä‘Ã¡ng ká»ƒ dung lÆ°á»£ng lÆ°u trá»¯ cá»§a mÃ´ hÃ¬nh vÃ  yÃªu cáº§u bÄƒng thÃ´ng bá»™ nhá»›, tá»« Ä‘Ã³ tÄƒng tá»‘c Ä‘á»™ suy luáº­n [1].

> **VÃ­ dá»¥ thá»±c tiá»…n**: Trong tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p, mÃ´ hÃ¬nh `SmolLM2-135M` sá»­ dá»¥ng `dtype float16`, má»™t dáº¡ng lÆ°á»£ng tá»­ hÃ³a, giÃºp mÃ´ hÃ¬nh chá»‰ chiáº¿m khoáº£ng 300MB VRAM, cho phÃ©p nÃ³ cháº¡y trÃªn cÃ¡c GPU cÃ³ bá»™ nhá»› háº¡n cháº¿ vá»›i Ä‘á»™ trá»… ráº¥t tháº¥p.

CÃ¡c phÆ°Æ¡ng phÃ¡p lÆ°á»£ng tá»­ hÃ³a phá»• biáº¿n bao gá»“m:
- **AWQ (Activation-aware Weight Quantization)**: Báº£o vá»‡ má»™t pháº§n nhá» cÃ¡c trá»ng sá»‘ quan trá»ng khá»i viá»‡c lÆ°á»£ng tá»­ hÃ³a Ä‘á»ƒ duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh.
- **GPTQ (Post-Training Quantization for GPT)**: Má»™t phÆ°Æ¡ng phÃ¡p lÆ°á»£ng tá»­ hÃ³a sau huáº¥n luyá»‡n, cá»‘ gáº¯ng giáº£m thiá»ƒu sai sá»‘ lÆ°á»£ng tá»­ hÃ³a.
- **GGUF (GPT-Generated Unified Format)**: Má»™t Ä‘á»‹nh dáº¡ng file Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ táº£i vÃ  cháº¡y cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°á»£ng tá»­ hÃ³a má»™t cÃ¡ch hiá»‡u quáº£ trÃªn CPU vÃ  GPU.

### 2.2. CÆ¡ Cháº¿ Caching: Tá»‘i Æ¯u HÃ³a KV Cache vÃ  Prefix Caching

Trong kiáº¿n trÃºc Transformer, cÆ¡ cháº¿ attention yÃªu cáº§u tÃ­nh toÃ¡n cÃ¡c ma tráº­n Key (K) vÃ  Value (V) cho má»—i token. **KV Cache** lÃ  má»™t ká»¹ thuáº­t ná»n táº£ng, lÆ°u trá»¯ láº¡i cÃ¡c giÃ¡ trá»‹ K vÃ  V Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh toÃ¡n cá»§a cÃ¡c token trong chuá»—i Ä‘áº§u vÃ o Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng khi sinh ra cÃ¡c token tiáº¿p theo, trÃ¡nh viá»‡c tÃ­nh toÃ¡n láº¡i tá»« Ä‘áº§u [2].

**Prefix Caching** (hay Prompt Caching) lÃ  má»™t bÆ°á»›c tiáº¿n xa hÆ¡n cá»§a KV Cache. NÃ³ lÆ°u trá»¯ vÃ  tÃ¡i sá»­ dá»¥ng KV cache cá»§a má»™t tiá»n tá»‘ (prefix) chung qua nhiá»u yÃªu cáº§u khÃ¡c nhau. Ká»¹ thuáº­t nÃ y cá»±c ká»³ hiá»‡u quáº£ trong cÃ¡c á»©ng dá»¥ng cÃ³ cáº¥u trÃºc prompt láº·p láº¡i, cháº³ng háº¡n nhÆ° chatbot vá»›i system prompt cá»‘ Ä‘á»‹nh hoáº·c cÃ¡c á»©ng dá»¥ng RAG (Retrieval-Augmented Generation).

> Theo tÃ i liá»‡u tá»« BentoML, Prefix Caching cÃ³ thá»ƒ giáº£m chi phÃ­ tÃ­nh toÃ¡n tá»›i 90% vÃ  Ä‘á»™ trá»… tá»›i 85% cho cÃ¡c yÃªu cáº§u cÃ³ prompt dÃ i vÃ  láº·p láº¡i [3]. Äá»ƒ tá»‘i Ä‘a hÃ³a hiá»‡u quáº£, cÃ¡c pháº§n tÄ©nh cá»§a prompt nÃªn Ä‘Æ°á»£c Ä‘áº·t á»Ÿ Ä‘áº§u, trong khi cÃ¡c pháº§n Ä‘á»™ng (dá»¯ liá»‡u ngÆ°á»i dÃ¹ng) nÃªn Ä‘Æ°á»£c Ä‘áº·t á»Ÿ cuá»‘i.

### 2.3. Batching vÃ  Scheduling: Continuous Batching vÃ  Chunked Prefill

**Continuous Batching** lÃ  má»™t cáº£i tiáº¿n so vá»›i static batching, cho phÃ©p thÃªm cÃ¡c yÃªu cáº§u má»›i vÃ o batch Ä‘ang xá»­ lÃ½ ngay khi cÃ³ tÃ i nguyÃªn GPU trá»‘ng, thay vÃ¬ pháº£i chá» toÃ n bá»™ batch hoÃ n thÃ nh. Äiá»u nÃ y giÃºp tÄƒng Ä‘Ã¡ng ká»ƒ GPU utilization vÃ  thÃ´ng lÆ°á»£ng.

Tuy nhiÃªn, quÃ¡ trÃ¬nh suy luáº­n LLM gá»“m hai giai Ä‘oáº¡n vá»›i Ä‘áº·c tÃ­nh khÃ¡c nhau: **prefill** (xá»­ lÃ½ prompt Ä‘áº§u vÃ o, tÃ­nh toÃ¡n song song vÃ  sá»­ dá»¥ng nhiá»u tÃ i nguyÃªn) vÃ  **decode** (sinh ra tá»«ng token má»™t, bá»‹ giá»›i háº¡n bá»Ÿi bÄƒng thÃ´ng bá»™ nhá»›). Sá»± khÃ¡c biá»‡t nÃ y gÃ¢y ra tÃ¬nh tráº¡ng "bong bÃ³ng" trong pipeline, lÃ m giáº£m hiá»‡u quáº£. **Chunked Prefill** giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch chia giai Ä‘oáº¡n prefill dÃ i thÃ nh cÃ¡c "chunk" nhá» hÆ¡n. CÃ¡c chunk nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c xáº¿p lá»‹ch (schedule) vÃ  xá»­ lÃ½ xen káº½ vá»›i cÃ¡c yÃªu cáº§u decode, giÃºp láº¥p Ä‘áº§y cÃ¡c khoáº£ng trá»‘ng tÃ i nguyÃªn vÃ  cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ thÃ´ng lÆ°á»£ng tá»•ng thá»ƒ [4].

### 2.4. CÃ¡c Chiáº¿n LÆ°á»£c Song Song HÃ³a (Parallelism)

Äá»ƒ cháº¡y cÃ¡c mÃ´ hÃ¬nh cÃ³ hÃ ng chá»¥c hoáº·c hÃ ng trÄƒm tá»· tham sá»‘, viá»‡c sá»­ dá»¥ng nhiá»u GPU lÃ  báº¯t buá»™c. **Tensor Parallelism (TP)** lÃ  má»™t ká»¹ thuáº­t phá»• biáº¿n, chia cÃ¡c phÃ©p nhÃ¢n ma tráº­n lá»›n trong mÃ´ hÃ¬nh ra nhiá»u GPU. Má»—i GPU chá»‰ giá»¯ má»™t pháº§n cá»§a trá»ng sá»‘ vÃ  thá»±c hiá»‡n má»™t pháº§n cá»§a phÃ©p tÃ­nh, sau Ä‘Ã³ káº¿t quáº£ Ä‘Æ°á»£c tá»•ng há»£p láº¡i. Máº·c dÃ¹ ká»¹ thuáº­t nÃ y cho phÃ©p cháº¡y cÃ¡c mÃ´ hÃ¬nh khá»•ng lá»“, nÃ³ cÅ©ng lÃ m tÄƒng chi phÃ­ giao tiáº¿p (communication overhead) giá»¯a cÃ¡c GPU [5]. Viá»‡c lá»±a chá»n `tensor-parallel-size` phÃ¹ há»£p lÃ  má»™t sá»± Ä‘Ã¡nh Ä‘á»•i giá»¯a dung lÆ°á»£ng bá»™ nhá»› trÃªn má»—i GPU vÃ  Ä‘á»™ trá»… do giao tiáº¿p.

### 2.5. CÃ¡c Chiáº¿n LÆ°á»£c Decode NÃ¢ng Cao: Speculative Decoding

**Speculative Decoding** lÃ  má»™t ká»¹ thuáº­t Ä‘á»™t phÃ¡ giÃºp giáº£m Ä‘á»™ trá»… báº±ng cÃ¡ch giáº£m sá»‘ lÆ°á»£ng forward pass cáº§n thiáº¿t Ä‘á»ƒ sinh ra má»™t chuá»—i token. Ã tÆ°á»Ÿng lÃ  sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh "nhÃ¡p" (draft model) nhá» vÃ  nhanh Ä‘á»ƒ sinh ra má»™t chuá»—i token á»©ng cá»­ viÃªn. Sau Ä‘Ã³, mÃ´ hÃ¬nh "chÃ­nh" (target model) lá»›n vÃ  chÃ­nh xÃ¡c sáº½ xÃ¡c thá»±c táº¥t cáº£ cÃ¡c token á»©ng cá»­ viÃªn nÃ y trong má»™t forward pass duy nháº¥t. CÃ¡c token Ä‘Æ°á»£c xÃ¡c thá»±c sáº½ Ä‘Æ°á»£c cháº¥p nháº­n, vÃ  quÃ¡ trÃ¬nh tiáº¿p tá»¥c tá»« token cuá»‘i cÃ¹ng Ä‘Æ°á»£c cháº¥p nháº­n. CÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° EAGLE-3 Ä‘Ã£ cáº£i tiáº¿n ká»¹ thuáº­t nÃ y báº±ng cÃ¡ch tÃ­ch há»£p cÆ¡ cháº¿ nhÃ¡p vÃ o chÃ­nh mÃ´ hÃ¬nh chÃ­nh, loáº¡i bá» nhu cáº§u vá» má»™t mÃ´ hÃ¬nh riÃªng biá»‡t [6].

### 2.6. Tá»‘i Æ¯u HÃ³a Cáº¥p Tháº¥p: FlashAttention vÃ  CUDA Graphs

- **FlashAttention**: LÃ  má»™t thuáº­t toÃ¡n attention Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a á»Ÿ cáº¥p Ä‘á»™ kernel, giÃºp giáº£m Ä‘Ã¡ng ká»ƒ viá»‡c Ä‘á»c/ghi vÃ o bá»™ nhá»› GPU, tá»« Ä‘Ã³ tÄƒng tá»‘c Ä‘á»™ vÃ  giáº£m bá»™ nhá»› cáº§n thiáº¿t cho quÃ¡ trÃ¬nh attention [7]. Háº§u háº¿t cÃ¡c framework serving hiá»‡n Ä‘áº¡i nhÆ° vLLM Ä‘á»u tÃ­ch há»£p sáºµn FlashAttention.
- **CUDA Graphs**: Cho phÃ©p ghi láº¡i má»™t chuá»—i cÃ¡c hoáº¡t Ä‘á»™ng trÃªn GPU vÃ  phÃ¡t láº¡i chÃºng mÃ  khÃ´ng cáº§n sá»± can thiá»‡p cá»§a CPU. Äiá»u nÃ y loáº¡i bá» overhead tá»« viá»‡c gá»i kernel láº·p Ä‘i láº·p láº¡i, Ä‘áº·c biá»‡t hiá»‡u quáº£ trong giai Ä‘oáº¡n decode khi cÃ¹ng má»™t kernel Ä‘Æ°á»£c thá»±c thi nhiá»u láº§n [8].

## 3. Triá»ƒn Khai Thá»±c Táº¿ vá»›i vLLM

vLLM lÃ  má»™t trong nhá»¯ng framework serving LLM phá»• biáº¿n nháº¥t hiá»‡n nay, tÃ­ch há»£p nhiá»u ká»¹ thuáº­t tá»‘i Æ°u hÃ³a Ä‘Ã£ Ä‘á» cáº­p. Viá»‡c tinh chá»‰nh cÃ¡c tham sá»‘ cá»§a vLLM lÃ  ráº¥t quan trá»ng Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘i Æ°u.

| Tham Sá»‘ vLLM | Chá»©c NÄƒng vÃ  TÃ¡c Äá»™ng | 
| :--- | :--- | 
| `--gpu-memory-utilization` | Thiáº¿t láº­p tá»· lá»‡ bá»™ nhá»› GPU dÃ nh cho KV Cache. TÄƒng giÃ¡ trá»‹ nÃ y cho phÃ©p xá»­ lÃ½ batch lá»›n hÆ¡n, nhÆ°ng cÃ³ thá»ƒ gÃ¢y lá»—i OOM. | 
| `--max-num-seqs` | Sá»‘ lÆ°á»£ng yÃªu cáº§u tá»‘i Ä‘a trong má»™t batch. TÄƒng giÃ¡ trá»‹ nÃ y giÃºp tÄƒng thÃ´ng lÆ°á»£ng nhÆ°ng cÅ©ng lÃ m tÄƒng Ä‘á»™ trá»…. | 
| `--max-model-len` | Äá»™ dÃ i chuá»—i tá»‘i Ä‘a (input + output). áº¢nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n yÃªu cáº§u bá»™ nhá»› cho KV Cache. | 
| `--enable-prefix-caching` | KÃ­ch hoáº¡t tÃ­nh nÄƒng Prefix Caching Ä‘á»ƒ tÄƒng tá»‘c cÃ¡c yÃªu cáº§u cÃ³ tiá»n tá»‘ chung. | 
| `--enable-chunked-prefill` | KÃ­ch hoáº¡t Chunked Prefill Ä‘á»ƒ cáº£i thiá»‡n thÃ´ng lÆ°á»£ng, Ä‘áº·c biá»‡t vá»›i cÃ¡c prompt dÃ i. | 
| `--tensor-parallel-size` | Sá»‘ lÆ°á»£ng GPU Ä‘Æ°á»£c sá»­ dá»¥ng cho Tensor Parallelism. | 
| `--quantization` | Chá»‰ Ä‘á»‹nh phÆ°Æ¡ng phÃ¡p lÆ°á»£ng tá»­ hÃ³a (vÃ­ dá»¥: `awq`, `gptq`) Ä‘á»ƒ cháº¡y cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c nÃ©n. | 

**PhÃ¢n tÃ­ch vÃ­ dá»¥ `SmolLM2-135M`**: Cáº¥u hÃ¬nh Ä‘Æ°á»£c cung cáº¥p trong tÃ i liá»‡u (`gpu-memory-utilization=0.3`, `max-num-seqs=512`) cho tháº¥y má»™t chiáº¿n lÆ°á»£c tá»‘i Æ°u hÃ³a cho mÃ´ hÃ¬nh ráº¥t nhá». VÃ¬ mÃ´ hÃ¬nh nháº¹, pháº§n lá»›n bá»™ nhá»› GPU cÃ³ thá»ƒ Ä‘Æ°á»£c dÃ nh cho KV Cache, cho phÃ©p xá»­ lÃ½ má»™t batch ráº¥t lá»›n (`512` sequences) Ä‘á»ƒ tá»‘i Ä‘a hÃ³a thÃ´ng lÆ°á»£ng, phÃ¹ há»£p cho cÃ¡c tÃ¡c vá»¥ phÃ¢n loáº¡i cáº£m xÃºc cÃ³ thá»ƒ xá»­ lÃ½ Ä‘á»“ng thá»i.

## 4. Quy TrÃ¬nh Tá»‘i Æ¯u HÃ³a Há»‡ Thá»‘ng

Má»™t quy trÃ¬nh tá»‘i Æ°u hÃ³a cÃ³ há»‡ thá»‘ng bao gá»“m cÃ¡c bÆ°á»›c sau:

1.  **PhÃ¢n tÃ­ch YÃªu cáº§u**: XÃ¡c Ä‘á»‹nh rÃµ cÃ¡c má»¥c tiÃªu vá» Ä‘á»™ trá»… (TTFT, ITL) vÃ  thÃ´ng lÆ°á»£ng (requests/giÃ¢y) cho á»©ng dá»¥ng cá»¥ thá»ƒ.
2.  **PhÃ¢n tÃ­ch Workload**: NghiÃªn cá»©u Ä‘áº·c Ä‘iá»ƒm cá»§a cÃ¡c yÃªu cáº§u (Ä‘á»™ dÃ i input/output trung bÃ¬nh vÃ  tá»‘i Ä‘a, tá»· lá»‡ láº·p láº¡i cá»§a prompt).
3.  **Lá»±a chá»n Pháº§n cá»©ng vÃ  MÃ´ hÃ¬nh**: Dá»±a trÃªn kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  yÃªu cáº§u hiá»‡u suáº¥t, chá»n GPU cÃ³ Ä‘á»§ bá»™ nhá»› (VRAM) vÃ  kháº£ nÄƒng tÃ­nh toÃ¡n. CÃ¢n nháº¯c sá»­ dá»¥ng cÃ¡c phiÃªn báº£n mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°á»£ng tá»­ hÃ³a.
4.  **Cáº¥u hÃ¬nh Ban Ä‘áº§u**: Báº¯t Ä‘áº§u vá»›i má»™t cáº¥u hÃ¬nh cÆ¡ báº£n, an toÃ n (vÃ­ dá»¥: `gpu-memory-utilization=0.8`, `max-num-seqs` á»Ÿ má»©c vá»«a pháº£i).
5.  **Benchmarking**: Sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ benchmark Ä‘á»ƒ Ä‘o lÆ°á»ng hiá»‡u suáº¥t cá»§a há»‡ thá»‘ng dÆ°á»›i cÃ¡c má»©c táº£i khÃ¡c nhau, ghi láº¡i cÃ¡c chá»‰ sá»‘ chÃ­nh.
6.  **Tinh chá»‰nh vÃ  Láº·p láº¡i**: Dá»±a trÃªn káº¿t quáº£ benchmark, tinh chá»‰nh cÃ¡c tham sá»‘ (vÃ­ dá»¥: tÄƒng `max-num-seqs` náº¿u GPU utilization cÃ²n tháº¥p, giáº£m náº¿u Ä‘á»™ trá»… quÃ¡ cao) vÃ  thá»±c hiá»‡n láº¡i benchmark cho Ä‘áº¿n khi Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu.

## 5. Káº¿t Luáº­n vÃ  Xu HÆ°á»›ng TÆ°Æ¡ng Lai

Tá»‘i Æ°u hÃ³a serving cho LLM lÃ  má»™t lÄ©nh vá»±c phá»©c táº¡p nhÆ°ng cá»±c ká»³ quan trá»ng, Ä‘Ã²i há»i sá»± hiá»ƒu biáº¿t sÃ¢u sáº¯c vá» cáº£ mÃ´ hÃ¬nh, pháº§n cá»©ng vÃ  cÃ¡c thuáº­t toÃ¡n há»‡ thá»‘ng. Báº±ng cÃ¡ch Ã¡p dá»¥ng má»™t cÃ¡ch cÃ³ há»‡ thá»‘ng cÃ¡c ká»¹ thuáº­t nhÆ° lÆ°á»£ng tá»­ hÃ³a, prefix caching, chunked prefill, vÃ  speculative decoding, cÃ¡c tá»• chá»©c cÃ³ thá»ƒ giáº£m Ä‘Ã¡ng ká»ƒ chi phÃ­ váº­n hÃ nh vÃ  cáº£i thiá»‡n tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng. CÃ¡c framework nhÆ° vLLM Ä‘Ã£ dÃ¢n chá»§ hÃ³a viá»‡c tiáº¿p cáº­n cÃ¡c ká»¹ thuáº­t nÃ y, nhÆ°ng viá»‡c tinh chá»‰nh chuyÃªn sÃ¢u váº«n lÃ  yáº¿u tá»‘ quyáº¿t Ä‘á»‹nh Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t Ä‘á»‰nh cao. 

Trong tÆ°Æ¡ng lai, cÃ¡c xu hÆ°á»›ng nhÆ° serving phÃ¢n tÃ¡n (distributed serving) vá»›i cÆ¡ cháº¿ Ä‘á»‹nh tuyáº¿n thÃ´ng minh dá»±a trÃªn prefix (prefix-aware routing) vÃ  sá»­ dá»¥ng cÃ¡c táº§ng bá»™ nhá»› khÃ¡c nhau (tiered KV cache) Ä‘á»ƒ má»Ÿ rá»™ng dung lÆ°á»£ng cache há»©a háº¹n sáº½ tiáº¿p tá»¥c Ä‘áº©y xa hÆ¡n ná»¯a giá»›i háº¡n vá» hiá»‡u suáº¥t vÃ  hiá»‡u quáº£ cá»§a cÃ¡c há»‡ thá»‘ng phá»¥c vá»¥ LLM.

---

### TÃ i liá»‡u tham kháº£o

[1] Kim, D. (2024). *A Deep Dive into GPTQ and AWQ Quantization*. Medium. Truy cáº­p táº¡i: https://medium.com/@kimdoil1211/speeding-up-large-language-models-a-deep-dive-into-gptq-and-awq-quantization-0bb001eaabd4

[2] vLLM Project. *Automatic Prefix Caching*. vLLM Documentation. Truy cáº­p táº¡i: https://docs.vllm.ai/en/stable/design/prefix_caching.html

[3] BentoML. *Prefix caching | LLM Inference Handbook*. BentoML. Truy cáº­p táº¡i: https://bentoml.com/llm/inference-optimization/prefix-caching

[4] Moon, D. (2024). *LLM Inference Optimizations - Chunked Prefills and Decode Maximal Batching*. Medium. Truy cáº­p táº¡i: https://donmoon.medium.com/llm-inference-optimizations-2-chunked-prefill-764407b3a67a

[5] Hanley, E., & Rockwell, B. (2025). *vLLM Performance Tuning: The Ultimate Guide to xPU Inference Configuration*. Google Cloud Blog. Truy cáº­p táº¡i: https://cloud.google.com/blog/topics/developers-practitioners/vllm-performance-tuning-the-ultimate-guide-to-xpu-inference-configuration

[6] Li, J., Yu, C., & Guo, H. (2025). *An Introduction to Speculative Decoding for Reducing Latency in AI Inference*. NVIDIA Technical Blog. Truy cáº­p táº¡i: https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/

[7] Dao, T. (2024). *FlashAttention-3: Fast and Accurate Attention with Fused Heads*. Tri Dao's Blog. Truy cáº­p táº¡i: https://tridao.me/blog/2024/flash3/

[8] vLLM Project. *CUDA Graphs*. vLLM Documentation. Truy cáº­p táº¡i: https://docs.vllm.ai/en/stable/design/cuda_graphs/

---

## ğŸ’¡ TÃ³m táº¯t chiáº¿n thuáº­t cho Pika:

1. **Má»Ÿ nhiá»u lÃ n (`max-num-seqs` cao):** VÃ¬ Pika lÃ  model tÃ­ hon, hÃ£y táº­n dá»¥ng Ä‘á»ƒ phá»¥c vá»¥ cáº£ lá»›p há»c cÃ¹ng lÃºc.
    
2. **Äá»«ng xÃ¢y bÄƒng chuyá»n quÃ¡ dÃ i (`max-model-len` vá»«a Ä‘á»§):** Pika chá»‰ chat ngáº¯n, Ä‘á»«ng lÃ£ng phÃ­ tÃ i nguyÃªn cho nhá»¯ng bÄƒng chuyá»n dÃ i lÃª thÃª.
    
3. **LuÃ´n báº­t cháº¿ Ä‘á»™ "Copy bÃ i máº«u" (`prefix-caching`):** ÄÃ¢y lÃ  bÃ­ máº­t Ä‘á»ƒ Pika pháº£n há»“i nhanh nhÆ° Ä‘iá»‡n khi cÃ³ nhiá»u ngÆ°á»i há»i cÃ¹ng lÃºc.
    
4. **Äá»ƒ thá»£ tá»± lÃ m (`enforce-eager = False`):** Äá»«ng báº¯t CPU quáº£n lÃ½ vi mÃ´, hÃ£y Ä‘á»ƒ GPU tá»± do cháº¡y háº¿t tá»‘c lá»±c (CUDA Graphs).


## 3.1 VÃ­ dá»¥ serving HuggingFaceTB/SmolLM2-135M-Instruct


```

#!/bin/bash

# ============================================================

# RUN SMOLLM2-135M EMOTION CLASSIFIER - Ultra Fast Tiny Model

# Target: GPU 0 (~12GB free VRAM - nhÆ°ng model nÃ y chá»‰ cáº§n ~300MB!)

# Dá»± kiáº¿n latency: < 10ms (nhanh hÆ¡n Phi-3 gáº¥p 3-5 láº§n)

# ============================================================

  

set -e

  

# Colors

GREEN='\033[0;32m'

YELLOW='\033[1;33m'

RED='\033[0;31m'

NC='\033[0m'

  

echo -e "${GREEN}============================================${NC}"

echo -e "${GREEN} Â SMOLLM2-135M EMOTION CLASSIFIER (ULTRA) Â  ${NC}"

echo -e "${GREEN}============================================${NC}"

  

# Configuration

VENV_DIR="$HOME/venv_phi3" Â # TÃ¡i sá»­ dá»¥ng venv cÅ©

GPU_ID=0

PORT=30030

MODEL_NAME="HuggingFaceTB/SmolLM2-135M-Instruct" Â # âš¡ TINY MODEL

GPU_MEMORY_UTIL=0.15 Â # Chá»‰ cáº§n 15% VRAM (~1.8GB)

MAX_MODEL_LEN=2048

MAX_NUM_SEQS=128 Â  Â  Â # TÄƒng batch size vÃ¬ model nhá»

  
  

# Step 1: Check GPU

echo -e "\n${YELLOW}[1/3] Checking GPU ${GPU_ID} availability...${NC}"

FREE_MEM=$(nvidia-smi -i $GPU_ID --query-gpu=memory.free --format=csv,noheader,nounits)

echo "GPU ${GPU_ID} Free Memory: ${FREE_MEM} MiB"

  

if [ "$FREE_MEM" -lt 2000 ]; then

Â  Â  echo -e "${RED}WARNING: GPU ${GPU_ID} has less than 2GB free!${NC}"

Â  Â  echo "SmolLM2-135M only needs ~300MB, but 2GB is recommended for overhead."

fi

  
  

# Step 2: Activate venv

echo -e "\n${YELLOW}[2/3] Activating Python environment...${NC}"

  

if [ ! -d "$VENV_DIR" ]; then

Â  Â  echo -e "${RED}ERROR: Virtual environment not found at $VENV_DIR${NC}"

Â  Â  echo "Creating new venv..."

Â  Â  python3.11 -m venv "$VENV_DIR"

Â  Â  source "$VENV_DIR/bin/activate"

Â  Â  echo "Installing vLLM..."

Â  Â  pip install --upgrade pip

Â  Â  pip install vllm==0.12.0 torch==2.5.1

else

Â  Â  source "$VENV_DIR/bin/activate"

Â  Â  echo "âœ“ Activated venv: $VENV_DIR"

fi

  

# Verify

if ! python -c "import vllm" 2>/dev/null; then

Â  Â  echo -e "${RED}ERROR: vLLM not found!${NC}"

Â  Â  exit 1

fi

  

VLLM_VERSION=$(python -c "import vllm; print(vllm.__version__)" 2>/dev/null)

echo "âœ“ vLLM version: $VLLM_VERSION"

  
  

# Step 3: Launch server

echo -e "\n${YELLOW}[3/3] Launching vLLM server...${NC}"

echo -e "${GREEN}============================================${NC}"

echo "Model: ${MODEL_NAME} (135M params)"

echo "GPU: ${GPU_ID}"

echo "Port: ${PORT}"

echo "Memory Utilization: ${GPU_MEMORY_UTIL} (~300MB only)"

echo "Expected Latency: < 10ms per request"

echo -e "${GREEN}============================================${NC}"

echo ""

echo "ğŸš€ ULTRA-FAST MODE: SmolLM2 is 3.7x smaller than Qwen-0.5B"

echo "Server starting... Press Ctrl+C to stop"

echo ""

  

# Log file

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

LOG_FILE="$SCRIPT_DIR/smollm2_server.log"

  

# Launch vLLM vá»›i SmolLM2-135M

# NOTE: Model nÃ y khÃ´ng cÃ³ sáºµn AWQ version, nhÆ°ng FP16 Ä‘Ã£ Ä‘á»§ nhanh!

CUDA_VISIBLE_DEVICES=$GPU_ID python -m vllm.entrypoints.openai.api_server \

Â  Â  --model "$MODEL_NAME" \

Â  Â  --dtype float16 \

Â  Â  --host 0.0.0.0 \

Â  Â  --port $PORT \

Â  Â  --gpu-memory-utilization $GPU_MEMORY_UTIL \

Â  Â  --max-model-len $MAX_MODEL_LEN \

Â  Â  --max-num-seqs $MAX_NUM_SEQS \

Â  Â  --enable-prefix-caching \

Â  Â  --trust-remote-code \

Â  Â  --disable-log-requests \

Â  Â  2>&1 | tee "$LOG_FILE"

  
  

# ALTERNATIVE: Náº¿u muá»‘n quantize thá»§ cÃ´ng Ä‘á»ƒ nhanh hÆ¡n ná»¯a

# Uncomment dÃ²ng dÆ°á»›i vÃ  comment block trÃªn

# CUDA_VISIBLE_DEVICES=$GPU_ID python -m vllm.entrypoints.openai.api_server \

# Â  Â  --model "prithivMLmods/SmolLM2-135M-GGUF" \

# Â  Â  --quantization gguf \

# Â  Â  --dtype auto \

# Â  Â  --host 0.0.0.0 \

# Â  Â  --port $PORT \

# Â  Â  --gpu-memory-utilization $GPU_MEMORY_UTIL \

# Â  Â  --max-model-len $MAX_MODEL_LEN \

# Â  Â  --max-num-seqs $MAX_NUM_SEQS \

# Â  Â  --enable-prefix-caching \

# Â  Â  --trust-remote-code \

# Â  Â  2>&1 | tee "$LOG_FILE"

  

```

  
  

```

curl --location 'http://103.253.20.30:30030/v1/chat/completions' \

--header 'Content-Type: application/json' \

--data '{

Â  Â  "model": "HuggingFaceTB/SmolLM2-135M-Instruct",

Â  Â  "messages": Â [

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  "role": "system",

Â  Â  Â  Â  Â  Â  "content": "You are now intention detection. Given user'\''s input, detect the suitable emotion and the need of celebrate for it.\nUser input in format\nprevious Question: string\nprevious Answer: string\nResponse to check: string to check\n\nYou will extract the '\''Response to check'\'' and check:\n\n1. For emotion, pick from the list below:\n- happy, happy_2: when intention about happiness\n- calm: when intentionis to comfort\n- excited, excited_2: when expressing the exciting emotion\n- playful,playful_2,playful_3: intention about playing some fun activity\n- no_problem: intention when telling something is fine\n- encouraging,encouraging_2: intention when tell to try to do something\n- curious: intention when show curiousity\n- surprised: when being shocked or surprised\n- proud,proud_2: when showing proud\n- thats_right, thats_right_2: when telling something is correct\n- sad: when sad\n- angry: showing anger\n- worry: when show worriness\n- afraid: when feel scared\n- noisy: When intention about can'\''t hear properly\n- thinking: intention about thinking\n\n2. For learn_score, if related to english learning\n- true: if question is about english learning, repeating something, and answer correctly\n- false: for other case include positive and negative\n\n\n return in single line json format.\n{\"emotion\":\"<emotion name>\",\"learn_score\":true/flase}"

Â  Â  Â  Â  },

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  "role": "user",

Â  Â  Â  Â  Â  Â  "content": "Previous Question: Tá»› buá»“n quÃ¡. \nPrevious Answer: i think a yummy\nResponse to check: Nghe vui quÃ¡! Bá»ƒ Háº£, cáº­u cÃ³ muá»‘n chÆ¡i trÃ² ká»ƒ tÃªn cÃ¡c loáº¡i trÃ¡i cÃ¢y báº±ng tiáº¿ng Anh khÃ´ng? Name fruits in English!"

Â  Â  Â  Â  }

Â  Â  ],

Â  Â  "temperature": 0,

Â  Â  "max_tokens": 50

Â  }'

```

  
  

```

{

Â  Â  "id": "chatcmpl-8ca7b299c9702fde",

Â  Â  "object": "chat.completion",

Â  Â  "created": 1765172321,

Â  Â  "model": "HuggingFaceTB/SmolLM2-135M-Instruct",

Â  Â  "choices": [

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  "index": 0,

Â  Â  Â  Â  Â  Â  "message": {

Â  Â  Â  Â  Â  Â  Â  Â  "role": "assistant",

Â  Â  Â  Â  Â  Â  Â  Â  "content": "\"Tá»› buá»“n quÃ¡. Nghe vui quÃ¡!\" Bá»ƒ Háº£, cáº­u cÃ³ muá»‘n chÆ¡i trÃ² ká»ƒ tÃªn cÃ¡c",

Â  Â  Â  Â  Â  Â  Â  Â  "refusal": null,

Â  Â  Â  Â  Â  Â  Â  Â  "annotations": null,

Â  Â  Â  Â  Â  Â  Â  Â  "audio": null,

Â  Â  Â  Â  Â  Â  Â  Â  "function_call": null,

Â  Â  Â  Â  Â  Â  Â  Â  "tool_calls": [],

Â  Â  Â  Â  Â  Â  Â  Â  "reasoning": null,

Â  Â  Â  Â  Â  Â  Â  Â  "reasoning_content": null

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  "logprobs": null,

Â  Â  Â  Â  Â  Â  "finish_reason": "length",

Â  Â  Â  Â  Â  Â  "stop_reason": null,

Â  Â  Â  Â  Â  Â  "token_ids": null

Â  Â  Â  Â  }

Â  Â  ],

Â  Â  "service_tier": null,

Â  Â  "system_fingerprint": null,

Â  Â  "usage": {

Â  Â  Â  Â  "prompt_tokens": 428,

Â  Â  Â  Â  "total_tokens": 478,

Â  Â  Â  Â  "completion_tokens": 50,

Â  Â  Â  Â  "prompt_tokens_details": null

Â  Â  },

Â  Â  "prompt_logprobs": null,

Â  Â  "prompt_token_ids": null,

Â  Â  "kv_transfer_params": null

}

```

  

Ket qua 170ms ()

  

Tháº¥p hÆ¡n qwen0.6B = 50-80ms tren H100, tren cung server lÃ  150ms

va thap hon Groq oss-20b/120b gpt mÃ  cÃ³ 150ms

