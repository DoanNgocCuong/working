# üöÄ T·ªêI ∆ØU OUTPUT FORMAT CHO LLM: MECE ANALYSIS

ƒê√¢y l√† ph√¢n t√≠ch **MECE (Mutually Exclusive, Collectively Exhaustive)** to√†n di·ªán ƒë·ªÉ b·∫°n ch·ªçn format t·ªëi ∆∞u nh·∫•t cho Pika.

---

## üìä B·∫¢NG T·ªîNG H·ª¢P: Format N√†o Nhanh Nh·∫•t?

|Rank|Output Format|T·ªëc ƒë·ªô (Speed)|Token Efficiency|ƒê·ªô Tin C·∫≠y (Reliability)|Parsing|**Response Time Impact**|
|---|---|---|---|---|---|---|
|**1**|**YAML**|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê|‚≠ê‚≠ê‚≠ê‚≠ê|D·ªÖ|‚¨áÔ∏è **Gi·∫£m 30-40%** (Do √≠t token nh·∫•t)|
|**2**|**Plain Text (Delimited)**|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê|‚≠ê‚≠ê|Kh√≥|‚¨áÔ∏è **Gi·∫£m 35-45%** (Nhanh nh·∫•t nh∆∞ng kh√≥ parse)|
|**3**|**Minified JSON**|‚≠ê‚≠ê‚≠ê|‚≠ê‚≠ê‚≠ê|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê|R·∫•t D·ªÖ|‚¨áÔ∏è **Gi·∫£m 10-20%** (Chu·∫©n nh∆∞ng nhi·ªÅu token th·ª´a)|
|**4**|**Standard JSON**|‚≠ê‚≠ê|‚≠ê‚≠ê|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê|R·∫•t D·ªÖ|‚ûñ **Baseline** (Nhi·ªÅu d·∫•u `{`, `"` t·ªën token)|
|**5**|**XML / HTML**|‚≠ê|‚≠ê|‚≠ê‚≠ê‚≠ê|Trung b√¨nh|‚¨ÜÔ∏è **TƒÉng 10-20%** (R·∫•t verbosely)|

---

## 1Ô∏è‚É£ PH√ÇN T√çCH CHI TI·∫æT T·ª™NG FORMAT

## **Option A: JSON (Standard & Minified)**

> **Ph·ªï bi·∫øn nh·∫•t, nh∆∞ng kh√¥ng hi·ªáu qu·∫£ nh·∫•t.**

- **C·∫•u tr√∫c:** `{"key": "value", "list": [1, 2]}`
    
- **V·∫•n ƒë·ªÅ:** R·∫•t nhi·ªÅu k√Ω t·ª± c√∫ ph√°p (`"`, `:`, `,`, `{`, `}`) ‚Üí T·ªën nhi·ªÅu token.
    
- **Minified JSON:** `{"k":"v"}` ‚Üí Gi·∫£m space nh∆∞ng v·∫´n t·ªën k√Ω t·ª± c√∫ ph√°p.
    
- **Hi·ªáu nƒÉng:**
    
    - Token count: **Cao** (100%)
        
    - Parsing overhead: **Th·∫•p** (native support m·ªçi ng√¥n ng·ªØ)
        
    - **Khi n√†o d√πng:** Khi c·∫ßn ƒë·ªô tin c·∫≠y tuy·ªát ƒë·ªëi (structured output mode).
        

## **Option B: YAML (Recommended for Performance)**

> **C√¢n b·∫±ng ho√†n h·∫£o gi·ªØa Token Efficiency v√† Readability.**

- **C·∫•u tr√∫c:**
    
    text
    
    `emotion: happy celebrate: yes`
    
- **L·ª£i th·∫ø:** √çt k√Ω t·ª± th·ª´a (kh√¥ng ngo·∫∑c, kh√¥ng d·∫•u ph·∫©y). Tokenizer x·ª≠ l√Ω YAML r·∫•t t·ªët (g·ªôp `key:` th√†nh 1 token).
    
- **Hi·ªáu nƒÉng:**
    
    - Token count: **Th·∫•p** (60-70% so v·ªõi JSON)
        
    - **Response Time:** **Nhanh h∆°n 30%** do sinh √≠t token h∆°n.
        
    - Parsing: D·ªÖ (`pyyaml` ho·∫∑c `js-yaml`).
        

## **Option C: Plain Text / Delimited (Ultimate Speed)**

> **T·ªëc ƒë·ªô t·ªëi ƒëa, nh∆∞ng r·ªßi ro cao.**

- **C·∫•u tr√∫c:** `happy|yes` ho·∫∑c `happy`
    
- **L·ª£i th·∫ø:** Kh√¥ng c√≥ c√∫ ph√°p th·ª´a. Ch·ªâ ƒë√∫ng d·ªØ li·ªáu c·∫ßn thi·∫øt.
    
- **Hi·ªáu nƒÉng:**
    
    - Token count: **C·ª±c th·∫•p** (40-50% so v·ªõi JSON).
        
    - **Response Time:** **Nhanh nh·∫•t**.
        
    - **R·ªßi ro:** N·∫øu model tr·∫£ l·ªùi th√™m l·ªùi d·∫´n ("Here is the output: happy") ‚Üí Parsing fail.
        

## **Option D: Protobuf / Custom Binary**

> **Kh√¥ng kh·∫£ thi cho LLM.**

- LLM l√† m√¥ h√¨nh sinh **text**, kh√¥ng ph·∫£i sinh binary. Vi·ªác b·∫Øt LLM sinh hex code c·ªßa protobuf l√† **c·ª±c ch·∫≠m v√† sai s√≥t**. B·ªè qua.
    

---

## 2Ô∏è‚É£ CASE STUDY: PIKA EMOTION CLASSIFICATION

Gi·∫£ s·ª≠ input: "Th·ªß ƒë√¥ VN l√† HN" ‚Üí Output: `emotion=proud`, `celebrate=yes`

## **So s√°nh Output Token Generation:**

## **1. Standard JSON (30 tokens)**

json

`{   "emotion": "proud",  "celebrate": "yes" }`

_T·ªën token cho kho·∫£ng tr·∫Øng v√† xu·ªëng d√≤ng._

## **2. Minified JSON (18 tokens)**

json

`{"emotion":"proud","celebrate":"yes"}`

_T·ªën token cho `"` v√† `{}`._

## **3. YAML (12 tokens) ‚≠ê**

text

`emotion: proud celebrate: yes`

_R·∫•t s·∫°ch, √≠t token._

## **4. Plain Text (Delimited) (5 tokens) ‚≠ê‚≠ê**

text

`proud|yes`

_Si√™u ng·∫Øn!_

---

## üéØ ƒê·ªÄ XU·∫§T T·ªêI ∆ØU CHO PIKA

## **Chi·∫øn l∆∞·ª£c 1: An to√†n & Nhanh (Khuy√™n d√πng)**

üëâ **Output Format: Minified JSON v·ªõi `stop` tokens**

- L√Ω do: Code d·ªÖ maintain, kh√¥ng c·∫ßn th∆∞ vi·ªán YAML, response time v·∫´n r·∫•t t·ªët n·∫øu prompt t·ªët.
    
- Prompt: `Output JSON: {"e":"<tag>","c":"y|n"}` (D√πng key ng·∫Øn `e`, `c` ƒë·ªÉ ti·∫øt ki·ªám token!)
    

**Optimized Output:** `{"e":"proud","c":"y"}` (ch·ªâ 10 tokens!)

## **Chi·∫øn l∆∞·ª£c 2: T·ªëc ƒë·ªô t·ªëi ƒëa (High Performance)**

üëâ **Output Format: Plain Text v·ªõi Separator**

- Prompt: `Output format: emotion|celebrate (e.g., happy|yes). No JSON.`
    
- Output th·ª±c t·∫ø: `proud|yes`
    
- Parsing logic: `emotion, celebrate = output.split("|")`
    

**K·∫øt qu·∫£:**

- Response time gi·∫£m **50%** so v·ªõi JSON g·ªëc.
    
- Token usage gi·∫£m **60%**.
    

---

## üöÄ CODE TRI·ªÇN KHAI (Chi·∫øn l∆∞·ª£c 2 - Plain Text)

python

`# System Prompt t·ªëi ∆∞u SYSTEM_PROMPT = """Classify emotion. Output format: <emotion>|<celebrate_yes_no> Tags: happy, proud, sad... Example: happy|no """ # User Prompt USER_PROMPT = """U: "Th·ªß ƒë√¥ VN l√† HN" P: "ƒê√∫ng!" Output:""" # Call LLM response = client.chat.completions.create(     model="openai/gpt-oss-20b",    messages=[...],    max_tokens=10,  # üî• Ch·ªâ c·∫ßn 10 tokens!    stop=["\n"],    # üî• D·ª´ng ngay khi xu·ªëng d√≤ng    temperature=0 ) # Parsing si√™u ƒë∆°n gi·∫£n raw = response.choices[0].message.content.strip() if "|" in raw:     emotion, celebrate = raw.split("|") else:     emotion = raw    celebrate = "no"`

## **K·∫øt lu·∫≠n:**

- N·∫øu b·∫°n mu·ªën **nhanh nh·∫•t**: Ch·ªçn **Plain Text (`proud|yes`)**.
    
- N·∫øu b·∫°n mu·ªën **c√¢n b·∫±ng & d·ªÖ debug**: Ch·ªçn **Minified JSON v·ªõi key ng·∫Øn (`{"e":"proud"}`)**.
    
- **ƒê·ª´ng d√πng Standard JSON** (format ƒë·∫πp) trong production API v√¨ l√£ng ph√≠ token v√† latency.
    

1. [https://www.gravitee.io/blog/protobuf-vs-json](https://www.gravitee.io/blog/protobuf-vs-json)
2. [https://dev.to/devflex-pro/json-vs-messagepack-vs-protobuf-in-go-my-real-benchmarks-and-what-they-mean-in-production-48fh](https://dev.to/devflex-pro/json-vs-messagepack-vs-protobuf-in-go-my-real-benchmarks-and-what-they-mean-in-production-48fh)
3. [https://latitude-blog.ghost.io/blog/serialization-protocols-for-low-latency-ai-applications/](https://latitude-blog.ghost.io/blog/serialization-protocols-for-low-latency-ai-applications/)
4. [https://auth0.com/blog/beating-json-performance-with-protobuf/](https://auth0.com/blog/beating-json-performance-with-protobuf/)
5. [https://github.com/inomera/proto-json-benchmark](https://github.com/inomera/proto-json-benchmark)
6. [https://dylancastillo.co/posts/say-what-you-mean-sometimes.html](https://dylancastillo.co/posts/say-what-you-mean-sometimes.html)
7. [https://blog.tashif.codes/blog/JSON-YAML-LLM](https://blog.tashif.codes/blog/JSON-YAML-LLM)
8. [https://www.reddit.com/r/PromptEngineering/comments/1mb80ra/whats_the_best_format_to_pass_data_to_an_llm_for/](https://www.reddit.com/r/PromptEngineering/comments/1mb80ra/whats_the_best_format_to_pass_data_to_an_llm_for/)
9. [https://www.aihero.dev/workshops/ai-sdk-v5-crash-course/data-represented-as-tokens-crjhu](https://www.aihero.dev/workshops/ai-sdk-v5-crash-course/data-represented-as-tokens-crjhu)
10. [https://www.linkedin.com/posts/ajay-krishna-36193018b_professionalcommunication-jobverification-activity-7395316894146191361-Jx9f](https://www.linkedin.com/posts/ajay-krishna-36193018b_professionalcommunication-jobverification-activity-7395316894146191361-Jx9f)
11. [https://dev.to/shrsv/taming-llms-how-to-get-structured-output-every-time-even-for-big-responses-445c](https://dev.to/shrsv/taming-llms-how-to-get-structured-output-every-time-even-for-big-responses-445c)
12. [https://www.linkedin.com/pulse/yaml-vs-json-why-wins-large-language-model-outputs-luciano-ayres-5kqif](https://www.linkedin.com/pulse/yaml-vs-json-why-wins-large-language-model-outputs-luciano-ayres-5kqif)
13. [https://www.reddit.com/r/MachineLearning/comments/18f7w2f/d_is_there_other_better_data_format_for_llm_to/](https://www.reddit.com/r/MachineLearning/comments/18f7w2f/d_is_there_other_better_data_format_for_llm_to/)
14. [https://winder.ai/calculating-token-counts-llm-context-windows-practical-guide/](https://winder.ai/calculating-token-counts-llm-context-windows-practical-guide/)
15. [https://www.reddit.com/r/csharp/comments/s6fide/protobuf_vs_json_performance_benchmarks_in_c/](https://www.reddit.com/r/csharp/comments/s6fide/protobuf_vs_json_performance_benchmarks_in_c/)
16. [https://www.reddit.com/r/LocalLLaMA/comments/1hcj0ur/structured_outputs_can_hurt_the_performance_of/](https://www.reddit.com/r/LocalLLaMA/comments/1hcj0ur/structured_outputs_can_hurt_the_performance_of/)
17. [https://betterprogramming.pub/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df](https://betterprogramming.pub/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df)
18. [https://apxml.com/courses/langchain-production-llm/chapter-1-advanced-langchain-architecture/advanced-output-parsing](https://apxml.com/courses/langchain-production-llm/chapter-1-advanced-langchain-architecture/advanced-output-parsing)
19. [https://www.toon-kit.com/playground](https://www.toon-kit.com/playground)
20. [https://vatsalshah.in/blog/toon-token-oriented-object-notation-guide](https://vatsalshah.in/blog/toon-token-oriented-object-notation-guide)