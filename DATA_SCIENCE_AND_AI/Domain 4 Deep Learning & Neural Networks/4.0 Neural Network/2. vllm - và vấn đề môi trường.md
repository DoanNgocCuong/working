```
Traceback (most recent call last):

  File "/home/ubuntu/.local/bin/vllm", line 3, in <module>

    from vllm.scripts import main

  File "/home/ubuntu/.local/lib/python3.11/site-packages/vllm/__init__.py", line 3, in <module>

    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs

  File "/home/ubuntu/.local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 8, in <module>

    import torch

  File "/home/ubuntu/.local/lib/python3.11/site-packages/torch/__init__.py", line 409, in <module>

    from torch._C import *  # noqa: F403

    ^^^^^^^^^^^^^^^^^^^^^^

ImportError: /home/ubuntu/.local/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: cuptiActivityEnableDriverApi, version libcupti.so.12
```


## Fix cho CUDA 12.2

- I ran these commands (in order). You can copy-run them.

```bash
# 1) Activate venv and audit
source /home/ubuntu/fintech/OCR/.venv311/bin/activate
python -V && which python && which vllm || echo no-vllm
pip show torch vllm || true
```

```bash
# 2) Remove incompatible stack (CUDA 12.6 wheels)
pip uninstall -y vllm torch torchvision torchaudio
```

```bash
# 3) Try install CUDA 12.1 PyTorch stack (ok) and vLLM cu121 (not available)
pip install --index-url https://download.pytorch.org/whl/cu121 \
  torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121
pip install vllm-cu121==0.6.3.post1 || true
```

```bash
# 4) Install vLLM (pulls own deps, wanted torch==2.4.0)
pip install vllm==0.6.3.post1 || true
```

```bash
# 5) Fix numpy metadata/conflict, pin <2
pip uninstall -y numpy || true
rm -rf /home/ubuntu/fintech/OCR/.venv311/lib/python3.11/site-packages/numpy* \
       /home/ubuntu/fintech/OCR/.venv311/lib/python3.11/site-packages/*~umpy*
pip install "numpy<2"
```

```bash
# 6) Re-attempt vLLM install
pip install --no-cache-dir vllm==0.6.3.post1 || true
```

```bash
# 7) Inspect environment
pip list --format=columns
pip freeze
```

```bash
# 8) Remove mismatched torchaudio, ensure vLLM entrypoint exists
pip uninstall -y torchaudio
pip install --no-deps vllm==0.6.3.post1 || true
which vllm
python -c "import torch, vllm; print(torch.__version__, vllm.__version__)"
```

```bash
# 9) Fix missing lzma support for torchvision import path
pip install backports.lzma
python -c "import lzma; print('lzma ok')"
```

- Start server (using venv binary)
```bash
/home/ubuntu/fintech/OCR/.venv311/bin/vllm serve rednote-hilab/dots.ocr \
  --trust-remote-code \
  --gpu-memory-utilization 0.60 \
  --host 0.0.0.0 \
  --port 30010 \
  --max-num-batched-tokens 8192 \
  --max-num-seqs 8 \
  > /home/ubuntu/fintech/OCR/vllm.log 2>&1 &
sleep 2 && tail -n 200 /home/ubuntu/fintech/OCR/vllm.log
```

Note: `rednote-hilab/dots.ocr` is not supported by vLLM. If it errors, switch model (e.g., `Qwen/Qwen2-VL-7B-Instruct`) or serve DotsOCR via Transformers.