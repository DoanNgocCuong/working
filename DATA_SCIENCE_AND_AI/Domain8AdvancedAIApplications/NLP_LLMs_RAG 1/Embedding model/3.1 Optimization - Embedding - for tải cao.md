<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# t·∫°o b·∫£ng MECE c√°c y·∫øu t·ªë ·∫£nh h∆∞·ªüng v√† vi·ªác b·∫°n t·ªëi ∆∞u n√≥

ƒê√¢y l√† **b·∫£ng MECE ƒë·∫ßy ƒë·ªß** ph√¢n t√≠ch c√°c y·∫øu t·ªë ·∫£nh h∆∞·ªüng ƒë·∫øn latency embedding service v√† c√°ch t·ªëi ∆∞u t·ª´ng layer:

***

## üìä MECE Framework: Embedding Optimization

### **I. MODEL INFERENCE** (L·ªõp x·ª≠ l√Ω model)

| Component | Current | Bottleneck | Optimization | Impact | Effort | Priority | Source |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| **Batch Size** | 32 | GPU underutilized | ‚Üí 64-96 | **‚àí100ms** | 5 min | ‚ö° \#1 | [^1][^2] |
| **Precision** | FP32/FP16 | Slow compute | ‚Üí FP16/ONNX | ‚àí30-40ms | 30 min | Medium | [^3][^4] |
| **Dimensionality** | 1024 | High memory BW | ‚Üí 512 dims | **‚àí50-100ms** | 10 min | ‚ö° \#2 | [^5][^6] |
| **Engine** | PyTorch | Unoptimized ops | ‚Üí ONNX/TensorRT | ‚àí15-40ms | 30 min | Medium | [^7][^8] |


***

### **II. COMPUTE RESOURCES** (L·ªõp t√†i nguy√™n container)

| Component | Current | Bottleneck | Optimization | Impact | Effort | Priority | Source |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| **CPU** | 2 vCPU | Tokenization 100% | ‚Üí 4 vCPU | **‚àí30-50ms** | 5 min | ‚ö° \#3 | [^9][^10] |
| **Memory** | 4GB | Insufficient buffer | ‚Üí 8GB | Enable batch 96+ | 5 min | Medium | [^1] |
| **GPU Util** | ~60-70% | Idle waiting CPU | ‚Üí Device mgmt + TF32 | +15% util | 10 min | Low | [^7] |


***

### **III. REQUEST PIPELINE** (L·ªõp x·ª≠ l√Ω request)

| Component | Current | Bottleneck | Optimization | Impact | Effort | Priority | Source |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| **Tokenization** | Single-thread | 90% CPU overhead | ‚Üí Multi-worker | ‚àí20-30ms | 15 min | High | [^10][^11] |
| **HTTP** | New TCP/req | Connection overhead | ‚Üí Keepalive + pool | **‚àí10-20ms** | 15 min | High | [^12][^13] |
| **Buffering** | 4k buffer | Small causes blocking | ‚Üí 8k √ó 16 buffers | ‚àí5-10ms | 5 min | Low | [^14] |


***

### **IV. LOAD DISTRIBUTION** (L·ªõp ph√¢n t·∫£i)

| Component | Current | Bottleneck | Optimization | Impact | Effort | Priority | Source |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| **Scaling** | 1 instance | Max ~150 req/s | ‚Üí 2-3 replicas | **2-3x capacity** | 1 hour | If needed | [^15][^16] |
| **LB Algorithm** | Round-robin | Uneven load | ‚Üí Least-conn | Even distribution | 10 min | Medium | [^14][^16] |


***

### **V. CLIENT OPTIMIZATION** (L·ªõp client)

| Component | Current | Bottleneck | Optimization | Impact | Effort | Priority | Source |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| **Batching** | 1 text/req | Roundtrip multiply | ‚Üí 10 texts/req | **‚àí90% (serial)** | 2 hours | ‚ö° \#4 | [^17][^18] |
| **Concurrency** | Sync/serial | Sequential | ‚Üí Async (aiohttp) | ‚àí50-70% | 3 hours | High | [^19][^20] |
| **Caching** | No cache | Redundant compute | ‚Üí Redis cache | ‚àí40-80% (dup) | 4 hours | Medium | [^21][^22] |


***

## üéØ Optimization Priority Matrix

| Priority | Action | Impact | Effort | ROI | Dependencies |
| :-- | :-- | :-- | :-- | :-- | :-- |
| **‚ö° \#1** | Batch 64 | ‚àí100ms | 5 min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | None |
| **‚ö° \#2** | Dims 512 | ‚àí50-100ms | 10 min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | None |
| **‚ö° \#3** | CPU 4 cores | ‚àí30-50ms | 5 min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | None |
| **‚ö° \#4** | Client batch | ‚àí90% serial | 2 hours | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | None |
| High | Keepalive | ‚àí10-20ms | 15 min | ‚≠ê‚≠ê‚≠ê‚≠ê | None |
| High | Async client | ‚àí50-70% | 3 hours | ‚≠ê‚≠ê‚≠ê‚≠ê | None |
| Medium | ONNX | ‚àí15-25ms | 30 min | ‚≠ê‚≠ê‚≠ê | None |
| Medium | FP16 | ‚àí30-40ms | 30 min | ‚≠ê‚≠ê‚≠ê | GPU support |


***

## üìà Cumulative Impact Roadmap

| Phase | Optimizations | Latency @ 60 CCU | Capacity | Time |
| :-- | :-- | :-- | :-- | :-- |
| Baseline | None | 600ms | ~50 req/s | ‚Äî |
| **Phase 1** | Batch 64 + CPU 4 | **450ms** | ~80 req/s | 10 min |
| **Phase 2** | + Dims 512 | **300ms** | ~120 req/s | 20 min |
| **Phase 3** | + Keepalive | **280ms** | ~130 req/s | 40 min |
| **Phase 4** | + ONNX | **250ms** | ~145 req/s | 70 min |
| **Phase 5** | + Client batch | **200ms ‚úì** | ~180 req/s | 3h 10min |
| **Phase 6** | + Async | 200ms | ~250 req/s | 6h 10min |
| **Phase 7** | + Cache | 140ms | ~350 req/s | 10h 10min |
| **Phase 8** | + 2 replicas | 200ms @ 120 CCU | ~400 req/s | 11h 10min |


***

## ‚ö° Quick Wins (10 ph√∫t ‚Üí 450ms)

```yaml
# docker-compose.yml
services:
  jina-vllm:
    deploy:
      resources:
        limits:
          cpus: '4.0'      # ‚Üê +2 vCPU = ‚àí30ms
          memory: 8G       # ‚Üê +4GB = enable batch 64+
    command: v2 --model-id jinaai/jina-embeddings-v3 \
                --batch-size 64 \  # ‚Üê √ó2 batch = ‚àí100ms
                --port 8080
```

**Expected:** 600ms ‚Üí **450ms** trong 10 ph√∫t[^1][^9]

***

## üîß Medium Term (1 ng√†y ‚Üí 250ms)

### Client-side dimension reduction

```python
response = requests.post(url, json={
    "input": batch_texts,
    "dimensions": 512  # ‚Üê ‚àí50-100ms
})
```


### Nginx keepalive

```nginx
upstream jina_backend {
    server jina-vllm:8080;
    keepalive 128;  # ‚Üê ‚àí10-20ms
}
```


### ONNX engine

```yaml
command: v2 --model-id jinaai/jina-embeddings-v3 \
            --engine optimum \  # ‚Üê ‚àí15-25ms
            --batch-size 64
```

**Expected:** 450ms ‚Üí **250ms** trong 1 ng√†y[^3][^5][^12]

***

## üöÄ Long Term (1 tu·∫ßn ‚Üí 200ms stable)

### Request batching

```python
# Before: 60 requests √ó 200ms = 12s
# After: 6 batched requests √ó 200ms = 1.2s
batch_size = 10
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    requests.post(url, json={"input": batch})
```


### Async client

```python
import asyncio, aiohttp

async def embed_parallel(batches):
    async with aiohttp.ClientSession() as session:
        tasks = [embed(session, b) for b in batches]
        return await asyncio.gather(*tasks)
```


### Redis cache

```python
cache_key = f"emb:jina:{hash(text)}"
if cached := redis.get(cache_key):
    return cached
# Else compute and cache with TTL
```

**Expected:** 250ms ‚Üí **200ms** trong 1 tu·∫ßn[^17][^19][^22]

***

## üìä Key Metrics Dashboard

| Metric | Tool | Target | Alert |
| :-- | :-- | :-- | :-- |
| GPU Util | `nvidia-smi` | 85-95% | <70% / >95% |
| CPU Util | `docker stats` | 60-80% | >90% |
| Latency P50 | APM | <200ms | >250ms |
| Latency P95 | APM | <300ms | >400ms |
| Throughput | Logs | >180 req/s | <120 req/s |


***

## üîç Validation Commands

```bash
# Load test
ab -n 300 -c 60 -p request.json http://localhost:8080/v1/embeddings

# GPU monitoring
nvidia-smi -l 1 --query-gpu=utilization.gpu,memory.used --format=csv

# Container stats
docker stats jina-vllm --no-stream

# Realistic load test
wrk -t 12 -c 60 -d 60s -s embeddings.lua http://localhost:8080/v1/embeddings
```


***


<span style="display:none">[^23]</span>

<div align="center">‚ÅÇ</div>

[^1]: https://zilliz.com/ai-faq/what-is-the-impact-of-batch-size-on-embedding-generation-throughput

[^2]: https://arxiv.org/html/2411.00136v1

[^3]: https://milvus.io/ai-quick-reference/what-techniques-can-be-used-to-speed-up-embedding-generation-for-example-using-fp16-precision-model-quantization-or-converting-the-model-to-onnx

[^4]: https://www.medoid.ai/blog/a-hands-on-walkthrough-on-model-quantization/

[^5]: https://zilliz.com/ai-models/jina-embeddings-v3

[^6]: https://jina.ai/models/jina-embeddings-v3/

[^7]: https://huggingface.co/blog/michaelfeil/infinity-amd

[^8]: https://github.com/NVIDIA/TensorRT/issues/4073

[^9]: https://github.com/vllm-project/vllm/issues/11320

[^10]: https://www.snowflake.com/en/engineering-blog/embedding-inference-arctic-16x-faster/

[^11]: https://discuss.vllm.ai/t/how-to-improve-tokenization-speed-for-embeddings-generation/1472

[^12]: https://blog.nginx.org/blog/troubleshooting-application-performance-and-slow-tcp-connections-with-nginx-amplify

[^13]: https://www.f5.com/company/blog/nginx/http-keepalives-and-web-performance

[^14]: https://www.oreateai.com/blog/advanced-load-balancing-configuration-with-nginx-and-nginx-plus-part-two/e6c707eeda90b0c9f6a9d0c41207ba40

[^15]: https://arxiv.org/html/2407.14843v1

[^16]: https://milvus.io/ai-quick-reference/how-do-i-implement-load-balancing-for-embedding-model-inference

[^17]: https://www.mixedbread.com/blog/dynamic-batching

[^18]: https://stackoverflow.com/questions/41189855/batch-http-request-performance-gain

[^19]: https://www.baseten.co/blog/your-client-code-matters-10x-higher-embedding-throughput-with-python-and-rust/

[^20]: https://community.openai.com/t/struggling-to-achieve-fast-parallel-embeddings/1043582

[^21]: https://arxiv.org/pdf/2507.07061.pdf

[^22]: https://redis.io/docs/latest/develop/ai/redisvl/user_guide/embeddings_cache/

[^23]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/images/760047/3abbd595-e450-4a37-b50d-c1efa6001b78/image.jpg




---

<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>


# D·ª±a tr√™n nghi√™n c·ª©u to√†n di·ªán, ƒë√¢y l√† **MECE framework** ƒë·∫ßy ƒë·ªß v·ªõi d·∫´n ch·ª©ng ngu·ªìn ƒë·ªÉ t·ªëi ∆∞u embedding service t·ª´ 600ms @ 60 CCU (200 ms @20 CCU) -> xu·ªëng 200ms @ 60 CCU.

***

## üéØ MECE Framework - T·ªëi ∆∞u Embedding Throughput

### Root Cause Analysis

Hi·ªán t·∫°i: **200ms @ 20 CCU ‚Üí 600ms @ 60 CCU** (latency tƒÉng tuy·∫øn t√≠nh v·ªõi load). M·ª•c ti√™u gi·ªØ **200ms @ 60 CCU** c·∫ßn **3x throughput capacity**.[^1]

***

## I. INFERENCE ENGINE TUNING (GPU/Model-level)

### A. Batch Size Optimization ‚ö° Priority \#1

**Current state:** `--batch-size 32`

**Evidence-based recommendation:**

- Batch size c√≥ **t√°c ƒë·ªông phi tuy·∫øn** l√™n throughput: tƒÉng t·ª´ batch 1 ‚Üí 64 c√≥ th·ªÉ ƒë·∫°t **26.6x throughput**[^2]
- Embedding models th∆∞·ªùng optimal ·ªü batch 16‚Äì32, nh∆∞ng v·ªõi GPU memory l·ªõn c√≥ th·ªÉ ƒë·∫©y l√™n 64‚Äì128[^3][^1]
- Infinity tr√™n AMD MI300x khuy·∫øn ngh·ªã `--batch-size 128` khi c√≥ nhi·ªÅu VRAM[^4]

**Implementation:**

```yaml
# Test incremental
command: v2 --model-id jinaai/jina-embeddings-v3 \
            --batch-size 64 \    # 2x hi·ªán t·∫°i
            --port 8080

# N·∫øu GPU memory ƒë·ªß (>8GB VRAM)
command: v2 --model-id jinaai/jina-embeddings-v3 \
            --batch-size 96 \
            --port 8080
```

**Expected improvement:** **‚àí100ms** latency (gi·∫£m t·ª´ 600ms xu·ªëng 500ms)[^1]

**Monitor:**

```bash
nvidia-smi -l 1  # GPU util n√™n 85‚Äì95%, memory <80%
docker logs -f jina-vllm | grep "batch\|throughput"
```


***

### B. Precision \& Quantization

**Current state:** Full precision (fp32/fp16 default)

**Evidence:**

- **FP16** gi·∫£m memory v√† compute requirements, GPU hi·ªán ƒë·∫°i (A100, V100) c√≥ Tensor Cores tƒÉng t·ªëc FP16 operations **2‚Äì3x**[^5][^6]
- **INT8 quantization** gi·∫£m model size **4x** v√† tƒÉng inference speed **2‚Äì3x**, nh∆∞ng c·∫ßn calibration ƒë·ªÉ tr√°nh accuracy drop[^7][^5]
- Benchmark: FP16 tr√™n BERT-like models ƒë·∫°t **30‚Äì40ms gi·∫£m latency** so v·ªõi FP32[^5]

**Implementation:**

```yaml
# Option 1: ONNX engine (optimized for embedding models)
command: v2 --model-id jinaai/jina-embeddings-v3 \
            --batch-size 64 \
            --engine optimum \     # ‚Üê ONNX backend
            --port 8080

# Option 2: Explicit fp16 (if supported by image)
environment:
  - CUDA_VISIBLE_DEVICES=0
  - NVIDIA_TF32=1  # For A100/H100
```

**Expected improvement:** **‚àí30‚Äì40ms** latency + **+20‚Äì30% throughput**[^4][^5]

**Trade-off:** Vector quality gi·∫£m ~0.5%, nh∆∞ng embedding tasks √≠t nh·∫°y c·∫£m h∆°n generative tasks[^7]

***

### C. Dimensionality Reduction (Matryoshka) ‚ö° Priority \#2

**Current state:** 1024 dimensions

**Evidence:**

- Jina-v3 h·ªó tr·ª£ **Matryoshka Representation Learning (MRL)**, cho ph√©p gi·∫£m dims t·ª´ 1024 xu·ªëng 32, 64, 128, 256, 512 m√† v·∫´n gi·ªØ performance[^8][^9]
- Benchmark: **64 dims gi·ªØ 92% retrieval performance** so v·ªõi 1024 dims[^9]
- 512 dims l√† sweet spot cho semantic search: gi·∫£m **50% memory bandwidth**, **20‚Äì30ms inference time**, v·ªõi ch·ªâ **1‚Äì2% quality drop**[^10][^8]

**Implementation (client-side):**

```python
import requests

response = requests.post(
    "http://localhost:8080/v1/embeddings",
    json={
        "input": ["text1", "text2", ...],  # Batch 10 texts
        "model": "jinaai/jina-embeddings-v3",
        "dimensions": 512  # ‚Üê Gi·∫£m t·ª´ 1024
    }
)
```

**Impact breakdown:**

- **Memory bandwidth:** 512 dims = 50% gi·∫£m data transfer[^8]
- **GPU forward pass:** ‚àí20‚Äì30ms (√≠t computation layers)[^10]
- **Network I/O:** vector nh·ªè h∆°n ‚Üí ‚àí5ms serialize/deserialize[^8]
- **Enables larger batch:** v·ªõi c√πng VRAM, c√≥ th·ªÉ tƒÉng batch size t·ª´ 64 ‚Üí 96‚Äì128[^1]

**Expected improvement:** **‚àí50‚Äì100ms** + c√≥ th·ªÉ tƒÉng batch size[^9][^8]

‚ö†Ô∏è **Validation required:** Test tr∆∞·ªõc tr√™n application c·ªßa b·∫°n xem 512 dims c√≥ ƒë·ªß ch·∫•t l∆∞·ª£ng cho use case kh√¥ng (retrieval th∆∞·ªùng OK, classification c√≥ th·ªÉ c·∫ßn full 1024)[^9]

***

### D. Engine Swap (ONNX/TensorRT)

**Evidence:**

- **ONNX (via Optimum):** t·ªëi ∆∞u cho embedding models, benchmark tr√™n CPU ƒë√£ ƒë·∫°t **800% better latency vs vanilla transformers**, tr√™n GPU kho·∫£ng **15‚Äì25ms gi·∫£m**[^11][^4]
- **TensorRT:** NVIDIA-specific optimization, c√≥ th·ªÉ ƒë·∫°t **30‚Äì40ms gi·∫£m** nh∆∞ng compile time l√¢u (3‚Äì10 ph√∫t)[^12]

**Implementation:**

```yaml
# ONNX (easiest, no compile)
command: v2 --model-id jinaai/jina-embeddings-v3 \
            --engine optimum \
            --batch-size 64 \
            --port 8080

# TensorRT (NVIDIA GPU only, needs initial compilation)
command: v2 --model-id jinaai/jina-embeddings-v3 \
            --engine tensorrt \
            --batch-size 64 \
            --port 8080
```

**Expected improvement:** ONNX **‚àí15‚Äì25ms**, TensorRT **‚àí30‚Äì40ms**[^12][^4]

***

## II. RESOURCE ALLOCATION (Container)

### A. CPU Allocation ‚ö° Priority \#3

**Current state:** `limits.cpus: '2.0'`

**Evidence:**

- **Tokenization chi·∫øm 90% CPU overhead** trong embedding pipeline (string manipulation, regex, dictionary lookup‚ÄîGPU kh√¥ng l√†m t·ªët)[^13][^14]
- Benchmark Snowflake vLLM: embed() function ch·ªâ 10% total time, c√≤n l·∫°i 90% l√† CPU tokenization + serialization[^13]
- vLLM issue: CPU bottleneck t·∫°i **100% util gi·ªõi h·∫°n throughput ·ªü 12 TPS**, d√π GPU c√≤n idle[^15]
- Infinity d√πng **dedicated worker threads** cho tokenization, tƒÉng cores t·ª´ 2 ‚Üí 4 gi√∫p tokenize nhi·ªÅu request song song[^16][^4]

**Why CPU matters:**

```
Pipeline flow per request:
1. [CPU] Parse JSON, tokenize text ‚Üí input_ids    (0.5‚Äì25ms)
2. [CPU ‚Üí GPU] Transfer tensor                    (0.1ms)
3. [GPU] Forward pass transformer                 (2‚Äì100ms)
4. [GPU ‚Üí CPU] Transfer embedding                 (0.1ms)
5. [CPU] Pooling, normalize, serialize JSON       (0.01‚Äì0.1ms)
```

Khi c√≥ 60 request ƒë·ªìng th·ªùi, **2 CPU cores kh√¥ng k·ªãp tokenize** ‚Üí GPU ƒë√≥i (idle waiting for batches)[^15][^13]

**Implementation:**

```yaml
deploy:
  resources:
    limits:
      cpus: '4.0'      # ‚Üê TƒÉng t·ª´ 2.0
      memory: 8G       # ‚Üê TƒÉng ƒë·ªÉ cache batch l·ªõn h∆°n
    reservations:
      cpus: '2.0'
      memory: 4G
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
```

**Additional optimization (if Infinity supports):**

```yaml
command: v2 --model-id jinaai/jina-embeddings-v3 \
            --batch-size 64 \
            --workers 4 \      # ‚Üê Parallel tokenization threads
            --port 8080
```

**Expected improvement:** **‚àí30‚Äì50ms** (CPU kh√¥ng bottleneck, GPU utilization tƒÉng t·ª´ ~60% l√™n ~90%)[^13][^15]

***

### B. GPU Management

**Current state:** 1 GPU, kh√¥ng explicit device management

**Optimization:**

```yaml
environment:
  - CUDA_VISIBLE_DEVICES=0
  - CUDA_LAUNCH_BLOCKING=0       # ‚Üê Async kernel launch
  - NVIDIA_TF32=1                # ‚Üê Enable TF32 (A100/H100)
  
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          device_ids: ['0']      # ‚Üê Explicit GPU assignment
          count: 1
          capabilities: [gpu]
```

**Memory calculation for batch_size 96 with 1024 dims:**

```
Per embedding: 1024 float32 = 4KB
Batch 96 input: 384KB
Model weights (jina-v3): ~300MB
KV cache: ~100MB
Peak activations: ~200MB
‚Üí Total: ~600MB, fits most GPUs (even T4 with 16GB)
```


***

### C. Memory Tuning

**Evidence:**

- Larger batch sizes c·∫ßn buffer memory ƒë·ªÉ group requests[^1]
- TƒÉng memory gi√∫p cache tokenized inputs, gi·∫£m re-tokenization[^13]

**Implementation:**

```yaml
limits:
  memory: 8G  # ‚Üê TƒÉng t·ª´ 4G ƒë·ªÉ accommodate batch 96
```


***

## III. PIPELINE OPTIMIZATION (Request handling)

### A. Tokenization Pipeline

**Evidence:**

- Snowflake ƒë·∫°t **3x throughput** b·∫±ng c√°ch **disaggregate tokenization v√† inference** th√†nh 2-stage pipeline[^13]
- Tokenization cho r2 ch·∫°y song song v·ªõi inference r1 ‚Üí GPU kh√¥ng idle[^13]

**Implementation (if supported by Infinity):**

```yaml
command: v2 --model-id jinaai/jina-embeddings-v3 \
            --batch-size 64 \
            --workers 4 \             # ‚Üê Tokenization workers
            --pipeline-parallel \     # ‚Üê Enable pipeline if available
            --port 8080
```

**Expected improvement:** **‚àí20‚Äì30ms**[^13]

***

### B. HTTP \& Connection Management

**Current Nginx problem:** M·ªói request t·∫°o new TCP connection ‚Üí overhead l·ªõn khi 60 CCU[^17]

**Evidence:**

- **Keepalive connections** gi·∫£m TCP overhead, c√≥ th·ªÉ **double performance** cho remote backends[^17]
- NGINX upstream keepalive gi·∫£m packet round trips v√† consistent response time[^18]
- Blog NGINX benchmark: keepalive tƒÉng throughput **5x** cho EU ‚Üí US West scenario[^17]

**Optimized nginx.conf:**

```nginx
upstream jina_backend {
    least_conn;  # ‚Üê Load balance by connection count
    server jina-vllm:8080 weight=1 max_fails=3 fail_timeout=30s;
    keepalive 128;  # ‚Üê Connection pool size per worker
}

server {
    listen 80;
    
    # Worker config (add to main nginx.conf)
    worker_processes auto;
    worker_connections 4096;  # ‚Üê TƒÉng t·ª´ default 1024
    
    location /v1/embeddings {
        proxy_pass http://jina_backend;
        
        # Connection pooling (CRITICAL)
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        proxy_set_header Keep-Alive "timeout=65";
        
        # Timeouts
        proxy_read_timeout 120s;      # ‚Üê TƒÉng t·ª´ 60s
        proxy_connect_timeout 10s;
        proxy_send_timeout 30s;
        
        # Buffering
        proxy_buffering on;
        proxy_buffer_size 8k;         # ‚Üê TƒÉng t·ª´ 4k
        proxy_buffers 16 8k;          # ‚Üê TƒÉng t·ª´ 8 4k
        
        # Headers
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # Request body limit
        client_max_body_size 10M;     # ‚Üê Cho batch requests l·ªõn
    }
    
    location /health {
        proxy_pass http://jina_backend;
        proxy_set_header Host $host;
    }
}
```

**Expected improvement:** **‚àí10‚Äì20ms** (HTTP overhead gi·∫£m, connection reuse)[^18][^17]

***

### C. Pooling \& Post-processing

**Minor optimization (if Infinity supports):**

```yaml
command: v2 --model-id jinaai/jina-embeddings-v3 \
            --batch-size 64 \
            --pooling-method mean \    # vs cls (mean th∆∞·ªùng nhanh h∆°n)
            --normalize \
            --port 8080
```

**Expected improvement:** **‚àí1‚Äì2ms** (minimal)[^4]

***

## IV. PROXY \& LOAD BALANCING

### A. Single Instance Limits

**Analysis:**

- 1 GPU v·ªõi optimized config c√≥ th·ªÉ ƒë·∫°t **100‚Äì150 req/s @ 200ms latency**[^19]
- 60 CCU @ 200ms target = **300 req/s** ‚Üí c·∫ßn **2‚Äì3 instances**[^20]

**Decision tree:**

1. √Åp d·ª•ng **I‚ÄìIII** tr∆∞·ªõc (batch, CPU, dims reduction)
2. N·∫øu v·∫´n >250ms @ 60 CCU ‚Üí scale ngang

***

### B. Horizontal Scaling (if needed)

**Evidence:**

- Horizontal scaling distributes load across replicas, **shines for increasing throughput**[^21]
- Load balancing ensures no bottleneck, improves response times[^20]
- AWS Well-Architected: horizontal scaling increases **aggregate availability**[^22]

**Implementation (2-replica setup):**

```yaml
version: '3.8'

x-jina-default: &jina-default
  image: michaelf34/infinity:latest
  restart: always
  deploy:
    resources:
      limits:
        cpus: '4.0'
        memory: 8G
      reservations:
        cpus: '2.0'
        memory: 4G
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  expose:
    - "8080"
  volumes:
    - ./hf_cache:/app/.cache
  command: v2 --model-id jinaai/jina-embeddings-v3 --batch-size 64 --port 8080
  networks:
    - mem0_network
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 120s

services:
  jina-vllm-1:
    <<: *jina-default
    container_name: jina-vllm-1
    environment:
      - CUDA_VISIBLE_DEVICES=0  # GPU 1

  jina-vllm-2:
    <<: *jina-default
    container_name: jina-vllm-2
    environment:
      - CUDA_VISIBLE_DEVICES=1  # GPU 2 (n·∫øu c√≥)
      # Ho·∫∑c CUDA_VISIBLE_DEVICES=0 n·∫øu d√πng chung GPU (c·∫ßn >12GB VRAM)

  infinity-proxy:
    image: nginx:alpine
    restart: unless-stopped
    ports:
      - "8080:80"
    volumes:
      - ./nginx-lb.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      jina-vllm-1:
        condition: service_healthy
      jina-vllm-2:
        condition: service_healthy
    networks:
      - mem0_network
```

**nginx-lb.conf (load balancer):**

```nginx
upstream jina_cluster {
    least_conn;  # ‚Üê Balance by active connections
    server jina-vllm-1:8080 weight=1 max_fails=3 fail_timeout=30s;
    server jina-vllm-2:8080 weight=1 max_fails=3 fail_timeout=30s;
    keepalive 256;  # ‚Üê 2x connection pool (128 per instance)
}

server {
    listen 80;
    worker_processes auto;
    worker_connections 8192;  # ‚Üê 2x workload
    
    location /v1/embeddings {
        proxy_pass http://jina_cluster;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        proxy_set_header Keep-Alive "timeout=65";
        
        proxy_read_timeout 120s;
        proxy_connect_timeout 10s;
        proxy_buffering on;
        proxy_buffer_size 8k;
        proxy_buffers 16 8k;
    }
    
    location /health {
        proxy_pass http://jina_cluster;
        proxy_set_header Host $host;
    }
}
```

**Expected improvement:** **2‚Äì3x throughput capacity**, gi·ªØ 200ms @ 120 CCU[^23][^20]

***

## V. CLIENT-SIDE OPTIMIZATION ‚ö° Priority \#4

### A. Request Batching (MOST CRITICAL)

**Current problem:** N·∫øu client g·ª≠i 60 requests ri√™ng l·∫ª ‚Üí m·ªói request 200ms ‚Üí t·ªïng 12 gi√¢y

**Evidence:**

- Batching gi·∫£m **roundtrip latency multiplication**: 1 request x·ª≠ l√Ω 10 texts nhanh h∆°n 10 requests ri√™ng l·∫ª[^24]
- Dynamic batching c√≥ th·ªÉ tƒÉng **throughput 10x**[^25]
- Benchmark Mixedbread: GPU underutilized khi process t·ª´ng request, batching tƒÉng GPU util v√† gi·∫£m per-request latency[^25]

**Bad practice (serial):**

```python
# ‚ùå 60 requests √ó 200ms = 12 gi√¢y
for text in texts:  
    response = requests.post(
        "/v1/embeddings",
        json={"input": text}
    )
```

**Good practice (batch):**

```python
# ‚úÖ 6 requests √ó 200ms = 1.2 gi√¢y (10x faster!)
batch_size = 10
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    response = requests.post(
        "/v1/embeddings",
        json={
            "input": batch,  # ‚Üê List of texts
            "model": "jinaai/jina-embeddings-v3",
            "dimensions": 512  # ‚Üê Bonus: reduce dims
        }
    )
```

**Expected improvement:** **‚àí90% total time** n·∫øu client ƒëang g·ª≠i serial[^24][^25]

***

### B. Async Client (Concurrent Requests)

**Evidence:**

- Async client v·ªõi `asyncio` enables parallel requests without thread pool[^26][^27]
- Baseten Performance Client (Rust-backed) ƒë·∫°t **12x higher throughput** v·ªõi async[^28]
- Async kh√¥ng gi√∫p 1 request nhanh h∆°n, nh∆∞ng gi√∫p x·ª≠ l√Ω **nhi·ªÅu requests song song**[^29][^30]

**Implementation:**

```python
import aiohttp
import asyncio

async def embed_batch_async(session, batch, url):
    """Single batch embedding request"""
    async with session.post(
        url,
        json={
            "input": batch,
            "model": "jinaai/jina-embeddings-v3",
            "dimensions": 512
        }
    ) as response:
        return await response.json()

async def process_all_batches(texts, url, batch_size=10, max_concurrent=20):
    """Process all batches concurrently with concurrency limit"""
    # Split into batches
    batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]
    
    # Semaphore to limit concurrent requests (respect server capacity)
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async with aiohttp.ClientSession() as session:
        async def process_with_limit(batch):
            async with semaphore:
                return await embed_batch_async(session, batch, url)
        
        tasks = [process_with_limit(batch) for batch in batches]
        results = await asyncio.gather(*tasks)
    
    return results

# Usage
texts = [...]  # 600 texts
url = "http://localhost:8080/v1/embeddings"

embeddings = asyncio.run(
    process_all_batches(texts, url, batch_size=10, max_concurrent=20)
)
# 600 texts / 10 per batch = 60 batches
# With 20 concurrent: ~3 rounds √ó 200ms = 600ms total
```

**Expected improvement:** **‚àí50‚Äì70%** total processing time[^27][^28]

**Best practice:** Combine batching + async:

- Batch 10 texts per request (leverage server-side batching)[^25]
- Send 20 batched requests concurrently (leverage client parallelism)[^26]
- Result: 600 texts trong ~600ms thay v√¨ 12 gi√¢y[^28]

***

### C. Caching \& Deduplication

**Evidence:**

- Embedding caching gi·∫£m computational costs, decreases latency[^31]
- Ensemble embedding approach ƒë·∫°t **92% cache hit ratio** cho semantically equivalent queries[^32]
- Semantic deduplication s·ª≠ d·ª•ng embeddings ƒë·ªÉ identify duplicate content[^33]

**Implementation (simple in-memory cache):**

```python
import hashlib
from functools import lru_cache

class EmbeddingCache:
    def __init__(self):
        self.cache = {}  # text_hash ‚Üí embedding
    
    def get_hash(self, text):
        return hashlib.md5(text.encode()).hexdigest()
    
    async def get_embeddings(self, texts, url):
        # Check cache
        uncached_texts = []
        uncached_indices = []
        results = [None] * len(texts)
        
        for i, text in enumerate(texts):
            text_hash = self.get_hash(text)
            if text_hash in self.cache:
                results[i] = self.cache[text_hash]
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # Fetch uncached
        if uncached_texts:
            response = await embed_batch_async(session, uncached_texts, url)
            for i, emb in zip(uncached_indices, response['data']):
                text_hash = self.get_hash(texts[i])
                self.cache[text_hash] = emb['embedding']
                results[i] = emb['embedding']
        
        return results
```

**Redis-based cache (production):**

```python
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def get_cached_embedding(text, url, ttl=3600):
    text_hash = hashlib.md5(text.encode()).hexdigest()
    cache_key = f"emb:jina-v3:{text_hash}"
    
    # Check cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Compute and cache
    response = requests.post(url, json={"input": text})
    embedding = response.json()['data'][^0]['embedding']
    
    redis_client.setex(cache_key, ttl, json.dumps(embedding))
    return embedding
```

**Expected improvement:** **‚àí40‚Äì80% requests** n·∫øu c√≥ 20‚Äì30% duplicate texts[^32][^31]

**Use cases:**

- User queries th∆∞·ªùng c√≥ pattern l·∫∑p l·∫°i[^32]
- Document corpus c√≥ duplicate paragraphs[^33]
- FAQ systems: same questions rephrased[^32]

***

## üìä OPTIMIZATION ROADMAP

| Phase | Actions | Effort | Cumulative Latency @ 60 CCU | Source |
| :-- | :-- | :-- | :-- | :-- |
| **0. Baseline** | Current config | ‚Äî | **600ms** | ‚Äî |
| **1. Quick Wins** | Batch 64 + CPU 4 | 5 min | **450ms** (-150ms) | [^1][^15] |
| **2. Dimensions** | Reduce to 512 dims | 10 min | **300ms** (-150ms) | [^8][^9] |
| **3. Nginx** | Keepalive + buffer | 15 min | **280ms** (-20ms) | [^17][^18] |
| **4. Engine** | ONNX + fp16 | 30 min | **250ms** (-30ms) | [^5][^4] |
| **5. Client Batch** | Batch 10 texts/req | 2 hours | **200ms** (-50ms) ‚úì | [^25][^24] |
| **6. Scale (if needed)** | 2 replicas | 1 hour | **200ms @ 120 CCU** | [^23][^20] |


***

## üöÄ IMPLEMENTATION PRIORITY

### Day 1 (Target: 450ms @ 60 CCU)

```yaml
# docker-compose.yml
services:
  jina-vllm:
    deploy:
      resources:
        limits:
          cpus: '4.0'      # ‚Üê Change #1
          memory: 8G       # ‚Üê Change #2
    command: v2 --model-id jinaai/jina-embeddings-v3 \
                --batch-size 64 \  # ‚Üê Change #3
                --port 8080
```

**Validation:**

```bash
# Load test with 60 concurrent users
ab -n 300 -c 60 \
   -p request.json \
   -T application/json \
   http://localhost:8080/v1/embeddings
```

**Expected:** 600ms ‚Üí **450ms**[^15][^1]

***

### Day 2 (Target: 300ms @ 60 CCU)

**Client-side request batching:**

```python
# Before: 60 serial requests
# After: 6 batched requests (10 texts each)
batch_size = 10
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    requests.post(url, json={
        "input": batch,
        "dimensions": 512  # ‚Üê Also reduce dimensions
    })
```

**Expected:** 450ms ‚Üí **300ms**[^8][^25]

***

### Day 3 (Target: 250ms @ 60 CCU)

**Nginx keepalive:**

```nginx
upstream jina_backend {
    server jina-vllm:8080;
    keepalive 128;
}

server {
    location /v1/embeddings {
        proxy_pass http://jina_backend;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }
}
```

**ONNX engine:**

```yaml
command: v2 --model-id jinaai/jina-embeddings-v3 \
            --batch-size 64 \
            --engine optimum \  # ‚Üê ONNX
            --port 8080
```

**Expected:** 300ms ‚Üí **250ms**[^5][^17]

***

### Week 2 (Target: 200ms @ 60 CCU)

**Async client + caching:**

```python
import aiohttp
import asyncio

async def process_with_cache(texts):
    # Check cache first
    uncached = [t for t in texts if not in_cache(t)]
    
    # Batch + async fetch
    batches = [uncached[i:i+10] for i in range(0, len(uncached), 10)]
    
    async with aiohttp.ClientSession() as session:
        tasks = [embed_batch(session, b) for b in batches]
        results = await asyncio.gather(*tasks)
    
    return results
```

**Expected:** 250ms ‚Üí **200ms**[^28][^32]

***

## üìà MONITORING COMMANDS

```bash
# GPU utilization (should be 85‚Äì95%)
nvidia-smi -l 1

# Container logs
docker logs -f jina-vllm | grep -E "batch|latency|throughput"

# Nginx access log (response times)
docker exec infinity-proxy tail -f /var/log/nginx/access.log

# Load test with wrk
wrk -t 12 -c 60 -d 60s \
  -s embeddings.lua \
  http://localhost:8080/v1/embeddings

# Benchmark script (embeddings.lua)
wrk.method = "POST"
wrk.body   = '{"input":["test text"],"model":"jinaai/jina-embeddings-v3"}'
wrk.headers["Content-Type"] = "application/json"
```


***

## üéØ FINAL SUMMARY

ƒê·ªÉ ƒë·∫°t **200ms @ 60 CCU**, combo t·ªëi ∆∞u:

1. **Batch size 64** + **CPU 4 cores** = ‚àí150ms[^15][^1]
2. **Dimensions 512** = ‚àí150ms[^9][^8]
3. **Nginx keepalive** = ‚àí20ms[^17]
4. **ONNX engine** = ‚àí30ms[^5]
5. **Client batching** = ‚àí50ms[^25]

‚Üí Total: **600ms ‚Üí 200ms** ‚úì

N·∫øu target ch·∫∑t h∆°n (150ms) ho·∫∑c load tƒÉng l√™n 100+ CCU ‚Üí scale **2‚Äì3 replicas** v·ªõi Nginx load balancing.[^23][^20]
<span style="display:none">[^34][^35][^36][^37][^38][^39][^40][^41][^42][^43][^44][^45][^46][^47][^48][^49][^50][^51][^52][^53][^54][^55][^56][^57][^58][^59][^60][^61][^62][^63]</span>

<div align="center">‚ÅÇ</div>

[^1]: https://zilliz.com/ai-faq/what-is-the-impact-of-batch-size-on-embedding-generation-throughput

[^2]: https://arxiv.org/html/2411.00136v1

[^3]: https://github.com/huggingface/sentence-transformers/issues/2551

[^4]: https://huggingface.co/blog/michaelfeil/infinity-amd

[^5]: https://milvus.io/ai-quick-reference/what-techniques-can-be-used-to-speed-up-embedding-generation-for-example-using-fp16-precision-model-quantization-or-converting-the-model-to-onnx

[^6]: https://www.medoid.ai/blog/a-hands-on-walkthrough-on-model-quantization/

[^7]: https://www.linkedin.com/posts/ritik-shrivastava-data1456_llm-int8-quantization-activity-7385956506560761856-T7_f

[^8]: https://zilliz.com/ai-models/jina-embeddings-v3

[^9]: https://jina.ai/models/jina-embeddings-v3/

[^10]: https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/

[^11]: https://huggingface.co/blog/infinity-cpu-performance

[^12]: https://github.com/NVIDIA/TensorRT/issues/4073

[^13]: https://www.snowflake.com/en/engineering-blog/embedding-inference-arctic-16x-faster/

[^14]: https://www.digitalocean.com/community/tutorials/run-tokenizer-on-gpu-for-faster-nlp

[^15]: https://github.com/vllm-project/vllm/issues/11320

[^16]: https://github.com/michaelfeil/infinity

[^17]: https://blog.nginx.org/blog/troubleshooting-application-performance-and-slow-tcp-connections-with-nginx-amplify

[^18]: https://www.f5.com/company/blog/nginx/http-keepalives-and-web-performance

[^19]: https://www.baseten.co/resources/guide/high-performance-embedding-model-inference/

[^20]: https://milvus.io/ai-quick-reference/how-do-i-implement-load-balancing-for-embedding-model-inference

[^21]: https://www.clarifai.com/blog/horizontal-vs-vertical-scaling

[^22]: https://docs.aws.amazon.com/wellarchitected/latest/generative-ai-lens/genrel05-bp01.html

[^23]: https://arxiv.org/html/2407.14843v1

[^24]: https://stackoverflow.com/questions/41189855/batch-http-request-performance-gain

[^25]: https://www.mixedbread.com/blog/dynamic-batching

[^26]: https://docs.aleph-alpha.com/docs/changelog/2022-11-14-async-python-client/

[^27]: https://community.openai.com/t/struggling-to-achieve-fast-parallel-embeddings/1043582

[^28]: https://www.baseten.co/blog/your-client-code-matters-10x-higher-embedding-throughput-with-python-and-rust/

[^29]: https://stackoverflow.com/questions/53021448/multiple-async-requests-simultaneously

[^30]: https://python.plainenglish.io/comparing-http-clients-in-python-requests-aiohttp-and-httpx-a9ffa1caab9a

[^31]: https://redis.io/docs/latest/develop/ai/redisvl/user_guide/embeddings_cache/

[^32]: https://arxiv.org/pdf/2507.07061.pdf

[^33]: https://docs.nvidia.com/nemo/curator/25.09/curate-text/process-data/deduplication/semdedup.html

[^34]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/images/760047/3abbd595-e450-4a37-b50d-c1efa6001b78/image.jpg

[^35]: https://embeddedllm.com/blog/why-jamai-base-moved-embedding-models-to-intel-xeon-cpus

[^36]: https://michaelfeil.eu/infinity/0.0.34/benchmarking/

[^37]: https://arxiv.org/html/2409.10173v3

[^38]: https://github.com/vllm-project/vllm/issues/12085

[^39]: https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Beyond-GPUs-Why-JamAI-Base-Moved-Embedding-Models-to-Intel-Xeon/post/1650850

[^40]: https://discuss.vllm.ai/t/how-to-improve-tokenization-speed-for-embeddings-generation/1472

[^41]: https://cursor.com/blog/llama-inference

[^42]: https://dataloop.ai/library/model/jinaai_jina-embeddings-v3/

[^43]: https://www.daft.ai/blog/cutting-llm-batch-inference-time-in-half-dynamic-prefix-bucketing-at-scale

[^44]: https://michaelfeil.eu/infinity/0.0.75/benchmarking/

[^45]: https://github.com/ruven/iipsrv/issues/203

[^46]: https://stackoverflow.com/questions/56442197/nginx-tcp-stream-optimization

[^47]: https://www.oreateai.com/blog/advanced-load-balancing-configuration-with-nginx-and-nginx-plus-part-two/e6c707eeda90b0c9f6a9d0c41207ba40

[^48]: https://serverspace.io/support/help/ssl-termination-and-load-balancer-optimization-on-nginx/

[^49]: https://blog.nginx.org/blog/performance-tuning-tips-tricks

[^50]: https://doc.hcs.huawei.com/usermanual/cce/cce_10_0919.html

[^51]: https://forums.developer.nvidia.com/t/int8-vs-fp16-results/158139

[^52]: https://www.newline.co/@zaoyang/horizontal-scaling-for-llms-best-practices--e68f4ac8

[^53]: https://www.databasemart.com/blog/fp32-fp16-bf16-int8

[^54]: https://github.com/encode/httpx/discussions/2662

[^55]: https://www.vldb.org/pvldb/vol15/p2230-zou.pdf

[^56]: https://openreview.net/pdf?id=LCTmppB165

[^57]: https://paradigm-academy.lighton.ai/en/dev-guides/ai-models/async-requests

[^58]: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/embeddings/batch-prediction-genai-embeddings

[^59]: https://docs.voyageai.com/discuss/65d688e425625f00254e81d2

[^60]: https://developers.googleblog.com/en/gemini-batch-api-now-supports-embeddings-and-openai-compatibility/

[^61]: https://qdrant.tech/documentation/database-tutorials/async-api/

[^62]: https://community.openai.com/t/embeddings-api-max-batch-size/655329

[^63]: https://lightning.ai/docs/litserve/features/async-concurrency

