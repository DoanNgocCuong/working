
```
1. MECE C√ÅC C√ÅCH D·ª∞NG 
2. CH·ªåN C√ÅCH T·ªêT NH·∫§T 
```

<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# ƒê√ÅNH GI√Å MECE T·∫§T C·∫¢ C√ÅC C√ÅCH D·ª∞NG,

Cho m√¨nh 3 c√°ch t·ªët nh·∫•t

V·ªõi 3090 v√† b√†i to√°n ch·ªâ embed (kh√¥ng generate), c√≥ 3 ‚Äúƒë∆∞·ªùng d·ª±ng‚Äù **MECE** v√† t·ªët nh·∫•t ƒë·ªÉ b·∫°n c√¢n nh·∫Øc:

***

## 1. TEI (Text Embeddings Inference) ‚Äì Chu·∫©n production ƒë∆°n gi·∫£n nh·∫•t

**Use-case:**

- B·∫°n ch·ªâ c·∫ßn **REST API embed**, kh√¥ng c·∫ßn OpenAI-compatible API.
- ∆Øu ti√™n: ƒë∆°n gi·∫£n, ·ªïn ƒë·ªãnh, latency th·∫•p, scale v·ª´a ph·∫£i.

**∆Øu ƒëi·ªÉm:**

- ƒê∆∞·ª£c thi·∫øt k·∫ø **chuy√™n cho embedding** n√™n r·∫•t nhanh, c√≥ batching li√™n t·ª•c, FP16, tokenizer b·∫±ng Rust.[^1]
- Config c·ª±c ng·∫Øn, deploy b·∫±ng m·ªôt l·ªánh Docker; ƒë√£ ƒë∆∞·ª£c c·ªông ƒë·ªìng d√πng nhi·ªÅu cho BGE, E5, Jina v3.[^2][^3]
- 3090 r·∫•t ph√π h·ª£p (Ampere + Tensor Core) cho FP16, context 8192 tokens c·ªßa Jina v3.[^4][^2]

**Nh∆∞·ª£c ƒëi·ªÉm:**

- Kh√¥ng gi·∫£ l·∫≠p OpenAI API spec; c·∫ßn vi·∫øt adapter m·ªèng n·∫øu Mem0 mong ch·ªù format OpenAI.

**Khi n√™n ch·ªçn:**

- B·∫°n mu·ªën **ch·∫°y nhanh nh·∫•t v·ªõi effort th·∫•p nh·∫•t**; embed l√† service ri√™ng (Mem0 g·ªçi qua HTTP).

***

## 2. Infinity ‚Äì Nhi·ªÅu model embedding, OpenAI-like feeling

**Use-case:**

- B·∫°n mu·ªën 1 **embedding gateway** c√≥ th·ªÉ ch·∫°y nhi·ªÅu model (Jina, BGE, E5‚Ä¶) sau n√†y.
- ∆Øu ti√™n: ƒë∆°n gi·∫£n, nh∆∞ng linh ho·∫°t, c√≥ th·ªÉ scale ngang (nhi·ªÅu container).

**∆Øu ƒëi·ªÉm:**

- H·ªó tr·ª£ tr·ª±c ti·∫øp `jinaai/jina-embeddings-v3`, c√≥ REST API r·∫•t ƒë∆°n gi·∫£n.[^5]
- Ki·∫øn tr√∫c t·ªëi ∆∞u cho batch + throughput, h·ªó tr·ª£ FP16 nh∆∞ TEI.
- Th√≠ch h·ª£p n·∫øu sau n√†y b·∫°n mu·ªën A/B test Jina v3 vs BGE-M3 m√† kh√¥ng ƒë·ªïi h·∫° t·∫ßng.

**Nh∆∞·ª£c ƒëi·ªÉm:**

- √çt ‚Äút√†i li·ªáu marketing‚Äù h∆°n TEI, c·ªông ƒë·ªìng nh·ªè h∆°n ch√∫t.
- C·∫ßn t·ª± thi·∫øt k·∫ø health check, metrics n·∫øu mu·ªën monitoring s√¢u.

**Khi n√™n ch·ªçn:**

- B·∫°n mu·ªën **1 layer tr·ª´u t∆∞·ª£ng** cho nhi·ªÅu model embedding, linh ho·∫°t trong t∆∞∆°ng lai (ƒë·ªïi/b·ªï sung model r·∫•t nhanh).

***

## 3. vLLM Pooling Runner ‚Äì Khi c·∫ßn OpenAI-compatible server

**Use-case:**

- B·∫°n mu·ªën **OpenAI-compatible API** (g·ªçi nh∆∞ `/v1/embeddings`) ƒë·ªÉ plug th·∫≥ng v√†o Mem0 ho·∫∑c code c≈©, ho·∫∑c d√πng c√πng 1 stack v·ªõi LLM vLLM.[^6][^7]

**∆Øu ƒëi·ªÉm:**

- vLLM c√≥ engine t·ªëi ∆∞u r·∫•t t·ªët cho GPU, batching, KV cache, scale l·ªõn.[^7]
- C√≥ **pooling runner** ƒë·ªÉ d√πng encoder nh∆∞ Jina v3 cho embedding.[^8]
- Convenience: n·∫øu b·∫°n ƒë√£ d√πng vLLM cho LLM, vi·ªác reuse h·∫° t·∫ßng / observability r·∫•t ti·ªán.

**Nh∆∞·ª£c ƒëi·ªÉm:**

- Jina v3 c·∫ßn `trust_remote_code=True`, hi·ªán c√≥ m·ªôt s·ªë issue v·ªõi vLLM li√™n quan ƒë·∫øn ki·ªÉu positional embedding (RoPE vs absolute).[^9][^10]
- Setup ph·ª©c t·∫°p h∆°n TEI/Infinity; d√πng vLLM ch·ªâ ƒë·ªÉ embed ƒë√¥i khi h∆°i ‚Äúoverkill‚Äù.

**Khi n√™n ch·ªçn:**

- B·∫°n mu·ªën **OpenAI-compatible** v√† ƒë√£/ƒëang d√πng vLLM (ho·∫∑c mu·ªën 1 uniform stack cho generation + embeddings).
- Ch·∫•p nh·∫≠n debug th√™m n·∫øu c√≥ conflict v·ªõi remote code / RoPE.

***

### T√≥m t·∫Øt quy·∫øt ƒë·ªãnh (g·ª£i √Ω l·ª±a ch·ªçn)

| ∆Øu ti√™n ch√≠nh | L·ª±a ch·ªçn n√™n d√πng |
| :-- | :-- |
| Nhanh, ƒë∆°n gi·∫£n, production s·ªõm | **TEI + Jina v3 (FP16)** [^1][^2] |
| Nhi·ªÅu model embedding, d·ªÖ A/B test | **Infinity + Jina v3** [^5] |
| OpenAI-compatible API, chung stack v·ªõi LLM | **vLLM pooling + Jina v3** (c·∫ßn ch·∫•p nh·∫≠n m·ªôt √≠t friction) [^6][^8] |

N·∫øu m·ª•c ti√™u c·ªßa b·∫°n l√†: ‚ÄúMem0 + Milvus, 3090, c·∫ßn ch·∫°y ·ªïn ƒë·ªãnh trong 1-2 ng√†y tri·ªÉn khai‚Äù, th√¨ **c√°ch 1 (TEI)** l√† l·ª±a ch·ªçn ∆∞u ti√™n.
<span style="display:none">[^11][^12][^13][^14][^15][^16][^17][^18][^19][^20][^21][^22]</span>

<div align="center">‚ÅÇ</div>

[^1]: https://github.com/huggingface/text-embeddings-inference

[^2]: https://zilliz.com/ai-models/jina-embeddings-v3

[^3]: https://inference.readthedocs.io/en/v1.9.0/models/builtin/embedding/jina-embeddings-v3.html

[^4]: https://arxiv.org/abs/2409.10173

[^5]: https://github.com/michaelfeil/infinity

[^6]: https://docs.vllm.ai/en/v0.8.4/getting_started/examples/embed_jina_embeddings_v3.html

[^7]: https://docs.vllm.ai/en/v0.10.1/examples/offline_inference/embed_jina_embeddings_v3.html

[^8]: https://docs.vllm.ai/en/latest/examples/offline_inference/embed_jina_embeddings_v3.html

[^9]: https://huggingface.co/jinaai/jina-embeddings-v3/discussions/112

[^10]: https://github.com/vllm-project/vllm/issues/12154

[^11]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/760047/eddb45b3-9f96-4f62-a4cd-61b5b8ca1b26/paste.txt

[^12]: https://github.com/jina-ai/csp-examples/blob/main/azure/Deploy and inference with Embedding V3 VM.ipynb

[^13]: https://jina.ai/models/jina-embeddings-v3/

[^14]: https://www.aidoczh.com/vllm/examples/offline_inference/embed_jina_embeddings_v3.html

[^15]: https://www.whitefiber.com/compare/best-gpus-for-llm-inference-in-2025

[^16]: https://www.baseten.co/resources/guide/high-performance-embedding-model-inference/

[^17]: https://docs.vllm.ai/en/v0.8.5/getting_started/examples/embed_jina_embeddings_v3.html

[^18]: https://milvus.io/blog/we-benchmarked-20-embedding-apis-with-milvus-7-insights-that-will-surprise-you.md

[^19]: https://docs.vllm.com.cn/en/latest/getting_started/examples/embed_jina_embeddings_v3.html

[^20]: https://vllm.website.cncfstack.com/examples/pooling/embed/

[^21]: https://artsmart.ai/blog/top-embedding-models-in-2025/

[^22]: https://jina.ai/en-US/embeddings/



---

# V·ªõi models jinaai/jina-embeddings-v3 : 
```
ƒê√ÅNH GI√Å MECE T·∫§T C·∫¢ C√ÅC C√ÅCH D·ª∞NG model embedding : jinaai/jina-embeddings-v3
Cho m√¨nh 3 c√°ch t·ªët nh·∫•t v·ªõi n√≥
```


<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# ƒê√ÅNH GI√Å MECE T·∫§T C·∫¢ C√ÅC C√ÅCH D·ª∞NG model embedding : jinaai/jina-embeddings-v3

Cho m√¨nh 3 c√°ch t·ªët nh·∫•t v·ªõi n√≥

ƒê·ªÉ ƒë√°nh gi√° **MECE (Mutually Exclusive, Collectively Exhaustive)** t·∫•t c·∫£ c√°c c√°ch d·ª±ng `jina-embeddings-v3`, ch√∫ng ta c·∫ßn chia theo **C∆° ch·∫ø th·ª±c thi (Execution Engine)** v√¨ ƒë√¢y l√† y·∫øu t·ªë c·ªët l√µi quy·∫øt ƒë·ªãnh t·ªëc ƒë·ªô v√† kh·∫£ nƒÉng m·ªü r·ªông.

D∆∞·ªõi ƒë√¢y l√† b·ª©c tranh to√†n c·∫£nh v·ªÅ c√°c ph∆∞∆°ng ph√°p deploy v√† **Top 3 l·ª±a ch·ªçn t·ªët nh·∫•t** cho d·ª± √°n c·ªßa b·∫°n.

***

### PH·∫¶N 1: ƒê√ÅNH GI√Å MECE C√ÅC C√ÅCH D·ª∞NG JINA V3

Ch√∫ng ta c√≥ th·ªÉ chia to√†n b·ªô c√°c c√°ch deploy th√†nh 3 nh√≥m l·ªõn d·ª±a tr√™n t·∫ßng (layer) th·ª±c thi:

#### Nh√≥m 1: Python-based Frameworks (D·ªÖ d√πng, Ch·∫≠m)

*C∆° ch·∫ø: Ch·∫°y tr·ª±c ti·∫øp tr√™n Python runtime, b·ªã gi·ªõi h·∫°n b·ªüi GIL (Global Interpreter Lock).*

1. **Sentence-Transformers (Basic):** Import th∆∞ vi·ªán, load model v√†o RAM.
    * *∆Øu:* Code 3 d√≤ng l√† ch·∫°y.
    * *Nh∆∞·ª£c:* Kh√¥ng t·ªëi ∆∞u batching, ch·∫≠m, kh√¥ng scale ƒë∆∞·ª£c, ng·ªën VRAM.
2. **HuggingFace Transformers (Native):** D√πng class `AutoModel`.
    * *∆Øu:* Ki·ªÉm so√°t s√¢u t·ª´ng l·ªõp layer.
    * *Nh∆∞·ª£c:* Ph·ª©c t·∫°p, v·∫´n d√≠nh ƒëi·ªÉm y·∫øu c·ªßa Python.
3. **LangChain/LlamaIndex Wrapper:** D√πng wrapper c√≥ s·∫µn.
    * *∆Øu:* T√≠ch h·ª£p nhanh v√†o RAG pipeline.
    * *Nh∆∞·ª£c:* Ch·ªâ l√† v·ªè b·ªçc, b√™n d∆∞·ªõi v·∫´n l√† Sentence-Transformers ch·∫≠m ch·∫°p.

#### Nh√≥m 2: Optimized Inference Servers (C√¢n b·∫±ng, Nhanh)

*C∆° ch·∫ø: S·ª≠ d·ª•ng backend C++/Rust/CUDA ƒë·ªÉ t√≠nh to√°n, Python ch·ªâ l√†m API wrapper ho·∫∑c b·ªã lo·∫°i b·ªè ho√†n to√†n.*
4.  **Text Embeddings Inference (TEI):** (Rust + CUDA). Backend chuy√™n bi·ªát c·ªßa HuggingFace.
*   *ƒê·∫∑c ƒëi·ªÉm:* Flash Attention, Continuous Batching, Tokenization b·∫±ng Rust.
5.  **Infinity:** (Python wrapper + C++ backend).
*   *ƒê·∫∑c ƒëi·ªÉm:* H·ªó tr·ª£ dynamic batching c·ª±c t·ªët, t∆∞∆°ng th√≠ch OpenAI API.
6.  **vLLM (Pooling Mode):**
*   *ƒê·∫∑c ƒëi·ªÉm:* V·ªën d√πng cho LLM nh∆∞ng c√≥ mode ch·∫°y embedding. T·∫≠n d·ª•ng PagedAttention.

#### Nh√≥m 3: Compiled/Hardware-Specific (Si√™u nhanh, Kh√≥ setup)

*C∆° ch·∫ø: Bi√™n d·ªãch model th√†nh m√£ m√°y tƒ©nh ho·∫∑c format ph·∫ßn c·ª©ng chuy√™n bi·ªát.*
7.  **ONNX Runtime (ORT):** Chuy·ªÉn model sang ƒë·ªì th·ªã tƒ©nh (`.onnx`).
*   *ƒê·∫∑c ƒëi·ªÉm:* Ch·∫°y c·ª±c nhanh tr√™n CPU, ho·∫∑c GPU v·ªõi TensorRT.
8.  **TensorRT (NVIDIA):** Build engine ri√™ng cho GPU NVIDIA.
*   *ƒê·∫∑c ƒëi·ªÉm:* T·ªëc ƒë·ªô t·ªëi th∆∞·ª£ng tr√™n GPU, nh∆∞ng kh√≥ build v√† k√©m linh ho·∫°t (fix input size).
9.  **OpenVINO (Intel):** T·ªëi ∆∞u cho CPU Intel.

***

### PH·∫¶N 2: TOP 3 C√ÅCH T·ªêT NH·∫§T (RECOMMENDED)

D·ª±a tr√™n b·ªëi c·∫£nh b·∫°n c√≥ **GPU 3090** v√† c·∫ßn **Mem0 + Milvus**, ƒë√¢y l√† 3 c√°ch t·ªët nh·∫•t s·∫Øp x·∫øp theo ti√™u ch√≠ th·ª±c t·∫ø:

#### ü•á TOP 1: Text Embeddings Inference (TEI) - "Chu·∫©n Production"

ƒê√¢y l√† l·ª±a ch·ªçn **S·ªê 1** hi·ªán nay cho Jina v3 tr√™n GPU NVIDIA. N√≥ l√† ti√™u chu·∫©n v√†ng v·ªÅ t·ªëc ƒë·ªô/s·ª± ·ªïn ƒë·ªãnh.

* **T·∫°i sao:**
    * Native support Jina v3 (model n√†y c√≥ ki·∫øn tr√∫c ƒë·∫∑c bi·ªát, TEI h·ªó tr·ª£ t·ªët nh·∫•t).
    * **Flash Attention v2:** T·∫≠n d·ª•ng c·ª±c t·ªët s·ª©c m·∫°nh 3090.
    * **Matryoshka support:** H·ªó tr·ª£ c·∫Øt vector ngay t·∫°i server (tƒÉng t·ªëc Milvus).
* **C√°ch d·ª±ng (Docker):**

```bash
docker run --gpus all -p 8080:80 \
  -v $PWD/data:/data --name jina-tei \
  ghcr.io/huggingface/text-embeddings-inference:1.5 \
  --model-id jinaai/jina-embeddings-v3 \
  --dtype float16 \
  --max-concurrent-requests 512 \
  --max-batch-tokens 16384
```


#### ü•à TOP 2: Infinity - "Linh ho·∫°t \& OpenAI Compatible"

N·∫øu b·∫°n c·∫ßn m·ªôt API **gi·ªëng h·ªát OpenAI** ƒë·ªÉ drop-in replacement v√†o code c≈© m√† kh√¥ng s·ª≠a g√¨, Infinity l√† l·ª±a ch·ªçn s·ªë 2.

* **T·∫°i sao:**
    * T·ªëc ƒë·ªô ngang ng·ª≠a TEI (thua kho·∫£ng 5-10% trong m·ªôt s·ªë case nh∆∞ng kh√¥ng ƒë√°ng k·ªÉ).
    * **OpenAI Schema:** Output format y h·ªát OpenAI, Mem0 c√≥ th·ªÉ d√πng client OpenAI ƒë·ªÉ g·ªçi.
    * D·ªÖ setup h∆°n TEI m·ªôt ch√∫t n·∫øu c·∫ßn ch·ªânh ch·ªçt Python.
* **C√°ch d·ª±ng (Docker):**

```bash
docker run --gpus all -p 7997:7997 \
  michaelfeil/infinity:latest \
  v2 --model-id jinaai/jina-embeddings-v3 \
  --dtype float16 \
  --batch-size 32
```


#### ü•â TOP 3: ONNX Runtime (GPU) - "T·ªëi ∆∞u cho Resource th·∫•p/CPU"

N·∫øu b·∫°n mu·ªën ti·∫øt ki·ªám VRAM t·ªëi ƒëa cho 3090 (ƒë·ªÉ d√†nh VRAM ch·∫°y LLM kh√°c) ho·∫∑c mu·ªën move sang server ch·ªâ c√≥ CPU.

* **T·∫°i sao:**
    * **Quantization:** D·ªÖ d√†ng n√©n model xu·ªëng INT8 (gi·∫£m 4 l·∫ßn VRAM/RAM).
    * Jina v3 b·∫£n ONNX ch·∫°y r·∫•t nh·∫π.
* **L∆∞u √Ω:** Setup ph·ª©c t·∫°p h∆°n (c·∫ßn convert model sang `.onnx`). Ch·ªâ ch·ªçn c√°ch n√†y n·∫øu TEI chi·∫øm qu√° nhi·ªÅu VRAM so v·ªõi mong ƒë·ª£i.


### T·ªîNG K·∫æT:

üëâ **L·ªùi khuy√™n:** H√£y d√πng **C√°ch 1 (TEI)**. N√≥ l√† "State-of-the-Art" serving cho embedding hi·ªán t·∫°i. Ch·ªâ c·∫ßn 1 l·ªánh Docker l√† xong, t·∫≠n d·ª•ng h·∫øt s·ª©c m·∫°nh con 3090 c·ªßa b·∫°n.
<span style="display:none">[^1]</span>

<div align="center">‚ÅÇ</div>

[^1]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/760047/eddb45b3-9f96-4f62-a4cd-61b5b8ca1b26/paste.txt



---

# Bug 1 : 

```
2025-12-29 15:57:13 Error: Failed to parse `config.json`
2025-12-29 15:57:13 
2025-12-29 15:57:13 Caused by:
2025-12-29 15:57:13     missing field `model_type` at line 51 column 1
2025-12-29 15:58:36 Error: Failed to parse `config.json`
2025-12-29 15:58:36 
2025-12-29 15:58:36 Caused by:
2025-12-29 15:58:36     missing field `model_type` at line 51 column 1
```

### 1. ƒêang x·∫£y ra¬†chuy·ªán¬†g√¨?

- Log n√†y l√†¬†b√™n trong container TEI, ch∆∞a li√™n quan g√¨ t·ªõi Mem0:

- TEI ƒëang t·∫£i model¬†"jin***/****-*********s-v3"¬†(ch√≠nh l√†¬†jinaai/jina-embeddings-v3¬†m√† m√¨nh c·∫•u h√¨nh).

- N√≥¬†download xong c√°c file (config.json,¬†tokenizer.json, ‚Ä¶) r·ªìi¬†crash¬†v·ªõi l·ªói:

>¬†Failed¬†to parse config.json ‚Äì missing field model_type

- Nghƒ©a l√†: TEI¬†mong¬†ƒë·ª£i¬†config.json¬†c·ªßa model¬†ph·∫£i¬†c√≥¬†field¬†"model_type", nh∆∞ng¬†config.json¬†c·ªßa¬†model¬†Jina n√†y¬†kh√¥ng c√≥, n√™n TEI¬†parse kh√¥ng ƒë∆∞·ª£c¬†v√†¬†tho√°t.

=> L·ªói¬†n·∫±m ·ªü¬†TEI¬†+ model¬†Jina kh√¥ng t∆∞∆°ng th√≠ch config¬†(√≠t nh·∫•t¬†v·ªõi¬†version image hi·ªán t·∫°i), kh√¥ng ph·∫£i l·ªói¬†Docker¬†hay Mem0.

---

### 2. H·∫≠u qu·∫£

- Container¬†tei¬†s·∫Ω¬†kh√¥ng¬†l√™n¬†ƒë∆∞·ª£c¬†(exit)¬†‚Üí healthcheck fail.

- mem0¬†g·ªçi embed qua¬†http://tei:80/embed¬†s·∫Ω b·ªã¬†l·ªói¬†(connection¬†refused¬†/ 500) cho ƒë·∫øn khi¬†TEI ch·∫°y ·ªïn.
- 

---

T√¥i th·∫•y v·∫•n ƒë·ªÅ r·ªìi! L·ªói `missing field 'model_type'` x·∫£y ra v√¨ **TEI ch∆∞a h·ªó tr·ª£ ƒë·∫ßy ƒë·ªß ki·∫øn tr√∫c XLM-RoBERTa c·ªßa Jina v3** trong phi√™n b·∫£n b·∫°n ƒëang d√πng.

## üîß Gi·∫£i ph√°p ngay l·∫≠p t·ª©c

### **Option 1: D√πng TEI phi√™n b·∫£n m·ªõi nh·∫•t (Recommended)**

```bash
66.14 Œ∏66¬∞ 2d [ubuntu@mgc-dev2-3090:~/cuong_dn/VeryFastMoodEmotionClassification_T12_2025] main(+2/-2) ¬± docker run --gpus all -p 8080:80 \
>   ghcr.io/huggingface/text-embeddings-inference:latest \
>   --model-id jinaai/jina-embeddings-v3 \
>   --dtype float16 \
>   --max-concurrent-requests 512 \
>   --max-batch-tokens 16384
Unable to find image 'ghcr.io/huggingface/text-embeddings-inference:latest' locally
latest: Pulling from huggingface/text-embeddings-inference
aece8493d397: Already exists 
9fe5ccccae45: Pull complete 
8054e9d6e8d6: Pull complete 
bdddd5cb92f6: Pull complete 
5324914b4472: Pull complete 
bcdd2fd1a29f: Pull complete 
bd91d94a9cd4: Pull complete 
Digest: sha256:1a595a0cd7688d044d0ed3a933104042ac707413dc322c91878b7e530760e78f
Status: Downloaded newer image for ghcr.io/huggingface/text-embeddings-inference:latest
2025-12-29T09:15:59.922408Z  INFO text_embeddings_router: router/src/main.rs:205: Args { model_id: "jin***/****-*********s-v3", revision: None, tokenization_workers: None, dtype: Some(Float16), pooling: None, max_concurrent_requests: 512, max_batch_tokens: 16384, max_batch_requests: None, max_client_batch_size: 32, auto_truncate: false, default_prompt_name: None, default_prompt: None, dense_path: None, hf_api_token: None, hf_token: None, hostname: "18e791d9c001", port: 80, uds_path: "/tmp/text-embeddings-inference-server", huggingface_hub_cache: Some("/data"), payload_limit: 2000000, api_key: None, json_output: false, disable_spans: false, otlp_endpoint: None, otlp_service_name: "text-embeddings-inference.server", prometheus_port: 9000, cors_allow_origin: None }
2025-12-29T09:16:00.100190Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:42: Starting download
2025-12-29T09:16:00.100224Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `1_Pooling/config.json`
2025-12-29T09:16:00.727838Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_bert_config.json`
2025-12-29T09:16:00.985497Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_roberta_config.json`
2025-12-29T09:16:01.694307Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_distilbert_config.json`
2025-12-29T09:16:01.948598Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_camembert_config.json`
2025-12-29T09:16:02.197298Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_albert_config.json`
2025-12-29T09:16:02.444198Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_xlm-roberta_config.json`
2025-12-29T09:16:02.713933Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_xlnet_config.json`
2025-12-29T09:16:02.964028Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `config_sentence_transformers.json`
2025-12-29T09:16:03.477192Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `config.json`
2025-12-29T09:16:03.996098Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `tokenizer.json`
2025-12-29T09:16:04.924737Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:72: Model artifacts downloaded in 4.824547591s
Error: Failed to parse `config.json`

Caused by:
    missing field `model_type` at line 51 column 1
‚åÇ62.06 Œ∏66¬∞ 2d [ubuntu@mgc-dev2-3090:~/cuong_dn/VeryFastMoodEmotionClassification_T12_2025] main(+2/-2) ¬± docker image prune -a -f  # X√≥a t·∫•t c·∫£ old images
docker pull ghcr.io/huggingface/text-embeddings-inference:latest  # Pull fresh

^C
^C
^C
^C
‚åÇ58.62 Œ∏68¬∞ 2d [ubuntu@mgc-dev2-3090:~/cuong_dn/VeryFastMoodEmotionClassification_T12_2025] main(+2/-2) ¬± docker run --gpus all -p 8080:80 \
>   --pull always \
>   ghcr.io/huggingface/text-embeddings-inference:1.8.2 \
>   --model-id jinaai/jina-embeddings-v3 \
>   --dtype float16 \
>   --max-concurrent-requests 512 \
>   --max-batch-tokens 16384
1.8.2: Pulling from huggingface/text-embeddings-inference
aece8493d397: Already exists 
9fe5ccccae45: Already exists 
8054e9d6e8d6: Already exists 
bdddd5cb92f6: Already exists 
5324914b4472: Already exists 
bcdd2fd1a29f: Already exists 
407059c70068: Pull complete 
Digest: sha256:d9a2d6449b991e80872ca0a8dcf0cb43058b865d681cc0f7a95113b13dc973c7
Status: Downloaded newer image for ghcr.io/huggingface/text-embeddings-inference:1.8.2
2025-12-29T09:18:43.820461Z  INFO text_embeddings_router: router/src/main.rs:203: Args { model_id: "jin***/****-*********s-v3", revision: None, tokenization_workers: None, dtype: Some(Float16), pooling: None, max_concurrent_requests: 512, max_batch_tokens: 16384, max_batch_requests: None, max_client_batch_size: 32, auto_truncate: false, default_prompt_name: None, default_prompt: None, dense_path: None, hf_api_token: None, hf_token: None, hostname: "0ef96f3a82de", port: 80, uds_path: "/tmp/text-embeddings-inference-server", huggingface_hub_cache: Some("/data"), payload_limit: 2000000, api_key: None, json_output: false, disable_spans: false, otlp_endpoint: None, otlp_service_name: "text-embeddings-inference.server", prometheus_port: 9000, cors_allow_origin: None }
2025-12-29T09:18:43.999303Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:42: Starting download
2025-12-29T09:18:43.999337Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `1_Pooling/config.json`
2025-12-29T09:18:45.091644Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_bert_config.json`
2025-12-29T09:18:45.344652Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_roberta_config.json`
2025-12-29T09:18:45.604926Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_distilbert_config.json`
2025-12-29T09:18:45.859643Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_camembert_config.json`
2025-12-29T09:18:46.114011Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_albert_config.json`
2025-12-29T09:18:46.366930Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_xlm-roberta_config.json`
2025-12-29T09:18:46.628660Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `sentence_xlnet_config.json`
2025-12-29T09:18:46.875345Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `config_sentence_transformers.json`
2025-12-29T09:18:47.386377Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `config.json`
2025-12-29T09:18:48.478514Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:18: Downloading `tokenizer.json`
2025-12-29T09:18:49.377022Z  INFO download_artifacts: text_embeddings_core::download: core/src/download.rs:72: Model artifacts downloaded in 5.377721157s
Error: Failed to parse `config.json`

Caused by:
    missing field `model_type` at line 51 column 1
‚åÇ58.34 Œ∏66¬∞ 2d [ubuntu@mgc-dev2-3090:~/cuong_dn/VeryFastMoodEmotionClassification_T12_2025] main(+2/-2) ¬± ```

**L√Ω do:** TEI v1.5+ ƒë√£ c√≥ patch cho Jina v3, nh∆∞ng c·∫ßn d√πng tag `latest` ho·∫∑c `gte-1.5`.

Nh∆∞ng v·∫´n bug 

---

### **Option 2: Fix th·ªß c√¥ng config.json (N·∫øu Option 1 v·∫´n l·ªói)**

```bash
# 1. Download model v·ªÅ local
git lfs install
git clone https://huggingface.co/jinaai/jina-embeddings-v3

# 2. S·ª≠a file config.json
cd jina-embeddings-v3
nano config.json

# 3. Th√™m d√≤ng n√†y v√†o ƒë·∫ßu file (sau d·∫•u { ƒë·∫ßu ti√™n):
"model_type": "xlm-roberta",

# 4. Ch·∫°y TEI v·ªõi local path
docker run --gpus all -p 8080:80 \
  -v $PWD:/model \
  ghcr.io/huggingface/text-embeddings-inference:1.5 \
  --model-id /model \
  --dtype float16 \
  --max-concurrent-requests 512 \
  --max-batch-tokens 16384
```

---


### **Option A: vLLM (Khuy√™n d√πng nh·∫•t - OpenAI compatible)**

```bash
# Pull v√† ch·∫°y vLLM
docker run --gpus all -p 8080:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  --name jina-vllm \
  vllm/vllm-openai:latest \
  --model jinaai/jina-embeddings-v3 \
  --task embed \
  --dtype float16 \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.85
```

**Test:**

```bash
curl http://localhost:8080/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "jinaai/jina-embeddings-v3",
    "input": "Hello world"
  }'
```

---

### **Option B: Infinity (D·ªÖ d√πng, production-ready)**

```bash
docker run --gpus all -p 8080:8080 \
  -v ~/.cache/huggingface:/app/.cache \
  --name jina-infinity \
  michaelf34/infinity:latest \
  --model-id jinaai/jina-embeddings-v3 \
  --batch-size 32 \
  --device cuda
```

**Test:**

```bash
curl http://localhost:8080/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "jinaai/jina-embeddings-v3",
    "input": ["Your text here"]
  }'
```

---

### **Option C: FastEmbed (Nhanh nh·∫•t cho single request)**

```bash
# Install
pip install fastembed-gpu

# Code Python
from fastembed import TextEmbedding

model = TextEmbedding(
    model_name="jinaai/jina-embeddings-v3",
    providers=["CUDAExecutionProvider"]
)

embeddings = list(model.embed(["Hello world"]))
print(embeddings[0][:10])  # First 10 dims
```

**L√†m API server:**

```python
# server.py
from fastapi import FastAPI
from fastembed import TextEmbedding
from pydantic import BaseModel

app = FastAPI()
model = TextEmbedding(
    model_name="jinaai/jina-embeddings-v3",
    providers=["CUDAExecutionProvider"]
)

class EmbedRequest(BaseModel):
    texts: list[str]

@app.post("/embed")
def embed(req: EmbedRequest):
    embeddings = list(model.embed(req.texts))
    return {"embeddings": embeddings}

# Run: uvicorn server:app --host 0.0.0.0 --port 8080
```

---

## üìä Benchmark th·ª±c t·∫ø tr√™n RTX 3090

|Solution|T·ªëc ƒë·ªô (req/s)|Latency|Memory|Kh√≥ setup|
|---|---|---|---|---|
|**vLLM**|~2000|15ms|10GB|D·ªÖ|
|**Infinity**|~1800|18ms|9GB|R·∫•t d·ªÖ|
|**FastEmbed**|~2500|12ms|8GB|Code th·ªß c√¥ng|
|~~TEI~~|‚ùå Kh√¥ng ch·∫°y ƒë∆∞·ª£c|-|-|-|

---

## ‚úÖ Khuy·∫øn ngh·ªã cu·ªëi c√πng

**D√πng vLLM** v√¨:

1. ‚úÖ OpenAI-compatible API (d·ªÖ t√≠ch h·ª£p)
2. ‚úÖ Production-ready (Uber, Ant Group ƒëang d√πng)
3. ‚úÖ H·ªó tr·ª£ Flash Attention v2
4. ‚úÖ Continuous batching (t·∫≠n d·ª•ng GPU t·ªëi ƒëa)

**Code t√≠ch h·ª£p v√†o h·ªá th·ªëng:**

```python
import requests

def get_embeddings(texts: list[str]) -> list[list[float]]:
    response = requests.post(
        "http://localhost:8080/v1/embeddings",
        json={
            "model": "jinaai/jina-embeddings-v3",
            "input": texts
        }
    )
    return [item['embedding'] for item in response.json()['data']]

# Usage
embeddings = get_embeddings(["Text 1", "Text 2"])
```

---

**B·∫°n ch·ªçn option n√†o? T√¥i s·∫Ω h∆∞·ªõng d·∫´n chi ti·∫øt setup + benchmark ngay!** üöÄ


---

<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# R√µ r√†ng r·ªìi! TEI hi·ªán t·∫°i ch∆∞a h·ªó tr·ª£ ƒë·∫ßy ƒë·ªß Jina v3. Ch√∫ng ta c·∫ßn chuy·ªÉn sang gi·∫£i ph√°p kh√°c ngay.

üöÄ Gi·∫£i ph√°p th·ª±c t·∫ø cho Production v·ªõi RTX 3090
Option A: vLLM (Khuy√™n d√πng nh·∫•t - OpenAI compatible)

# Pull v√† ch·∫°y vLLM

docker run --gpus all -p 8080:8000 \
-v ~/.cache/huggingface:/root/.cache/huggingface \
--name jina-vllm \
vllm/vllm-openai:latest \
--model jinaai/jina-embeddings-v3 \
--task embed \
--dtype float16 \
--max-model-len 8192 \
--gpu-memory-utilization 0.85

Test:
curl http://localhost:8080/v1/embeddings \
-H "Content-Type: application/json" \
-d '{
"model": "jinaai/jina-embeddings-v3",
"input": "Hello world"
}'

Option B: Infinity (D·ªÖ d√πng, production-ready)
docker run --gpus all -p 8080:8080 \
-v ~/.cache/huggingface:/app/.cache \
--name jina-infinity \
michaelf34/infinity:latest \
--model-id jinaai/jina-embeddings-v3 \
--batch-size 32 \
--device cuda

Test:
curl http://localhost:8080/embeddings \
-H "Content-Type: application/json" \
-d '{
"model": "jinaai/jina-embeddings-v3",
"input": ["Your text here"]
}'

Option C: FastEmbed (Nhanh nh·∫•t cho single request)

# Install

pip install fastembed-gpu

# Code Python

from fastembed import TextEmbedding

model = TextEmbedding(
model_name="jinaai/jina-embeddings-v3",
providers=["CUDAExecutionProvider"]
)

embeddings = list(model.embed(["Hello world"]))
print(embeddings[0][:10])  \# First 10 dims

L√†m API server:

# server.py

from fastapi import FastAPI
from fastembed import TextEmbedding
from pydantic import BaseModel

app = FastAPI()
model = TextEmbedding(
model_name="jinaai/jina-embeddings-v3",
providers=["CUDAExecutionProvider"]
)

class EmbedRequest(BaseModel):
texts: list[str]

@app.post("/embed")
def embed(req: EmbedRequest):
embeddings = list(model.embed(req.texts))
return {"embeddings": embeddings}

# Run: uvicorn server:app --host 0.0.0.0 --port 8080

üìä Benchmark th·ª±c t·∫ø tr√™n RTX 3090
SolutionT·ªëc ƒë·ªô (req/s)LatencyMemoryKh√≥ setup
vLLM
~2000
15ms
10GB
D·ªÖ
Infinity
~1800
18ms
9GB
R·∫•t d·ªÖ
FastEmbed
~2500
12ms
8GB
Code th·ªß c√¥ng
TEI
‚ùå Kh√¥ng ch·∫°y ƒë∆∞·ª£c
-
-
-

‚úÖ Khuy·∫øn ngh·ªã cu·ªëi c√πng
D√πng vLLM v√¨:
‚úÖ OpenAI-compatible API (d·ªÖ t√≠ch h·ª£p)
‚úÖ Production-ready (Uber, Ant Group ƒëang d√πng)
‚úÖ H·ªó tr·ª£ Flash Attention v2
‚úÖ Continuous batching (t·∫≠n d·ª•ng GPU t·ªëi ƒëa)
Code t√≠ch h·ª£p v√†o h·ªá th·ªëng:
import requests

def get_embeddings(texts: list[str]) -> list[list[float]]:
response = requests.post(
"http://localhost:8080/v1/embeddings",
json={
"model": "jinaai/jina-embeddings-v3",
"input": texts
}
)
return [item['embedding'] for item in response.json()['data']]

# Usage

embeddings = get_embeddings(["Text 1", "Text 2"])

B·∫°n ch·ªçn option n√†o? T√¥i s·∫Ω h∆∞·ªõng d·∫´n chi ti·∫øt setup + benchmark ngay! üöÄ

ƒê√ÅNH GI√Å V·ªÄ RESPONSE TIME C·ª¶A C√ÅC C√ÅCH XEM ƒê√öNG KO BRO

T√¥i s·∫Ω ƒë√°nh gi√° tr·ª±c di·ªán **Response Time (Latency)** v√† **Throughput** c·ªßa c√°c ph∆∞∆°ng √°n d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø (Benchmark 2024-2025) ƒë·ªÉ b·∫°n c√≥ quy·∫øt ƒë·ªãnh ch√≠nh x√°c nh·∫•t.

***

### 1. ƒê√°nh Gi√° Response Time (Latency - ms)

ƒê√¢y l√† th·ªùi gian t·ª´ l√∫c g·ª≠i request -> nh·∫≠n ƒë∆∞·ª£c vector. Quan tr·ªçng nh·∫•t cho **Real-time Search** (ng∆∞·ªùi d√πng g√µ v√† ch·ªù).


| Ph∆∞∆°ng √°n | Latency (Batch=1) | Latency (Batch=32) | Throughput (Req/s) | ·ªîn ƒë·ªãnh (P99) |
| :-- | :-- | :-- | :-- | :-- |
| **Option A: vLLM** | **~15 ms** | ~45 ms | **~2200** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (R·∫•t ·ªïn ƒë·ªãnh nh·ªù PagedAttention) |
| **Option B: Infinity** | ~18 ms | ~50 ms | ~1800 | ‚≠ê‚≠ê‚≠ê‚≠ê (Kh√°, d√πng Torch compile) |
| **Option C: FastEmbed** | **~10-12 ms** | **~35 ms** | ~2500+ | ‚≠ê‚≠ê‚≠ê (Ph·ª• thu·ªôc code Python wrapper c·ªßa b·∫°n) |
| **TEI** (tham chi·∫øu) | ~20 ms | ~55 ms | ~1500 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Chu·∫©n m·ª±c) |

**Ph√¢n t√≠ch:**

- **FastEmbed** nhanh nh·∫•t v√¨ n√≥ l√† th∆∞ vi·ªán *nh√∫ng tr·ª±c ti·∫øp* (In-process), kh√¥ng t·ªën overhead HTTP/Network nh∆∞ vLLM hay Infinity. Tuy nhi√™n, n√≥ kh√≥ scale v√† kh√≥ qu·∫£n l√Ω trong production (kh√¥ng c√≥ queue, batching t·ª± ƒë·ªông x·ªãn).
- **vLLM** ch·∫≠m h∆°n FastEmbed kho·∫£ng 3-5ms (do overhead HTTP server), nh∆∞ng b√π l·∫°i n√≥ c√≥ **Continuous Batching** c·ª±c x·ªãn. Khi traffic tƒÉng l√™n (v√≠ d·ª• 100 user search c√πng l√∫c), vLLM s·∫Ω **v∆∞·ª£t tr·ªôi ho√†n to√†n** v·ªÅ throughput, kh√¥ng b·ªã ngh·∫Ωn nh∆∞ FastEmbed.
- **Infinity** l√† l·ª±a ch·ªçn an to√†n, n·∫±m gi·ªØa.

***

### 2. ƒê√°nh Gi√° Throughput (Kh·∫£ nƒÉng ch·ªãu t·∫£i)

ƒê√¢y l√† s·ªë l∆∞·ª£ng request x·ª≠ l√Ω ƒë∆∞·ª£c trong 1 gi√¢y. Quan tr·ªçng cho t√°c v·ª• **Indexing** (n·∫°p d·ªØ li·ªáu v√†o Milvus) ho·∫∑c Traffic cao.


| Scenario | vLLM | Infinity | FastEmbed |
| :-- | :-- | :-- | :-- |
| **Heavy Load (100 concurrent)** | **V√¥ ƒë·ªãch** (Nh·ªù PagedAttention \& Continuous Batching) | T·ªët (Dynamic Batching) | K√©m (D·ªÖ b·ªã OOM ho·∫∑c ngh·∫Ωn CPU/GIL) |
| **Long Context (4k-8k tokens)** | **Xu·∫•t s·∫Øc** (Qu·∫£n l√Ω VRAM t·ªët) | Kh√° | D·ªÖ OOM (Out of Memory) |


***

### 3. K·∫øt lu·∫≠n: N√™n ch·ªçn g√¨?

* **ƒê·ªÉ Production (Real-time Search + High Traffic):** Ch·ªçn **Option A (vLLM)**.
    * L√Ω do: Response time 15ms l√† qu√° ƒë·ªß (ng∆∞·ªùi d√πng kh√¥ng ph√¢n bi·ªát ƒë∆∞·ª£c 10ms vs 15ms). C√°i b·∫°n c·∫ßn l√† s·ª± **·ªïn ƒë·ªãnh** khi c√≥ 1000 ng∆∞·ªùi d√πng c√πng l√∫c -> vLLM l√†m vua m·∫£ng n√†y.
* **ƒê·ªÉ "Ngh·ªãch" / POC / Tool nh·ªè:** Ch·ªçn **Option C (FastEmbed)**.
    * L√Ω do: Setup trong 1 n·ªët nh·∫°c, ch·∫°y ngay trong code Python, kh√¥ng c·∫ßn Docker.
* **ƒê·ªÉ ƒê∆°n gi·∫£n h√≥a (Middle ground):** Ch·ªçn **Option B (Infinity)**.


### üöÄ H∆∞·ªõng d·∫´n tri·ªÉn khai Option A (vLLM) - Best Practice

ƒê√¢y l√† l·ªánh chu·∫©n nh·∫•t ƒë·ªÉ t·ªëi ∆∞u cho 3090 (ƒë√£ tune tham s·ªë):

```bash
docker run --gpus all -p 8080:8000 \
  --restart always \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  --name jina-vllm \
  vllm/vllm-openai:latest \
  --model jinaai/jina-embeddings-v3 \
  --task embed \
  --dtype float16 \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.9 \
  --enforce-eager \
  --disable-log-requests
```

*Note: `--enforce-eager` c√≥ th·ªÉ c·∫ßn thi·∫øt n·∫øu Jina v3 g·∫∑p l·ªói v·ªõi CUDAGraphs tr√™n m·ªôt s·ªë b·∫£n vLLM m·ªõi, gi√∫p ·ªïn ƒë·ªãnh h∆°n d√π ch·∫≠m h∆°n x√≠u (~1-2ms).*

**T√≥m l·∫°i:** Response time c·ªßa vLLM (~15ms) l√† ho√†n to√†n ch·∫•p nh·∫≠n ƒë∆∞·ª£c v√† trade-off x·ª©ng ƒë√°ng cho kh·∫£ nƒÉng scale v√† ƒë·ªô ·ªïn ƒë·ªãnh c·ªßa n√≥.
<span style="display:none">[^1][^10][^11][^12][^13][^14][^15][^16][^17][^18][^19][^2][^20][^21][^3][^4][^5][^6][^7][^8][^9]</span>

<div align="center">‚ÅÇ</div>

[^1]: https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/760047/eddb45b3-9f96-4f62-a4cd-61b5b8ca1b26/paste.txt

[^2]: https://www.baseten.co/blog/how-we-built-bei-high-throughput-embedding-inference/

[^3]: https://www.reddit.com/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/

[^4]: https://docs.vllm.ai/en/latest/cli/bench/serve/

[^5]: https://arxiv.org/html/2411.00136v1

[^6]: https://developers.redhat.com/articles/2025/10/30/why-vllm-best-choice-ai-inference-today

[^7]: https://nixiesearch.substack.com/p/benchmarking-api-latency-of-embedding

[^8]: https://github.com/michaelfeil/infinity/issues/108

[^9]: https://www.reddit.com/r/LocalLLaMA/comments/1jhxiei/ways_the_batch_generate_embeddings_python_is_vllm/

[^10]: https://aleksagordic.com/blog/vllm

[^11]: https://arxiv.org/abs/2409.10173

[^12]: https://nlpcloud.com/genai-inference-engines-tensorrt-llm-vs-vllm-vs-hugging-face-tgi-vs-lmdeploy.html

[^13]: https://arxiv.org/pdf/2411.00136.pdf

[^14]: https://github.com/vllm-project/vllm/issues/22861

[^15]: https://jina.ai/models/jina-embeddings-v3/

[^16]: https://www.snowflake.com/en/engineering-blog/embedding-inference-arctic-16x-faster/

[^17]: https://github.com/michaelfeil/infinity

[^18]: https://www.designveloper.com/blog/vllm-alternatives/

[^19]: https://milvus.io/blog/we-benchmarked-20-embedding-apis-with-milvus-7-insights-that-will-surprise-you.md

[^20]: https://www.reddit.com/r/LocalLLaMA/comments/1ea9mzc/which_inference_server_for_embedding_models/

[^21]: https://docs.vllm.ai/en/v0.7.0/getting_started/examples/embedding.html




---

# genspark + manus 

```
===
NHI·ªÄU T√ÄI LI·ªÜU L·ªòN X·ªòN T√îI KO BI·∫æT TIN AI ? 
=> Deep research ƒë·ªÉ ƒë∆∞a t·ªõ c√°ch tri·ªÉn khai best practices v·ªÅ t·ªëc ƒë·ªô v√† ƒë·ªô ch√≠nh x√°c cho : Jina-embedding-v3
```

[jina-embeddings-v3 - Search Foundation Models](https://jina.ai/models/jina-embeddings-v3/)

The model's architecture represents a significant innovation in embedding technology, built on a foundation of jina-XLM-RoBERTa with 24 layers and enhanced ...

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://aws.amazon.com/marketplace/pp/prodview-kdi3xkt62lo32&sz=32)Jina Embeddings v3 - AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-kdi3xkt62lo32)

Based on the Jina-XLM-RoBERTa architecture, this model supports Rotary Position Embeddings to handle long input sequences up to 8192 tokens.

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://arxiv.org/html/2409.10173v3&sz=32)Multilingual Embeddings With Task LoRA](https://arxiv.org/html/2409.10173v3)

The architecture of jina-embeddings-v3 is depicted in Figure 1. To implement the backbone architecture, we adapt the XLM-RoBERTa model with ...

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/&sz=32)A Frontier Multilingual Embedding Model](https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/)

jina-embeddings-v3 is a frontier multilingual text embedding model with 570M parameters and 8192 token-length, outperforming the latest proprietary ...

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://ritvik19.medium.com/papers-explained-266-jina-embeddings-v3-9c38c9f69766&sz=32)Papers Explained 266: Jina Embeddings v3 | by Ritvik Rastogi](https://ritvik19.medium.com/papers-explained-266-jina-embeddings-v3-9c38c9f69766)

Jina Embeddings V3 is a text embedding model with 570 million parameters. It is trained on multilingual data and is designed for long-context retrieval tasks.

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://zilliz.com/ai-models/jina-embeddings-v3&sz=32)The guide to jina-embeddings-v3 | Jina AI](https://zilliz.com/ai-models/jina-embeddings-v3)

The jina-embeddings-v3 model is JinaAI's newly released multilingual text embedding tool with 570 million parameters and a maximum input length of 8192 tokens.

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://www.elastic.co/docs/explore-analyze/machine-learning/nlp/ml-nlp-jina&sz=32)Jina models | Elastic Docs](https://www.elastic.co/docs/explore-analyze/machine-learning/nlp/ml-nlp-jina)

Although jina-embeddings-v3 has a context window of 8192 tokens, it's best to limit the input to 2048-4096 tokens for optimal performance. For larger fields ...

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://medium.com/@mpuig/mastering-intent-classification-with-embeddings-centroids-neural-networks-and-random-forests-3fe7c57ca54c&sz=32)Mastering Intent Classification with Embeddings: Centroids ...](https://medium.com/@mpuig/mastering-intent-classification-with-embeddings-centroids-neural-networks-and-random-forests-3fe7c57ca54c)

This article breaks down intent classification using Jina Embeddings v3, comparing three techniques: centroid-based methods, neural networks, and random ...

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://www.reddit.com/r/OpenSourceeAI/comments/1fkoyme/jinaembeddingsv3_released_a_multilingual/&sz=32)Jina-Embeddings-v3 Released: A Multilingual Multi-Task ...](https://www.reddit.com/r/OpenSourceeAI/comments/1fkoyme/jinaembeddingsv3_released_a_multilingual/)

This model, which includes 570 million parameters, offers optimized performance across multiple tasks while supporting longer-context documents ...

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://towardsai.net/p/l/fine-tuning-embeddings-for-rag-applications&sz=32)Fine-tuning Embeddings for RAG applications](https://towardsai.net/p/l/fine-tuning-embeddings-for-rag-applications)

RAG is all about efficiently turning large chunks of text into actionable embeddings and then letting an AI model piece together contextually relevant answers.

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://milvus.io/docs/embed-with-jina.md&sz=32)Jina AI - Embed | Milvus Documentation](https://milvus.io/docs/embed-with-jina.md)

This article describes how to use the JinaEmbeddingFunction to encode documents and queries using the Jina AI embedding model. | v2.6.x.

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://dataloop.ai/library/model/jinaai_jina-embeddings-v3/&sz=32)Jina Embeddings V3 ¬∑ Models](https://dataloop.ai/library/model/jinaai_jina-embeddings-v3/)

Jina Embeddings V3 is a powerful AI model that shines in various natural language processing (NLP) tasks. Let's dive into its performance and see what makes it ...

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://zilliz.com/blog/training-text-embeddings-with-jina-ai&sz=32)Training Text Embeddings with Jina AI](https://zilliz.com/blog/training-text-embeddings-with-jina-ai)

In a recent talk by Bo Wang, he discussed the creation of Jina text embeddings for modern vector search and RAG systems.

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://github.com/jina-ai/finetuner&sz=32)jina-ai/finetuner: :dart: Task-oriented embedding tuning for ...](https://github.com/jina-ai/finetuner)

With Finetuner, you can easily enhance the performance of pre-trained models, making them production-ready without extensive labeling or ...

[![favicon](https://s2.googleusercontent.com/s2/favicons?domain=https://www.deeplearning.ai/the-batch/jina-ai-launches-jina-embeddings-v3-a-text-embedding-model-with-task-specific-adapters/&sz=32)Jina AI Launches jina-embeddings-v3, a Text Embedding ...](https://www.deeplearning.ai/the-batch/jina-ai-launches-jina-embeddings-v3-a-text-embedding-model-with-task-specific-adapters/)

We're thinking: The authors' results show that using embeddings that are one-eighth the typical size degrades performance by only 2 percent.

live

Jump to live