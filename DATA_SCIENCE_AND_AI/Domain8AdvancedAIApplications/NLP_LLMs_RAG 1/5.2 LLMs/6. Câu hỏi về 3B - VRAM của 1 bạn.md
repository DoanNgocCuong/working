```
**Em đang gặp một case như này, mong các sếp﻿ mọi tư vấn giúp em**

- Hiện tại em đang làm đồ án về RAG chưa tới mức Agentic RAG chỉ dứng lại ở mức linear pipeline thì hiện tại API key em dùng bên nhà Groq (limit request) thì giờ em định host một model nhưng đang gặp một số vướng mắt kỹ thuật như này:

- Sever 16GB RAM system, 12GB VRAM GPU thì có thể host được một model với cấu hình như nào ạ ? (Em chỉ đang survey thì thấy nếu mà 3B para ở dạng AWQ INT4 thì hoàn toàn khả thi nhưng việc KV cache còn chừa lại rất ít thì việc system lúc này có thể khả thi được cho bao nhiêu req/s thì em không rõ)

- Việc sử dụng vLLM cấu hình hardcore cỡ nào để một sever như thế này chịu tải được khoảng 3,4 user cùng một lúc?

- Việc lựa chọn chuyển từ API sang self-host đánh đổi nhiều thứ như nào ạ mong mn giúp em nhìn được overview?
```

---

```
https://github.com/DoanNgocCuong/MiniProj_RAG3.2_RAG6_LegalChatbot_16032025

  

Em có thể tham khảo bài RAG trước anh làm.

  

1, Cỡ model 3B quantize xuống thì cấu hình : 16GB RAM system, 12GB VRAM GPU là hoàn toàn thoải mái (Vì 3B, FP8 có 3-6 VRAM thui)

2. Tải 3-4 user ko có gì phải lo

3. Chuyển từ sang self-host thì giúp điểm đồ án tăng lên. Còn API thì nhanh rùi nhưng ko chứng minh được kinh nghiệm làm việc với LLMs của em.

Groq thì siêu nhanh rồi, self-host thì response time bị lâu hơn, có khi gấp đôi
```