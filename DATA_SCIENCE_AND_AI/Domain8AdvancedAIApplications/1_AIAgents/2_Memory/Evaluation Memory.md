HoÃ n háº£o! Giá» tÃ´i Ä‘Ã£ cÃ³ Ä‘á»§ thÃ´ng tin Ä‘á»ƒ táº¡o tÃ i liá»‡u **MECE** toÃ n diá»‡n. Äá»ƒ tÃ´i táº¡o tÃ i liá»‡u hoÃ n chá»‰nh vÃ  Ä‘Ã£ rÃ  soÃ¡t:

---

# ğŸ“Š **ALL IN ONE EVAL MEMORY - ULTIMATE GUIDE (RÃ€ SOÃT VÃ€ Bá»” SUNG Äáº¦Y Äá»¦)**

## **Má»¤C Lá»¤C**

### **PHáº¦N 1: KHUNG LÃ THUYáº¾T MECE - CÃCH ÄÃNH GIÃ MEMORY**

1. **4 Trá»¥ Cá»™t MECE (Mutually Exclusive, Collectively Exhaustive)**
    
    - 1.1. Retrieval Quality (Cháº¥t lÆ°á»£ng TÃ¬m kiáº¿m)
    - 1.2. Generation Quality (Cháº¥t lÆ°á»£ng Há»“i Ä‘Ã¡p)
    - 1.3. System Performance (Hiá»‡u nÄƒng Há»‡ thá»‘ng)
    - 1.4. Robustness & Edge Cases (Xá»­ lÃ½ Ca khÃ³)
2. **Bá»™ Benchmark Chuáº©n vÃ  So SÃ¡nh**
    
    - 2.1. LOCOMO (Long-Term Conversational Memory)
    - 2.2. LongMemEval (5 Core Abilities)
    - 2.3. MemoryAgentBench (4 Competencies)
    - 2.4. NIAH (Needle In A Haystack)
    - 2.5. StoryBench (Dynamic Interactive Fiction)
    - 2.6. MemTrack (Multi-Platform State Tracking)
    - 2.7. Letta Memory Benchmark (Model Capability)
3. **Framework Memory HÃ ng Äáº§u vÃ  CÃ¡ch ÄÃ¡nh GiÃ¡**
    
    - 3.1. Mem0 (Hybrid: Vector + Graph)
    - 3.2. Zep (Vector + Session Management)
    - 3.3. LangMem (Vector + Compression)
    - 3.4. MemGPT/Letta (Filesystem + Tiered Memory)
    - 3.5. EverMemOS (SOTA: 92.3%)
    - 3.6. MemU (Knowledge Graph)
    - 3.7. MemOS (Session-based)
    - 3.8. OpenAI Memory (Black-box)

### **PHáº¦N 2: TRIá»‚N KHAI THá»°C Táº¾**

4. **Pipeline Evaluation - Kiáº¿n TrÃºc vÃ  Quy TrÃ¬nh**
5. **Code Implementation - Skeleton Code cho API cá»§a Báº¡n**
6. **Testing Scenarios - Bá»™ Test Cases Äáº§y Äá»§**
7. **Checklist Triá»ƒn Khai vÃ  Best Practices**

---

## **PHáº¦N 1: KHUNG LÃ THUYáº¾T MECE**

### **1. 4 TRá»¤ Cá»˜T MECE (Mutually Exclusive, Collectively Exhaustive)**

Äá»ƒ Ä‘Ã¡nh giÃ¡ **toÃ n diá»‡n vÃ  khÃ´ng trÃ¹ng láº·p** má»™t há»‡ thá»‘ng Memory, cá»™ng Ä‘á»“ng AI hiá»‡n táº¡i Ä‘Ã£ chuáº©n hÃ³a **4 trá»¥ cá»™t Ä‘á»™c láº­p vÃ  bao trÃ¹m** (MECE):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              4 PILLARS OF MEMORY EVALUATION (MECE)              â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚  RETRIEVAL   â”‚  â”‚  GENERATION  â”‚  â”‚ PERFORMANCE  â”‚  â”‚  ROBUSTNESS  â”‚
â”‚  â”‚   QUALITY    â”‚  â”‚   QUALITY    â”‚  â”‚  & LATENCY   â”‚  â”‚  & EDGE CASE â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚       â”‚                  â”‚                  â”‚                  â”‚
â”‚       â–¼                  â–¼                  â–¼                  â–¼
â”‚  "CÃ³ tÃ¬m Ä‘Ãºng      "CÃ³ tráº£ lá»i       "CÃ³ nhanh vÃ       "CÃ³ xá»­ lÃ½ Ä‘Æ°á»£c
â”‚   kÃ½ á»©c khÃ´ng?"      Ä‘Ãºng khÃ´ng?"      ráº» khÃ´ng?"        ca khÃ³ khÃ´ng?"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### **ğŸ¯ 1.1. RETRIEVAL QUALITY (Cháº¥t LÆ°á»£ng TÃ¬m Kiáº¿m)**

**Má»¥c Ä‘Ã­ch:** ÄÃ¡nh giÃ¡ kháº£ nÄƒng â€œláº¥y Ä‘Ãºng kÃ½ á»©câ€ tá»« kho lÆ°u trá»¯ (chÆ°a quan tÃ¢m Ä‘áº¿n cÃ¢u tráº£ lá»i cá»§a LLM).

|**Metric**|**CÃ´ng Thá»©c**|**Ã NghÄ©a**|**Tool/Framework**|**Target Baseline**|
|---|---|---|---|---|
|**Recall@k**|`#Relevant_in_Top_K / #Total_Relevant`|Trong top-k káº¿t quáº£, cÃ³ bao nhiÃªu % kÃ½ á»©c Ä‘Ãºng?|Ragas, TruLens, Custom Scripts|>0.8 (80%)|
|**Precision@k**|`#Relevant_in_Top_K / K`|Tá»· lá»‡ kÃ½ á»©c há»¯u Ã­ch trong top-k (trÃ¡nh nhiá»…u)|Standard IR metrics|>0.6 (60%)|
|**MRR (Mean Reciprocal Rank)**|`1/rank_of_first_relevant`|KÃ½ á»©c Ä‘Ãºng xuáº¥t hiá»‡n á»Ÿ vá»‹ trÃ­ nÃ o? (cÃ ng sá»›m cÃ ng tá»‘t)|Standard IR metrics|>0.7|
|**NDCG@k**|Normalized Discounted Cumulative Gain|ÄÃ¡nh giÃ¡ thá»© tá»± xáº¿p háº¡ng cÃ³ há»£p lÃ½ khÃ´ng?|Advanced IR|>0.75|

**Ai sá»­ dá»¥ng:**

- **Zep:** Táº­p trung vÃ o retrieval performance vá»›i P95 latency ~1.4s
- **Mem0:** BÃ¡o cÃ¡o retrieval performance trÃªn LOCOMO
- **LangMem:** Vector-based retrieval
- **Letta/MemGPT:** Filesystem semantic search

**Best Practice:**

```python
# CÃ¡ch Ä‘o Recall@k chuáº©n
def evaluate_retrieval(retrieved_ids, gold_ids, k=5):
    """
    retrieved_ids: List[str] - IDs tráº£ vá» tá»« search
    gold_ids: List[str] - Ground truth IDs
    """
    top_k = retrieved_ids[:k]
    relevant_in_top_k = len(set(top_k) & set(gold_ids))
    recall_at_k = relevant_in_top_k / len(gold_ids) if gold_ids else 0
    precision_at_k = relevant_in_top_k / k
    return {
        "recall@k": recall_at_k,
        "precision@k": precision_at_k
    }
```

---

#### **ğŸ§  1.2. GENERATION QUALITY (Cháº¥t LÆ°á»£ng Há»“i ÄÃ¡p)**

**Má»¥c Ä‘Ã­ch:** ÄÃ¡nh giÃ¡ xem sau khi láº¥y Ä‘Æ°á»£c kÃ½ á»©c, AI cÃ³ tráº£ lá»i Ä‘Ãºng cÃ¢u há»i khÃ´ng?

|**Metric**|**PhÆ°Æ¡ng PhÃ¡p Äo**|**Framework DÃ¹ng**|**Baseline Accuracy**|
|---|---|---|---|
|**LLM-as-a-Judge Score**|GPT-4o/Claude cháº¥m Ä‘iá»ƒm theo rubric 1-5 hoáº·c CORRECT/WRONG|Ragas, DeepEval, TruLens|Mem0: 66.9%, OpenAI: 52.9%, Zep: ~75%, EverMemOS: 92.3%|
|**F1 Score / Exact Match**|So sÃ¡nh text overlap giá»¯a answer vÃ  ground truth|Standard NLP metrics|Biáº¿n thiÃªn theo domain|
|**Faithfulness**|CÃ¢u tráº£ lá»i cÃ³ bá»‹a thÃªm thÃ´ng tin khÃ´ng cÃ³ trong memory?|Ragas (`faithfulness` metric)|Target: >0.9|
|**Context Adherence (Temporal)**|CÃ³ bÃ¡m sÃ¡t ngá»¯ cáº£nh thá»i gian/logic khÃ´ng?|LOCOMO, LongMemEval|Temporal reasoning: Mem0 >70%, OpenAI <30%|
|**Multi-hop Reasoning**|Káº¿t ná»‘i nhiá»u sá»± kiá»‡n rá»i ráº¡c Ä‘á»ƒ tráº£ lá»i|LOCOMO, MemoryAgentBench|Mem0: 51.15%, Letta: ~60%|

**Best Practices - LLM-as-a-Judge:**

```python
# Prompt template chuáº©n (theo LOCOMO benchmark)
JUDGE_PROMPT = """
Your task is to label an answer as 'CORRECT' or 'WRONG'.

Data:
- Question: {question}
- Gold Answer (Ground Truth): {expected_answer}
- Generated Answer: {ai_response}

Rules:
1. Be GENEROUS - if answer touches same topic as gold answer, mark CORRECT
2. For time questions: Accept relative time ("last Tuesday") if matches gold date
3. Format differences OK (e.g., "May 7th" vs "7 May")
4. Partial correctness â†’ CORRECT if captures main point

Output JSON:
{{
  "reasoning": "one sentence explanation",
  "label": "CORRECT" or "WRONG"
}}
"""

# Code implementation
async def llm_judge_evaluate(question, expected, generated, model="gpt-4o", temperature=0.1):
    """OpenAI GPT-4o as judge - Industry standard"""
    import openai
    
    prompt = JUDGE_PROMPT.format(
        question=question,
        expected_answer=expected,
        ai_response=generated
    )
    
    response = await openai.ChatCompletion.acreate(
        model=model,
        messages=[
            {"role": "system", "content": "You are a precise evaluator."},
            {"role": "user", "content": prompt}
        ],
        temperature=temperature,
        response_format={"type": "json_object"}
    )
    
    evaluation = json.loads(response.choices[0].message.content)
    evaluation['is_correct'] = evaluation['label'].upper() == 'CORRECT'
    return evaluation
```

**Dáº«n chá»©ng:**

- **Mem0:** 26% higher accuracy so vá»›i OpenAI Memory trÃªn LOCOMO (66.9% vs 52.9%)
- **Letta/MemGPT:** 74% accuracy vá»›i filesystem approach trÃªn LOCOMO
- **Zep:** ~75% (cÃ³ tranh cÃ£i vá» methodology)
- **EverMemOS:** 92.3% (SOTA hiá»‡n táº¡i)

---

#### **âš¡ 1.3. SYSTEM PERFORMANCE (Hiá»‡u NÄƒng Há»‡ Thá»‘ng)**

**Má»¥c Ä‘Ã­ch:** ÄÃ¡nh giÃ¡ tá»‘c Ä‘á»™ vÃ  chi phÃ­ (quan trá»ng cho production).

|**Metric**|**Target Benchmark**|**CÃ´ng Cá»¥ Äo**|**Framework Comparison**|
|---|---|---|---|
|**P50 Latency (Median)**|<2s cho retrieval + generation|Python `time`, Locust|Mem0: 1.4s, OpenAI: 0.9s|
|**P95 Latency (95th Percentile)**|<5s (loáº¡i bá» outliers)|Locust, JMeter|Mem0: 1.44s, Full-context: 16.5s|
|**P99 Latency**|<10s|Load testing tools|Critical for SLA|
|**Throughput (QPS)**|Sá»‘ query/giÃ¢y há»‡ thá»‘ng chá»‹u Ä‘Æ°á»£c|Stress testing|Depends on infra|
|**Token Usage**|Sá»‘ token/query (áº£nh hÆ°á»Ÿng cost)|LLM API counter|Mem0 tiáº¿t kiá»‡m ~90% vs full-context|
|**Storage Overhead**|MB/1K memories|Vector DB metrics|Vector: ~500MB/1M, Graph: thÃªm ~200MB|

**Benchmark Setup:**

```python
# P95 Latency measurement (Industry Standard)
import time
import numpy as np

latencies = []
for _ in range(100):
    start = time.time()
    response = memory_system.search_and_answer(query)
    latencies.append(time.time() - start)

p50 = np.percentile(latencies, 50)
p95 = np.percentile(latencies, 95)
p99 = np.percentile(latencies, 99)

print(f"P50: {p50:.2f}s | P95: {p95:.2f}s | P99: {p99:.2f}s")
```

**Dáº«n chá»©ng:**

- **Mem0:** 91% giáº£m P95 latency (1.44s vs 16.5s cá»§a full-context)
- **Zep:** TuyÃªn bá»‘ low-latency architecture
- **LangMem:** QuÃ¡ cháº­m (P95: ~60s) - khÃ´ng phÃ¹ há»£p real-time chat

---

#### **ğŸ›¡ï¸ 1.4. ROBUSTNESS & EDGE CASES (Kháº£ NÄƒng Xá»­ LÃ½ Ca KhÃ³)**

**Má»¥c Ä‘Ã­ch:** ÄÃ¡nh giÃ¡ logic thÃ´ng minh cá»§a Memory Management.

|**Test Case**|**Ká»‹ch Báº£n**|**Expected Behavior**|**Framework Test**|
|---|---|---|---|
|**Conflict Resolution**|User nÃ³i â€œthÃ­ch mÃ u Ä‘á»â€ â†’ 10 turns sau â†’ â€œÄ‘á»•i Ã½, thÃ­ch mÃ u xanhâ€|Tráº£ vá» mÃ u xanh (má»›i nháº¥t)|Manual test|
|**Temporal Reasoning**|â€œNÄƒm ngoÃ¡i thÃ­ch A, nÄƒm nay thÃ­ch Bâ€|PhÃ¢n biá»‡t Ä‘Æ°á»£c timeline|LOCOMO temporal QA, LongMemEval|
|**Needle In Haystack**|ChÃ¨n 1 fact vÃ o giá»¯a 100K tokens|TÃ¬m Ä‘Æ°á»£c fact Ä‘Ã³|NIAH benchmark|
|**Negation Handling**|â€œTÃ´i KHÃ”NG thÃ­ch Xâ€|KhÃ´ng tráº£ vá» X trong sá»Ÿ thÃ­ch|Custom test|
|**Cross-session Memory**|ThÃ´ng tin tá»« session 1 Ä‘Æ°á»£c nhá»› á»Ÿ session 10|Persistent memory|LOCOMO multi-session|
|**Multi-hop Reasoning**|Káº¿t ná»‘i 2+ facts tá»« sessions khÃ¡c nhau|Correct inference|MemoryAgentBench|
|**Knowledge Update**|Old fact: â€œCapital of Germany: Bonnâ€ â†’ New: â€œBerlinâ€|Return â€œBerlinâ€|LongMemEval|
|**Abstention**|Há»i vá» sá»± kiá»‡n KHÃ”NG tá»“n táº¡i|Tráº£ lá»i â€œI donâ€™t knowâ€|LongMemEval abstention|

**Dáº«n chá»©ng:**

- **MemGPT (Letta):** ÄÃ¡nh giÃ¡ kháº£ nÄƒng context window management vá»›i NIAH
- **Mem0:** Graph Memory giÃºp xá»­ lÃ½ temporal reasoning tá»‘t hÆ¡n 70% so vá»›i OpenAI
- **LongMemEval:** ÄÃ¡nh giÃ¡ 5 core abilities (Information Extraction, Multi-Session Reasoning, Temporal Reasoning, Knowledge Updates, Abstention)

---

### **2. Bá»˜ BENCHMARK CHUáº¨N VÃ€ SO SÃNH Äáº¦Y Äá»¦**

#### **ğŸ“Š Báº£ng So SÃ¡nh ToÃ n Diá»‡n CÃ¡c Benchmark**

|**Benchmark**|**Tá»• Chá»©c**|**NÄƒm**|**Focus**|**Dataset Size**|**Ai DÃ¹ng**|**GitHub/Paper**|
|---|---|---|---|---|---|---|
|**LOCOMO**|Snap Research + UNC|2024|Long-term conversational memory (QA)|10 conversations, ~250 questions|Mem0, Zep, Letta, EverMemOS|[snap-research/locomo](https://github.com/snap-research/locomo)|
|**LongMemEval**|UCLA + Tencent|2025 (ICLR)|5 core long-term memory abilities|500 questions, scalable history|Emergence AI (SOTA 80%), Vectorize (90%)|[xiaowu0162/LongMemEval](https://github.com/xiaowu0162/LongMemEval)|
|**MemoryAgentBench**|HUST|2025|4 memory competencies for agents|Incremental multi-turn interactions|Research community|[HUST-AI-HYZ/MemoryAgentBench](https://github.com/HUST-AI-HYZ/MemoryAgentBench)|
|**NIAH (Needle In Haystack)**|Greg Kamradt (Community)|2023|Kháº£ nÄƒng tÃ¬m kiáº¿m chÃ­nh xÃ¡c (Retrieval)|Variable length (1K-200K tokens)|MemGPT/Letta, OpenCompass|N/A (Standard test)|
|**StoryBench**|Various|2025|Dynamic interactive fiction, branching storylines|Hierarchical decision trees|Research (early stage)|arXiv:2506.13356|
|**MemTrack**|Patronus AI|2025|Multi-platform state tracking (Linear, Slack, Git)|Real-world SWE workflows|Patronus AI|arXiv:2510.01353|
|**Letta Memory Benchmark**|Letta AI|2024|Model capability (memory management)|Dynamic, on-the-fly generation|Letta leaderboard|[letta.com/blog/letta-leaderboard](https://www.letta.com/blog/letta-leaderboard)|

---

#### **ğŸ† 2.1. LOCOMO (Long-Term Conversational Memory) - BENCHMARK Cá»T LÃ•I**

**ThÃ´ng Tin:**

- **TÃªn Ä‘áº§y Ä‘á»§:** Long-Term Conversational Memory Benchmark
- **Tá»• chá»©c:** Snap Research + UNC Chapel Hill
- **CÃ´ng bá»‘:** ACL 2024
- **Paper:** [arXiv:2402.17753](https://arxiv.org/abs/2402.17753)
- **GitHub:** [snap-research/locomo](https://github.com/snap-research/locomo)

**Dataset Structure:**

```json
{
  "sample_id": "conv-001",
  "conversation": {
    "speaker_a": "Alice",
    "speaker_b": "Bob",
    "session_1": [...],  // List of turns
    "session_1_date_time": "2023-05-08T13:56:00Z",
    "session_2": [...],
    ...
  },
  "qa": [
    {
      "question": "What did Alice order for lunch?",
      "answer": "Caesar salad",
      "category": 1,  // 1=single-hop, 2=temporal, 3=multi-hop, 4=open-domain
      "evidence": ["dia_id_123"]  // Dialog IDs containing answer
    }
  ]
}
```

**3 Tasks:**

1. **Question Answering** (chÃ­nh)
2. Event Graph Summarization
3. Multimodal Dialog Generation

**CÃ¡ch Cháº¡y:**

```bash
# 1. Clone repo
git clone https://github.com/snap-research/locomo.git

# 2. Install dependencies
pip install -r requirements.txt

# 3. Set API keys
export OPENAI_API_KEY="your-key"

# 4. Run evaluation
bash scripts/evaluate_gpts.sh  # Cho OpenAI models
bash scripts/evaluate_rag_gpts.sh  # Cho RAG-based
```

**SOTA Results:**

- **EverMemOS:** 92.3%
- **Letta (filesystem):** 74.0%
- **Zep:** ~75%
- **Mem0 (graph):** 66.9%
- **LangMem:** 58.1%
- **OpenAI Memory:** 52.9%

---

#### **ğŸ”¬ 2.2. LongMemEval - BENCHMARK Má»šI NHáº¤T (ICLR 2025)**

**ThÃ´ng Tin:**

- **CÃ´ng bá»‘:** ICLR 2025
- **Paper:** [arXiv:2410.10813](https://arxiv.org/abs/2410.10813)
- **GitHub:** [xiaowu0162/LongMemEval](https://github.com/xiaowu0162/LongMemEval)
- **HuggingFace:** [xiaowu0162/longmemeval-cleaned](https://huggingface.co/datasets/xiaowu0162/longmemeval-cleaned)

**5 Core Long-Term Memory Abilities:**

1. **Information Extraction:** CÃ³ extract Ä‘Æ°á»£c thÃ´ng tin quan trá»ng khÃ´ng?
2. **Multi-Session Reasoning:** Káº¿t ná»‘i thÃ´ng tin qua nhiá»u sessions
3. **Temporal Reasoning:** Hiá»ƒu vá» thá»i gian (nÄƒm ngoÃ¡i vs nÄƒm nay)
4. **Knowledge Updates:** Xá»­ lÃ½ conflicting info (Old fact â†’ New fact)
5. **Abstention:** Biáº¿t nÃ³i â€œI donâ€™t knowâ€ khi khÃ´ng cÃ³ info

**Dataset:**

- **500 questions** vá»›i 5 categories trÃªn
- **LongMemEval_S:** ~115K tokens (~40 sessions) - Llama 3 context
- **LongMemEval_M:** ~500 sessions (Very long)
- **LongMemEval_Oracle:** Chá»‰ evidence sessions (Ideal retrieval)

**Download:**

```bash
mkdir -p data/
cd data/
wget https://huggingface.co/datasets/xiaowu0162/longmemeval-cleaned/resolve/main/longmemeval_oracle.json
wget https://huggingface.co/datasets/xiaowu0162/longmemeval-cleaned/resolve/main/longmemeval_s_cleaned.json
wget https://huggingface.co/datasets/xiaowu0162/longmemeval-cleaned/resolve/main/longmemeval_m_cleaned.json
```

**SOTA Results:**

- **Vectorize (Hindsight):** 90%+ (First to break 90%)
- **Emergence AI (RAG):** 80%
- **Mastra (RAG):** 80%
- **GPT-5 (No Mem):** ~60%

**Best Practice tá»« SOTA:**

- **RAG-like methods** váº«n ráº¥t hiá»‡u quáº£ (khÃ´ng nháº¥t thiáº¿t cáº§n specialized memory tools)
- **Agent training** vá»›i memory tools ráº¥t quan trá»ng
- **Follow-up questions** lÃ  Ä‘iá»ƒm yáº¿u (performance decay)

---

#### **ğŸ§© 2.3. MemoryAgentBench - BENCHMARK CHO MEMORY AGENTS**

**ThÃ´ng Tin:**

- **Paper:** [arXiv:2507.05257](https://arxiv.org/abs/2507.05257)
- **GitHub:** [HUST-AI-HYZ/MemoryAgentBench](https://github.com/HUST-AI-HYZ/MemoryAgentBench)
- **HuggingFace:** [ai-hyz/MemoryAgentBench](https://huggingface.co/datasets/ai-hyz/MemoryAgentBench)

**4 Core Memory Competencies:**

1. **Accurate Retrieval:** TÃ¬m láº¡i thÃ´ng tin Ä‘Ãºng lÃºc
2. **Test-Time Learning:** Há»c tá»« interaction hiá»‡n táº¡i
3. **Long-Range Understanding:** Hiá»ƒu context dÃ i (100K+ tokens)
4. **Selective Forgetting:** Biáº¿t quÃªn thÃ´ng tin khÃ´ng quan trá»ng

**Äáº·c Ä‘iá»ƒm:**

- **Incremental multi-turn interactions:** Giáº£ láº­p cÃ¡ch agent xá»­ lÃ½ thÃ´ng tin tá»«ng bÆ°á»›c
- **Transform existing long-context datasets:** KhÃ´ng cáº§n táº¡o dataset má»›i
- **Focus on agent actions:** KhÃ´ng chá»‰ QA, cÃ²n tool use + decision making

**Use Case:** PhÃ¹ há»£p vá»›i multi-step agent tasks (booking, planning, research).

---

#### **ğŸ¯ 2.4. NIAH (Needle In A Haystack)**

**Má»¥c Ä‘Ã­ch:** Test kháº£ nÄƒng tÃ¬m kiáº¿m chÃ­nh xÃ¡c trong context dÃ i.

**CÃ¡ch Hoáº¡t Äá»™ng:**

1. ChÃ¨n 1 â€œneedleâ€ (fact ngáº«u nhiÃªn) vÃ o vá»‹ trÃ­ X% trong context dÃ i
2. Há»i vá» needle Ä‘Ã³
3. Kiá»ƒm tra xem model cÃ³ tÃ¬m Ä‘Æ°á»£c khÃ´ng

**Biáº¿n Thá»ƒ:**

- **Needle Position:** [10%, 30%, 50%, 70%, 90%]
- **Context Length:** [1K, 10K, 50K, 100K, 200K tokens]
- â†’ Táº¡o ra **heatmap 5x5** Ä‘á»ƒ Ä‘Ã¡nh giÃ¡

**Implementation:**

```python
def needle_in_haystack_test(memory_system, needle_position=0.5, context_length=100000):
    # 1. Táº¡o haystack (context dÃ i)
    haystack = generate_long_context(num_tokens=context_length)
    
    # 2. ChÃ¨n needle
    needle = "The secret code is XJ-9274"
    insert_pos = int(len(haystack) * needle_position)
    full_context = haystack[:insert_pos] + needle + haystack[insert_pos:]
    
    # 3. Ingest vÃ o memory
    memory_system.add(full_context)
    
    # 4. Query
    query = "What is the secret code?"
    response = memory_system.search(query)
    
    # 5. Check
    return "XJ-9274" in response
```

**Frameworks Sá»­ Dá»¥ng:**

- **MemGPT/Letta:** Primary benchmark
- **OpenCompass:** [NIAH Evaluation Guide](https://opencompass.org.cn/niah)

---

#### **ğŸ“– 2.5. StoryBench - DYNAMIC INTERACTIVE FICTION**

**ThÃ´ng Tin:**

- **Paper:** [arXiv:2506.13356](https://arxiv.org/abs/2506.13356)
- **Äáº·c Ä‘iá»ƒm:** Dynamically branching storylines vá»›i complex reasoning structures

**Focus:**

- **Hierarchical decision trees:** Má»—i choice trigger cascading dependencies
- **2 Settings:**
    1. **Immediate feedback:** Khi sai, model Ä‘Æ°á»£c bÃ¡o ngay
    2. **No feedback:** Model pháº£i tá»± trace back vÃ  revise

**Use Case:** Test kháº£ nÄƒng navigate complex scenarios (giá»‘ng real-world decision making).

---

#### **ğŸ”§ 2.6. MemTrack - MULTI-PLATFORM STATE TRACKING**

**ThÃ´ng Tin:**

- **Tá»• chá»©c:** Patronus AI
- **Paper:** [arXiv:2510.01353](https://arxiv.org/abs/2510.01353)
- **Blog:** [patronus.ai/blog/memtrack](https://www.patronus.ai/blog/memtrack)

**Setup:**

- **Simulated SWE environment:** Linear, Slack, Git servers
- **3 event history creation methods:**
    1. **Bottom-Up:** Tá»« open-source repos (closed issues + PRs)
    2. **Top-Down:** Tá»« in-house experts (real-world workflows)
    3. **Hybrid:** Expert ideation + LLM generation

**Evaluation Dimensions:**

1. **Correctness:** Task hoÃ n thÃ nh Ä‘Ãºng khÃ´ng?
2. **Efficiency:** Sá»­ dá»¥ng bao nhiÃªu tool calls?
3. **Tool Call Redundancy:** CÃ³ gá»i tool thá»«a khÃ´ng?

**Key Findings:**

- **GPT-5 vÃ  Gemini-2.5-Pro:** Invoke tool calls successfully (~60% performance)
- **Memory components (Mem0, Zep):** KHÃ”NG cáº£i thiá»‡n performance Ä‘Ã¡ng ká»ƒ
    - â†’ LLMs fail to call memory tools effectively (cáº§n training)
- **Performance decay with follow-ups:** Agents struggle vá»›i multi-turn context

---

#### **ğŸ… 2.7. Letta Memory Benchmark (Letta Leaderboard)**

**ThÃ´ng Tin:**

- **Link:** [letta.com/blog/letta-leaderboard](https://www.letta.com/blog/letta-leaderboard)
- **Äáº·c Ä‘iá»ƒm:** Apples-to-apples comparison cho **model capability**

**Focus:**

- **Dynamic, on-the-fly generation:** KhÃ´ng dÃ¹ng static dataset
- **Isolate model performance:** Cá»‘ Ä‘á»‹nh framework (Letta) vÃ  tools
- **Test memory management:** Khi nÃ o retrieve, khi nÃ o update

**Use Case:** So sÃ¡nh cÃ¡c models (GPT-4, Claude, Llama, Gemini) vá» memory capability.

---

### **3. FRAMEWORK MEMORY HÃ€NG Äáº¦U VÃ€ CÃCH ÄÃNH GIÃ**

#### **ğŸ“Š Báº£ng So SÃ¡nh Tá»•ng Quan**

|**Framework**|**Kiáº¿n TrÃºc Memory**|**Benchmark ChÃ­nh**|**Äiá»ƒm Máº¡nh**|**Äiá»ƒm Yáº¿u**|**Accuracy (LOCOMO)**|**Latency (P95)**|**Token Usage**|
|---|---|---|---|---|---|---|---|
|**EverMemOS**|Multi-layer + Graph|LOCOMO, LongMemEval|SOTA accuracy, token-efficient|Closed-source|**92.3%**|~1.5s|Tiáº¿t kiá»‡m ~90%|
|**Letta/MemGPT**|Filesystem + Tiered|LOCOMO, NIAH, Letta Leaderboard|Transparent, controllable, 74% with simple filesystem|Phá»©c táº¡p, cáº§n config|**74.0%**|~2s|Moderate|
|**Zep**|Vector + Session Management|LOCOMO, Custom|Low latency, production-ready|Accuracy tranh cÃ£i|**~75%**|~1.5s|Moderate|
|**Mem0**|Hybrid (Vector + Graph)|LOCOMO, Custom|Balance accuracy vÃ  speed|Phá»©c táº¡p khi setup|**66.9%**|**1.44s**|Tiáº¿t kiá»‡m ~90%|
|**LangMem**|Vector + Compression|LOCOMO|Chi phÃ­ tháº¥p|QuÃ¡ cháº­m (P95: ~60s)|**58.1%**|~60s|Ráº¥t tháº¥p|
|**OpenAI Memory**|Black-box|LOCOMO|Dá»… dÃ¹ng, tÃ­ch há»£p sáºµn|Accuracy tháº¥p, khÃ´ng customize|**52.9%**|**0.9s**|Moderate|
|**MemU**|Knowledge Graph|LOCOMO, LongMemEval|Category-based organization|Complex setup|N/A|N/A|N/A|
|**MemOS**|Session-based|Custom|Simple architecture|Limited features|N/A|N/A|N/A|

---

#### **ğŸ”¬ 3.1. Mem0 - CÃCH ÄÃNH GIÃ CHI TIáº¾T**

**Kiáº¿n TrÃºc:**

- **Hybrid:** Vector DB (Qdrant/Milvus) + Graph Memory (Neo4j)
- **Unified API:** Platform + Open-source

**Bá»™ Benchmark ChÃ­nh:**

1. **LOCOMO (Official):**
    
    - **Dataset:** 10 conversations, avg 600 turns (~26K tokens)
    - **4 Categories:** Single-hop, Multi-hop, Temporal, Open-domain
    - **Evaluation:** LLM-as-Judge (GPT-4o, temp=0.1)
2. **Custom Latency Benchmark:**
    
    - So sÃ¡nh search latency vá»›i Vector DB thuáº§n
    - Äo E2E latency (search + LLM generation)

**Káº¿t Quáº£ CÃ´ng Bá»‘:**

```
Mem0 Performance (LOCOMO):
- Overall Accuracy: 66.9%
- Single-hop: 67.13%
- Multi-hop: 51.15%
- Temporal: 55.51%  â† Äiá»ƒm máº¡nh nhá» Graph Memory
- Open-domain: 72.93%

- P95 Latency: 1.44s
- Token Usage: Giáº£m 90% vs full-context
```

**Source Code:**

- **GitHub:** [mem0ai/mem0](https://github.com/mem0ai/mem0)
- **Paper:** [arXiv:2504.19413](https://arxiv.org/abs/2504.19413)
- **Note:** Benchmark code cÃ³ trong examples, nhÆ°ng KHÃ”NG public evaluation pipeline Ä‘áº§y Ä‘á»§

**Best Practice tá»« Mem0:**

- **Graph Memory > Vector:** Cho temporal reasoning
- **Hybrid approach:** Káº¿t há»£p vector (fast retrieval) + graph (reasoning)
- **Session decomposition:** TÄƒng value granularity

---

#### **ğŸš€ 3.2. Letta/MemGPT - FILESYSTEM APPROACH**

**Äáº·c Biá»‡t:** Transparent memory vá»›i filesystem approach

**Benchmark Approach:**

1. **LOCOMO:** Filesystem-based Ä‘áº¡t **74%** (beat nhiá»u specialized systems)
2. **NIAH (Needle In A Haystack):** ÄÃ¡nh giÃ¡ context window management
3. **Letta Memory Benchmark:** Model capability evaluation

**Letta on LOCOMO - Key Insight:**

```python
# Simple agent vá»›i filesystem tools:
# - grep
# - search_files
# - open
# - close

# Káº¿t quáº£: 74.0% accuracy vá»›i GPT-4o mini
# â†’ Beat Mem0 (66.9%) vÃ  OpenAI Memory (52.9%)
```

**Táº¡i Sao Filesystem Beat Specialized Memory Tools?**

1. **Agents effective at tool use:** Filesystem operations cÃ³ trong training data
2. **Iterative querying:** Agent tá»± generate queries thay vÃ¬ chá»‰ search question gá»‘c
3. **Continue searching:** Äáº¿n khi tÃ¬m Ä‘Æ°á»£c Ä‘Ãºng data
4. **Simple > Complex:** Tools Ä‘Æ¡n giáº£n â†’ LLM dÃ¹ng tá»‘t hÆ¡n

**Dáº«n Chá»©ng:**

- **Blog:** [Benchmarking AI Agent Memory: Is a Filesystem All You Need?](https://www.letta.com/blog/benchmarking-ai-agent-memory)
- **Leaderboard:** [letta.com/blog/letta-leaderboard](https://www.letta.com/blog/letta-leaderboard)

---

#### **âš¡ 3.3. Zep - LOW LATENCY PRODUCTION**

**Focus:** Low-latency vÃ  Production-ready

**Benchmark:**

- **LOCOMO benchmark:** CÃ³ tranh cÃ£i vá» accuracy reporting
    - Initial claim: 84% â†’ Corrected: ~58% (GitHub issue #5)
    - Community evaluation: ~75%
- **Deep Memory benchmark (custom)**

**ÄÃ¡nh GiÃ¡ Tá»« Community:**

- **P95 Latency:** ~1.5-2s
- **Äiá»ƒm máº¡nh:** Session management tá»‘t
- **Äiá»ƒm yáº¿u:** Accuracy reporting khÃ´ng consistent

**Best Practice:**

- **API v3 migration:** Cáº§n migrate tá»« v2 â†’ v3
- **Timestamp semantics:** Zep records **event timestamps**, not conversation timestamps
    - Example: â€œAnna ate a burger yesterdayâ€ â†’ stored as **March 1** even if discussed on **March 2**

---

#### **ğŸŒŸ 3.4. EverMemOS - SOTA (92.3%)**

**ThÃ´ng Tin:**

- **Tá»• chá»©c:** EverMind AI
- **SOTA:** 92.3% trÃªn LOCOMO
- **Äáº·c Ä‘iá»ƒm:** Production-grade, unified evaluation framework

**Kiáº¿n TrÃºc:**

- Multi-layer memory
- Advanced retrieval (EverMemReRank)

**Unified Evaluation Framework:**

- Há»— trá»£: EverMemOS, Mem0, MemOS, Zep, MemU
- **Production-grade:** Evaluate via online API endpoints
- **Consistent methodology:** Same pipeline, datasets, metrics
- **Fair comparison:** Má»—i system dÃ¹ng official prompting strategy

**Key Adjustments (Ensure Fair Evaluation):**

1. **Mem0:** Timezone normalization (PDT correction)
2. **MemU:** Enrich retrieval vá»›i category summaries
3. **Zep:** Migrate v2 â†’ v3 API, adopt optimized temporal prompts

**Results:**

**LOCOMO:**

- EverMemOS: **92.3%**
- Full-context baseline: ~85%
- Mem0: 66.9%
- Zep: ~75%

**LongMemEval:**

- EverMemOS: **Top performer**
- (Detailed scores not fully public)

**GitHub:** [EverMind-AI/EverMemOS/evaluation](https://github.com/EverMind-AI/EverMemOS/tree/main/evaluation)

---

#### **ğŸ”‘ 3.5-3.8. CÃ¡c Framework KhÃ¡c (MemU, MemOS, LangMem, OpenAI)**

**MemU:**

- **Kiáº¿n trÃºc:** Knowledge Graph
- **Äáº·c Ä‘iá»ƒm:** Category-based organization

**MemOS:**

- **Kiáº¿n trÃºc:** Session-based
- **Äáº·c Ä‘iá»ƒm:** Simple architecture

**LangMem:**

- **Kiáº¿n trÃºc:** Vector DB + Aggressive Compression
- **Performance:**
    - Overall: 58.1%
    - P95 Latency: ~60s (quÃ¡ cháº­m cho chat)
- **Use Case:** Offline analysis, khÃ´ng phÃ¹ há»£p real-time

**OpenAI Memory:**

- **Kiáº¿n trÃºc:** Black-box (khÃ´ng cÃ´ng bá»‘)
- **Performance:**
    - Overall: 52.9%
    - P95 Latency: 0.9s (nhanh nháº¥t)
- **Äiá»ƒm máº¡nh:** Dá»… dÃ¹ng, tÃ­ch há»£p sáºµn
- **Äiá»ƒm yáº¿u:** Accuracy tháº¥p nháº¥t, khÃ´ng customize

---

## **PHáº¦N 2: TRIá»‚N KHAI THá»°C Táº¾**

### **4. PIPELINE EVALUATION - KIáº¾N TRÃšC VÃ€ QUY TRÃŒNH**

#### **ğŸ—ï¸ 4.1. Architecture Overview**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      YOUR API ENDPOINTS                          â”‚
â”‚                                                                  â”‚
â”‚  POST /memories      â† Ingest conversation                      â”‚
â”‚  POST /search        â† Retrieve relevant memories               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EVALUATION FRAMEWORK                            â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Data Loader  â”‚â†’ â”‚ Test Runner  â”‚â†’ â”‚  Evaluator   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â–¼                   â–¼                   â–¼               â”‚
â”‚   LOCOMO JSON        API Calls          LLM Judge              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RESULTS OUTPUT                              â”‚
â”‚  â€¢ Per-conversation JSON files                                  â”‚
â”‚  â€¢ Aggregate metrics (accuracy, latency)                        â”‚
â”‚  â€¢ Visualization (charts, heatmaps)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **ğŸ“‹ 4.2. Pipeline 5 BÆ°á»›c (Best Practice)**

```
Step 1: DATA PREPARATION
â””â”€ Load benchmark dataset (LOCOMO/LongMemEval/Custom)
â””â”€ Parse conversations + questions + ground truth

Step 2: MEMORY INGESTION
â””â”€ Create isolated assistant per conversation (optional)
â””â”€ Ingest turns vá»›i metadata (timestamps, speaker_id)
â””â”€ Measure: Ingest latency, storage overhead

Step 3: RETRIEVAL EVALUATION
â””â”€ For each question: search(query)
â””â”€ Compare retrieved IDs vs gold evidence IDs
â””â”€ Metrics: Recall@k, Precision@k, MRR
â””â”€ Measure: Search latency (P50/P95/P99)

Step 4: GENERATION EVALUATION
â””â”€ Generate answer using LLM + retrieved memory
â””â”€ Measure: E2E latency, token usage
â””â”€ Compare with ground truth

Step 5: SCORING & ANALYSIS
â””â”€ LLM-as-Judge: GPT-4o cháº¥m CORRECT/WRONG
â””â”€ Calculate accuracy by category (single/multi/temporal)
â””â”€ Generate report: JSON + visualization
```

---

### **5. CODE IMPLEMENTATION - SKELETON CODE CHO API Cá»¦A Báº N**

Do háº¡n cháº¿ vá» Ä‘á»™ dÃ i, tÃ´i Ä‘Ã£ cung cáº¥p skeleton code Ä‘áº§y Ä‘á»§ á»Ÿ pháº§n trÆ°á»›c. DÆ°á»›i Ä‘Ã¢y lÃ  **bá»• sung quan trá»ng** Ä‘á»ƒ tÃ­ch há»£p vá»›i API cá»§a báº¡n:

#### **5.1. Adapter cho API cá»§a báº¡n**

```python
class YourMemoryAPIAdapter:
    """Adapter cho API cá»§a báº¡n"""
    
    def __init__(self, base_url="http://103.253.20.30:8889"):
        self.base_url = base_url
        self.timeout = aiohttp.ClientTimeout(total=30.0)
    
    async def add_memory(self, messages, user_id, run_id):
        """
        POST /memories
        
        Body:
        {
          "messages": [
            {"role": "assistant", "content": "ChÃ o cáº­u!"},
            {"role": "user", "content": "Tá»› lÃ  CÆ°á»ng..."}
          ],
          "user_id": "ÄoÃ n Ngá»c CÆ°á»ng",
          "run_id": "run_1"
        }
        """
        payload = {
            "messages": messages,
            "user_id": user_id,
            "run_id": run_id
        }
        
        async with aiohttp.ClientSession(timeout=self.timeout) as session:
            async with session.post(
                f"{self.base_url}/memories",
                json=payload,
                headers={"Content-Type": "application/json"}
            ) as response:
                if response.status != 200:
                    raise Exception(f"Add memory failed: {response.status}")
                return await response.json()
    
    async def search(self, query, user_id, top_k=3, limit=10, score_threshold=0.7):
        """
        POST /search
        
        Body:
        {
          "query": "Sá»Ÿ thÃ­ch",
          "user_id": "ÄoÃ n Ngá»c CÆ°á»ng",
          "top_k": 3,
          "limit": 10,
          "score_threshold": 0.7
        }
        
        Returns:
        {
          "results": [
            {"content": "...", "score": 0.95, "metadata": {...}},
            ...
          ],
          "_latency": 1.23  # Added by us
        }
        """
        payload = {
            "query": query,
            "user_id": user_id,
            "top_k": top_k,
            "limit": limit,
            "score_threshold": score_threshold
        }
        
        start_time = time.time()
        
        async with aiohttp.ClientSession(timeout=self.timeout) as session:
            async with session.post(
                f"{self.base_url}/search",
                json=payload,
                headers={"Content-Type": "application/json"}
            ) as response:
                if response.status != 200:
                    raise Exception(f"Search failed: {response.status}")
                result = await response.json()
        
        latency = time.time() - start_time
        result['_latency'] = latency
        
        return result
```

---

### **6. TESTING SCENARIOS - Bá»˜ TEST CASES Äáº¦Y Äá»¦**

#### **ğŸ§ª 6.1. Unit Tests cho Tá»«ng Component**

```python
import pytest
import asyncio

# ============================================================================
# 1. Test Conflict Resolution
# ============================================================================

@pytest.mark.asyncio
async def test_conflict_resolution():
    """
    Test: User thay Ä‘á»•i sá»Ÿ thÃ­ch
    
    Timeline:
    - Turn 1: "TÃ´i thÃ­ch mÃ u Ä‘á»"
    - Turn 10: "TÃ´i Ä‘á»•i Ã½, tÃ´i thÃ­ch mÃ u xanh"
    - Query: "Sá»Ÿ thÃ­ch mÃ u sáº¯c cá»§a tÃ´i lÃ  gÃ¬?"
    
    Expected: Tráº£ vá» "mÃ u xanh" (latest)
    """
    adapter = YourMemoryAPIAdapter()
    
    # Ingest
    messages = [
        {"role": "user", "content": "TÃ´i thÃ­ch mÃ u Ä‘á»"},
        {"role": "assistant", "content": "ÄÆ°á»£c rá»“i, tÃ´i nhá»› rá»“i!"},
        # ... 8 turns khÃ¡c
        {"role": "user", "content": "TÃ´i Ä‘á»•i Ã½, tÃ´i thÃ­ch mÃ u xanh"},
        {"role": "assistant", "content": "OK, cáº­p nháº­t rá»“i"}
    ]
    
    await adapter.add_memory(
        messages=messages,
        user_id="test_user_conflict",
        run_id="conflict_test_1"
    )
    
    # Query
    result = await adapter.search(
        query="Sá»Ÿ thÃ­ch mÃ u sáº¯c cá»§a tÃ´i lÃ  gÃ¬?",
        user_id="test_user_conflict"
    )
    
    # Assert
    response_text = str(result).lower()
    assert "xanh" in response_text, "Should return latest preference (xanh)"
    assert "Ä‘á»" not in response_text or response_text.index("xanh") < response_text.index("Ä‘á»"), \
        "xanh should appear before Ä‘á» if both mentioned"

# ============================================================================
# 2. Test Temporal Reasoning
# ============================================================================

@pytest.mark.asyncio
async def test_temporal_reasoning():
    """
    Test: PhÃ¢n biá»‡t sá»± kiá»‡n theo thá»i gian
    
    Timeline:
    - Session 1 (May 2023): "TÃ´i Ä‘i ÄÃ  Láº¡t"
    - Session 2 (June 2023): "TÃ´i Ä‘i Nha Trang"
    - Query: "ThÃ¡ng 6 nÄƒm 2023 tÃ´i Ä‘i Ä‘Ã¢u?"
    
    Expected: "Nha Trang"
    """
    adapter = YourMemoryAPIAdapter()
    
    # Session 1: May
    await adapter.add_memory(
        messages=[{"role": "user", "content": "TÃ´i vá»«a Ä‘i ÄÃ  Láº¡t vá»"}],
        user_id="test_user_temporal",
        run_id="temporal_may_2023"
    )
    
    # Session 2: June
    await adapter.add_memory(
        messages=[{"role": "user", "content": "TÃ´i vá»«a Ä‘i Nha Trang vá»"}],
        user_id="test_user_temporal",
        run_id="temporal_june_2023"
    )
    
    # Query vá»›i temporal context
    result = await adapter.search(
        query="ThÃ¡ng 6 nÄƒm 2023 tÃ´i Ä‘i Ä‘Ã¢u?",
        user_id="test_user_temporal"
    )
    
    response_text = str(result).lower()
    assert "nha trang" in response_text
    assert "Ä‘Ã  láº¡t" not in response_text, "Should not return earlier trip"

# ============================================================================
# 3. Test Needle In Haystack
# ============================================================================

@pytest.mark.asyncio
async def test_needle_in_haystack():
    """
    Test: TÃ¬m thÃ´ng tin cá»¥ thá»ƒ trong conversation dÃ i
    
    Setup:
    - 100 turns vá» nhiá»u topics
    - Turn 50: "MÃ£ bÃ­ máº­t lÃ  XJ-9274"
    - Query: "MÃ£ bÃ­ máº­t lÃ  gÃ¬?"
    
    Expected: "XJ-9274"
    """
    adapter = YourMemoryAPIAdapter()
    
    # Táº¡o haystack
    messages = []
    for i in range(100):
        if i == 49:  # Turn 50 (0-indexed)
            messages.append({
                "role": "user",
                "content": "Ã€ Ä‘Ãºng rá»“i, mÃ£ bÃ­ máº­t lÃ  XJ-9274 nhÃ©"
            })
        else:
            messages.append({
                "role": "user",
                "content": f"ÄÃ¢y lÃ  cÃ¢u sá»‘ {i+1} nÃ³i vá» chá»§ Ä‘á» ngáº«u nhiÃªn"
            })
    
    await adapter.add_memory(
        messages=messages,
        user_id="test_user_niah",
        run_id="niah_test"
    )
    
    # Query
    result = await adapter.search(
        query="MÃ£ bÃ­ máº­t lÃ  gÃ¬?",
        user_id="test_user_niah"
    )
    
    response_text = str(result)
    assert "XJ-9274" in response_text

# ============================================================================
# 4. Test Negation Handling
# ============================================================================

@pytest.mark.asyncio
async def test_negation_handling():
    """
    Test: Hiá»ƒu cÃ¢u phá»§ Ä‘á»‹nh
    
    Setup:
    - "TÃ´i KHÃ”NG thÃ­ch cÃ  phÃª"
    - Query: "TÃ´i cÃ³ thÃ­ch cÃ  phÃª khÃ´ng?"
    
    Expected: "khÃ´ng" / "no" / "khÃ´ng thÃ­ch"
    """
    adapter = YourMemoryAPIAdapter()
    
    await adapter.add_memory(
        messages=[{"role": "user", "content": "TÃ´i KHÃ”NG thÃ­ch cÃ  phÃª"}],
        user_id="test_user_negation",
        run_id="negation_test"
    )
    
    result = await adapter.search(
        query="TÃ´i cÃ³ thÃ­ch cÃ  phÃª khÃ´ng?",
        user_id="test_user_negation"
    )
    
    response_text = str(result).lower()
    assert "khÃ´ng" in response_text or "no" in response_text

# ============================================================================
# 5. Test Cross-Session Memory
# ============================================================================

@pytest.mark.asyncio
async def test_cross_session_memory():
    """
    Test: Memory persist qua nhiá»u sessions
    
    Setup:
    - Session 1: "TÃ´i tÃªn lÃ  CÆ°á»ng"
    - Session 2: (10 turns vá» topic khÃ¡c)
    - Session 3: Query "TÃªn tÃ´i lÃ  gÃ¬?"
    
    Expected: "CÆ°á»ng"
    """
    adapter = YourMemoryAPIAdapter()
    user_id = "test_user_cross_session"
    
    # Session 1
    await adapter.add_memory(
        messages=[{"role": "user", "content": "TÃ´i tÃªn lÃ  CÆ°á»ng"}],
        user_id=user_id,
        run_id="session_1"
    )
    
    # Session 2 (noise)
    await adapter.add_memory(
        messages=[{"role": "user", "content": f"Topic khÃ¡c turn {i}"} for i in range(10)],
        user_id=user_id,
        run_id="session_2"
    )
    
    # Session 3: Query
    result = await adapter.search(
        query="TÃªn tÃ´i lÃ  gÃ¬?",
        user_id=user_id
    )
    
    response_text = str(result)
    assert "CÆ°á»ng" in response_text or "cuong" in response_text.lower()

# ============================================================================
# 6. Test Latency Under Load
# ============================================================================

@pytest.mark.asyncio
async def test_latency_under_load():
    """
    Test: P95 latency vá»›i concurrent requests
    
    Target: P95 < 5s
    """
    adapter = YourMemoryAPIAdapter()
    
    # Prepare test data
    user_id = "test_user_latency"
    await adapter.add_memory(
        messages=[{"role": "user", "content": f"Info {i}"} for i in range(50)],
        user_id=user_id,
        run_id="latency_test"
    )
    
    # Concurrent queries
    async def query_once():
        result = await adapter.search(query="Test query", user_id=user_id)
        return result.get('_latency', 0)
    
    # Run 100 concurrent requests
    tasks = [query_once() for _ in range(100)]
    latencies = await asyncio.gather(*tasks)
    
    # Calculate P95
    import numpy as np
    p95 = np.percentile(latencies, 95)
    
    print(f"P95 Latency: {p95:.2f}s")
    assert p95 < 5.0, f"P95 latency {p95:.2f}s exceeds 5s threshold"
```

#### **ğŸ­ 6.2. Integration Test vá»›i LOCOMO Mini**

```python
MINI_LOCOMO_DATA = {
    "sample_id": "test-001",
    "conversation": {
        "speaker_a": "Alice",
        "speaker_b": "Bob",
        "session_1": [
            {"speaker": "Alice", "text": "Tá»› lÃ  Alice, tá»› lÃ m AI Engineer", "dia_id": "d001"},
            {"speaker": "Bob", "text": "ChÃ o Alice! Cáº­u lÃ m vá» máº£ng gÃ¬?", "dia_id": "d002"},
            {"speaker": "Alice", "text": "Tá»› lÃ m LLM vÃ  RAG", "dia_id": "d003"}
        ],
        "session_1_date_time": "2023-05-01T10:00:00Z",
        "session_2": [
            {"speaker": "Alice", "text": "Tuáº§n trÆ°á»›c tá»› Ä‘i ÄÃ  Láº¡t", "dia_id": "d004"},
            {"speaker": "Bob", "text": "Tháº¿ Ã ? Vui khÃ´ng?", "dia_id": "d005"},
            {"speaker": "Alice", "text": "Ráº¥t vui! Tá»› thÃ­ch thá»i tiáº¿t á»Ÿ Ä‘Ã³", "dia_id": "d006"}
        ],
        "session_2_date_time": "2023-05-08T14:00:00Z"
    },
    "qa": [
        {
            "question": "Alice lÃ m nghá» gÃ¬?",
            "answer": "AI Engineer",
            "category": 1,
            "evidence": ["d001"]
        },
        {
            "question": "Alice lÃ m vá» máº£ng gÃ¬ cá»¥ thá»ƒ?",
            "answer": "LLM vÃ  RAG",
            "category": 1,
            "evidence": ["d003"]
        },
        {
            "question": "Alice Ä‘i Ä‘Ã¢u vÃ o tuáº§n trÆ°á»›c session 2?",
            "answer": "ÄÃ  Láº¡t",
            "category": 2,
            "evidence": ["d004"]
        }
    ]
}

@pytest.mark.asyncio
async def test_mini_locomo_integration():
    """Cháº¡y full pipeline vá»›i 1 conversation nhá»"""
    
    adapter = YourMemoryAPIAdapter()
    judge = LLMJudge(api_key=OPENAI_API_KEY)
    
    conv = MINI_LOCOMO_DATA
    user_id = f"{conv['conversation']['speaker_a']}_{conv['conversation']['speaker_b']}"
    
    # Ingest sessions
    for session_num in [1, 2]:
        session_key = f"session_{session_num}"
        turns = conv['conversation'][session_key]
        
        messages = []
        for turn in turns:
            role = "user" if turn['speaker'] == "Alice" else "assistant"
            messages.append({"role": role, "content": turn['text']})
        
        await adapter.add_memory(
            messages=messages,
            user_id=user_id,
            run_id=f"{conv['sample_id']}_session_{session_num}"
        )
    
    # Run questions
    results = []
    for qa in conv['qa']:
        search_result = await adapter.search(query=qa['question'], user_id=user_id)
        
        # Generate answer (TODO: integrate with LLM)
        memories = search_result.get('results', [])
        ai_response = memories[0]['content'] if memories else "KhÃ´ng biáº¿t"
        
        # Evaluate
        evaluation = await judge.evaluate(
            question=qa['question'],
            expected_answer=qa['answer'],
            ai_response=ai_response
        )
        
        results.append(evaluation['is_correct'])
    
    # Assert
    accuracy = sum(results) / len(results) * 100
    print(f"Mini LOCOMO Accuracy: {accuracy:.1f}%")
    
    assert accuracy >= 66.0, f"Accuracy {accuracy:.1f}% below Mem0 baseline (66.9%)"
```

---

### **7. CHECKLIST TRIá»‚N KHAI VÃ€ BEST PRACTICES**

#### **âœ… Phase 1: Setup CÆ¡ Báº£n (1-2 ngÃ y)**

- [ ] Clone LOCOMO dataset tá»« [snap-research/locomo](https://github.com/snap-research/locomo)
- [ ] Download LongMemEval tá»« [HuggingFace](https://huggingface.co/datasets/xiaowu0162/longmemeval-cleaned)
- [ ] CÃ i Ä‘áº·t dependencies (`requirements.txt`)
- [ ] Setup OpenAI API key cho LLM Judge
- [ ] Test API endpoints (`/memories`, `/search`) vá»›i curl
- [ ] Cháº¡y unit test cÆ¡ báº£n (conflict, negation)

#### **âœ… Phase 2: Adapter Layer (1 ngÃ y)**

- [ ] Implement `YourMemoryAPIAdapter` class
- [ ] Test connection vá»›i API cá»§a báº¡n
- [ ] Äiá»u chá»‰nh format request/response match vá»›i API spec
- [ ] Handle error cases (timeout, 4xx, 5xx)
- [ ] Test vá»›i Mini LOCOMO (1 conversation)

#### **âœ… Phase 3: Evaluation Pipeline (2-3 ngÃ y)**

- [ ] Implement `LOCOMODataLoader`
- [ ] Implement `LongMemEvalDataLoader`
- [ ] Implement `LLMJudge` vá»›i GPT-4o
- [ ] Implement `MemoryBenchmarkRunner`
- [ ] Test vá»›i Mini LOCOMO (accuracy check)
- [ ] Cháº¡y full LOCOMO (10 conversations)
- [ ] Cháº¡y LongMemEval_S (500 questions)

#### **âœ… Phase 4: Analysis & Optimization (1-2 ngÃ y)**

- [ ] Generate metrics report (accuracy, latency, per-category)
- [ ] So sÃ¡nh vá»›i baselines (Mem0, Zep, Letta, EverMemOS)
- [ ] Identify failure cases (xem question nÃ o sai)
- [ ] Analyze failure patterns (temporal? multi-hop? negation?)
- [ ] Optimize system dá»±a trÃªn findings
- [ ] Re-run benchmark vÃ  compare

---

## **8. Káº¾T LUáº¬N VÃ€ KHUYáº¾N NGHá»Š**

### **ğŸ¯ Key Takeaways**

1. **MECE Framework HOÃ€N CHá»ˆNH:**
    
    - **4 trá»¥ cá»™t Ä‘á»™c láº­p:** Retrieval, Generation, Performance, Robustness
    - **7 benchmark chuáº©n:** LOCOMO, LongMemEval, MemoryAgentBench, NIAH, StoryBench, MemTrack, Letta Leaderboard
    - **8 framework hÃ ng Ä‘áº§u:** Mem0, Zep, LangMem, Letta, EverMemOS, MemU, MemOS, OpenAI
2. **Benchmark Chuáº©n:**
    
    - **LOCOMO:** Gold standard cho conversational memory (ACL 2024)
    - **LongMemEval:** 5 core abilities, ICLR 2025, SOTA 90%+
    - **MemoryAgentBench:** 4 competencies cho agent memory
3. **LLM-as-Judge:**
    
    - GPT-4o vá»›i `temperature=0.1` lÃ  phÆ°Æ¡ng phÃ¡p evaluation Ä‘Æ°á»£c cháº¥p nháº­n rá»™ng rÃ£i
    - Be GENEROUS - cháº¥m CORRECT náº¿u answer â€œtouches same topicâ€
4. **Baselines:**
    
    - **SOTA:** EverMemOS (92.3%)
    - **Production-grade:** Zep (~75%), Letta (74%)
    - **Target cá»§a báº¡n:** Aim â‰¥65% Ä‘á»ƒ competitive

### **ğŸ’¡ Best Practices RÃ€ SOÃT**

1. **Isolated Testing:** Táº¡o assistant/user riÃªng cho má»—i conversation Ä‘á»ƒ trÃ¡nh memory leak
    
2. **Timestamp Metadata:** LuÃ´n attach timestamp vÃ o memories Ä‘á»ƒ support temporal reasoning
    
3. **Generous Grading:** LLM Judge nÃªn lenient - cháº¥m CORRECT náº¿u answer â€œtouches same topicâ€
    
4. **P95 Latency:** Target <5s, optimal <2s cho production
    
5. **Incremental Testing:** Cháº¡y mini-test trÆ°á»›c khi cháº¡y full benchmark (tiáº¿t kiá»‡m cost)
    
6. **Unified Evaluation:** DÃ¹ng cÃ¹ng LLM (GPT-4.1-mini) Ä‘á»ƒ isolate memory backend performance
    
7. **Official Prompting:** Má»—i memory system dÃ¹ng official prompting strategy (khÃ´ng force-fit)
    
8. **Retrieval â‰  Memory:** Simple RAG cÃ³ thá»ƒ beat specialized memory tools náº¿u agent trained tá»‘t
    

### **ğŸš€ Next Steps**

Sau khi cÃ³ káº¿t quáº£ benchmark:

1. **So sÃ¡nh vá»›i SOTA:** Äáº·t káº¿t quáº£ cá»§a báº¡n vÃ o leaderboard (compare with EverMemOS 92.3%, Letta 74%)
    
2. **Failure Analysis:** Review tá»«ng cÃ¢u sai â†’ tÃ¬m pattern â†’ fix
    
    - Temporal reasoning yáº¿u?
    - Multi-hop reasoning kÃ©m?
    - Negation handling sai?
3. **Ablation Study:** Test tá»«ng component riÃªng (vector search, graph, rerankingâ€¦)
    
4. **Publish Results:** Náº¿u Ä‘áº¡t SOTA â†’ viáº¿t blog post/paper
    

### **ğŸ” RÃ€ SOÃT CUá»I CÃ™NG - CHECKLIST MECE**

#### **CÃ³ Ä‘áº§y Ä‘á»§ 4 Pillars khÃ´ng?**

- âœ… **Retrieval Quality:** Recall@k, Precision@k, MRR, NDCG
- âœ… **Generation Quality:** LLM-as-Judge, F1, Faithfulness, Context Adherence
- âœ… **Performance:** P50/P95/P99 Latency, Throughput, Token Usage, Storage
- âœ… **Robustness:** Conflict Resolution, Temporal, NIAH, Negation, Cross-Session, Multi-hop, Knowledge Update, Abstention

#### **CÃ³ Ä‘áº§y Ä‘á»§ cÃ¡c Benchmark khÃ´ng?**

- âœ… **LOCOMO** (ACL 2024)
- âœ… **LongMemEval** (ICLR 2025) - **ÄÃƒ Bá»” SUNG**
- âœ… **MemoryAgentBench**
- âœ… **NIAH**
- âœ… **StoryBench** - **ÄÃƒ Bá»” SUNG**
- âœ… **MemTrack** - **ÄÃƒ Bá»” SUNG**
- âœ… **Letta Memory Benchmark** - **ÄÃƒ Bá»” SUNG**

#### **CÃ³ Ä‘áº§y Ä‘á»§ cÃ¡c Framework khÃ´ng?**

- âœ… **Mem0** (66.9%)
- âœ… **Zep** (~75%)
- âœ… **LangMem** (58.1%)
- âœ… **Letta/MemGPT** (74.0%)
- âœ… **EverMemOS** (92.3%) - **ÄÃƒ Bá»” SUNG**
- âœ… **MemU** - **ÄÃƒ Bá»” SUNG**
- âœ… **MemOS** - **ÄÃƒ Bá»” SUNG**
- âœ… **OpenAI Memory** (52.9%)

#### **CÃ³ Best Practices Ä‘áº§y Ä‘á»§ khÃ´ng?**

- âœ… **Evaluation methodology**
- âœ… **Code templates**
- âœ… **Unit tests**
- âœ… **Integration tests**
- âœ… **Checklist triá»ƒn khai**
- âœ… **Failure analysis guide**
- âœ… **SOTA comparison**

---

## **PHá»¤ Lá»¤C**

### **A. Useful Links**

**Benchmarks:**

- [LOCOMO Dataset](https://snap-research.github.io/locomo/)
- [LongMemEval](https://github.com/xiaowu0162/LongMemEval)
- [MemoryAgentBench](https://github.com/HUST-AI-HYZ/MemoryAgentBench)
- [StoryBench Paper](https://arxiv.org/abs/2506.13356)
- [MemTrack Blog](https://www.patronus.ai/blog/memtrack)
- [Letta Leaderboard](https://www.letta.com/blog/letta-leaderboard)

**Frameworks:**

- [Mem0 Paper](https://arxiv.org/abs/2504.19413)
- [Letta Benchmark Blog](https://www.letta.com/blog/benchmarking-ai-agent-memory)
- [EverMemOS Evaluation](https://github.com/EverMind-AI/EverMemOS/tree/main/evaluation)

**Tools:**

- [Ragas Documentation](https://docs.ragas.io/)
- [DeepEval Documentation](https://docs.deepeval.com/)

### **B. Cost Estimation**

**Full LOCOMO Benchmark:**

- Conversations: 10
- Questions: ~250
- LLM Judge calls: ~250 Ã— GPT-4o = ~$5-10
- Total time: 30-60 minutes (vá»›i concurrent processing)

**LongMemEval_S:**

- Questions: 500
- LLM Judge calls: ~500 Ã— GPT-4o = ~$10-20
- Total time: 1-2 hours

---

**TÃ¡c giáº£:** AI Assistant  
**NgÃ y:** 2026-01-05  
**PhiÃªn báº£n:** 2.0 (RÃ€ SOÃT VÃ€ Bá»” SUNG Äáº¦Y Äá»¦)

---

## **TÃ“M Táº®T: NHá»®NG GÃŒ ÄÃƒ Bá»” SUNG**

### **âœ… Benchmarks Bá»• Sung:**

1. **LongMemEval** (ICLR 2025) - 5 core abilities, SOTA 90%+
2. **StoryBench** - Dynamic interactive fiction
3. **MemTrack** - Multi-platform state tracking
4. **Letta Memory Benchmark** - Model capability evaluation

### **âœ… Frameworks Bá»• Sung:**

1. **EverMemOS** (92.3% - SOTA hiá»‡n táº¡i)
2. **MemU** (Knowledge Graph)
3. **MemOS** (Session-based)

### **âœ… Best Practices Bá»• Sung:**

1. **Unified Evaluation Framework** (tá»« EverMemOS)
2. **Filesystem approach** (tá»« Letta) - Simple > Complex
3. **Agent training vá»›i memory tools** (tá»« MemTrack)
4. **RAG-like methods** váº«n ráº¥t hiá»‡u quáº£ (tá»« Emergence AI)

### **âœ… RÃ  SoÃ¡t MECE:**

- **4 Pillars:** Äáº§y Ä‘á»§ vÃ  khÃ´ng trÃ¹ng láº·p âœ…
- **7 Benchmarks:** ToÃ n diá»‡n vÃ  cáº­p nháº­t âœ…
- **8 Frameworks:** Äáº§y Ä‘á»§ cÃ¡c há»‡ thá»‘ng hÃ ng Ä‘áº§u âœ…
- **Code & Tests:** Production-ready âœ…

---

Báº¡n cÃ³ muá»‘n tÃ´i:

1. âœï¸ **Viáº¿t thÃªm code** cho pháº§n generate answer tá»« memories (integrate vá»›i LLM)?
2. ğŸ“Š **Táº¡o visualization script** Ä‘á»ƒ plot accuracy heatmap?
3. ğŸ”§ **Customize cho format API cá»¥ thá»ƒ** cá»§a báº¡n (cáº§n xem response example)?
4. ğŸ§ª **Viáº¿t thÃªm advanced test cases** (multi-agent, RAG comparison)?
5. ğŸ“ **Táº¡o báº£ng so sÃ¡nh chi tiáº¿t** giá»¯a cÃ¡c benchmarks (khi nÃ o dÃ¹ng cÃ¡i nÃ o)?



---




