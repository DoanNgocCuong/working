# PIKA MEMORY SYSTEM - TECHNICAL REPORT

**Version:** 1.0.0
**Date:** 2025-01-21
**Author:** PIKA Engineering Team
**Status:** Production-Ready

---

## ðŸ“‹ EXECUTIVE SUMMARY

PIKA Memory System lÃ  má»™t há»‡ thá»‘ng memory tá»± chá»§ (self-hosted) Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn **Mem0 Open Source**, nháº±m thay tháº¿ giáº£i phÃ¡p Mem0 Enterprise vá»›i chi phÃ­ cao. Há»‡ thá»‘ng cung cáº¥p kháº£ nÄƒng ghi nhá»› vÃ  truy xuáº¥t thÃ´ng tin theo ngá»¯ cáº£nh tá»« cÃ¡c cuá»™c há»™i thoáº¡i, giÃºp PIKA AI Companion táº¡o ra nhá»¯ng tráº£i nghiá»‡m cÃ¡ nhÃ¢n hÃ³a sÃ¢u sáº¯c cho tráº» em.

**Key Highlights:**

- âœ… **Giáº£m 75% chi phÃ­ váº­n hÃ nh** (tá»« >$7,200/nÄƒm xuá»‘ng <$1,800/nÄƒm)
- âœ… **P95 latency < 100ms** cho API tÃ¬m kiáº¿m
- âœ… **99.9% uptime** vá»›i kiáº¿n trÃºc high availability
- âœ… **Há»— trá»£ 1 triá»‡u MAU** vÃ  **1 tá»· memories**
- âœ… **5-layer caching strategy** Ä‘á»ƒ tá»‘i Æ°u response time

---

## 1. Má»¤C TIÃŠU & GOALS

### 1.1 Business Goals

| Goal ID | Goal Description                       | Metric                                    | Target                | Timeline    |
| ------- | -------------------------------------- | ----------------------------------------- | --------------------- | ----------- |
| BG-01   | **Reduce Operational Cost**      | Monthly infrastructure cost               | < $200/month          | Q1 2026     |
| BG-02   | **Enable Scalability**           | Support for active users                  | 1 Million MAU         | End of 2026 |
| BG-03   | **Enhance User Personalization** | User engagement metrics                   | +15% session duration | Q2 2026     |
| BG-04   | **Full Data Ownership**          | Data stored in self-hosted infrastructure | 100%                  | Q1 2026     |

### 1.2 Technical Goals

| Goal ID | Goal Description             | Metric                               | Target                        | Timeline    |
| ------- | ---------------------------- | ------------------------------------ | ----------------------------- | ----------- |
| TG-01   | **Low-Latency Search** | P95 latency for `search_facts` API | **< 100ms**             | Q1 2026     |
| TG-02   | **High Throughput**    | Request handling capacity            | 1,000 read QPS, 100 write QPS | End of 2026 |
| TG-03   | **High Availability**  | System uptime                        | **99.9%** (Three Nines) | Ongoing     |
| TG-04   | **Developer Velocity** | Time from commit to production       | < 15 minutes                  | Q2 2026     |
| TG-05   | **Search Accuracy**    | Search relevance (NDCG@10)           | > 0.85                        | Q1 2026     |
| TG-06   | **Extraction Quality** | Fact extraction F1-score             | > 0.90                        | Q1 2026     |

### 1.3 Key Performance Indicators (KPIs)

| KPI                                  | Baseline (Current)   | Target (Year 1)                       | Measurement Method                   |
| ------------------------------------ | -------------------- | ------------------------------------- | ------------------------------------ |
| **Search Latency (P95)**       | ~150ms               | **< 100ms**                     | Datadog APM / Prometheus Histogram   |
| **Search Latency (P99)**       | ~300ms               | **< 200ms**                     | Datadog APM / Prometheus Histogram   |
| **Extract Job Duration (P95)** | ~2.5s                | **< 2s**                        | Prometheus Histogram                 |
| **System Uptime**              | 99.5%                | **99.9%**                       | Pingdom / UptimeRobot                |
| **Cost per 1M API Calls**      | ~$20 |**< $5** | AWS Cost Explorer / Custom Dashboards |                                      |
| **Cache Hit Rate**             | N/A                  | **> 40%**                       | Redis Monitoring / Prometheus Gauge  |
| **Error Rate**                 | 0.5%                 | **< 0.1%**                      | Datadog / Sentry                     |
| **Search Accuracy (NDCG@10)**  | N/A                  | **> 0.85**                      | Offline evaluation with test dataset |
| **Extraction F1-Score**        | N/A                  | **> 0.90**                      | Manual evaluation + automated tests  |

### 1.4 Success Criteria

**MVP Success Criteria:**

- âœ… Core APIs (`search_facts`, `extract_facts`) fully functional
- âœ… Self-hosted stack deployed on Kubernetes
- âœ… Asynchronous extraction with background workers
- âœ… Basic observability (logging, metrics, dashboards)
- âœ… Security foundation (API key auth, network policies)

**Production Success Criteria:**

- âœ… P95 latency < 100ms achieved
- âœ… Cache hit rate > 40%
- âœ… Error rate < 0.1%
- âœ… System handles 500 concurrent users
- âœ… All SLOs being met

---

## 2. HIGH-LEVEL DESIGN (HLD)

### 2.1 System Architecture Overview

Há»‡ thá»‘ng PIKA Memory System Ä‘Æ°á»£c thiáº¿t káº¿ theo kiáº¿n trÃºc **Clean Architecture** vá»›i **Domain-Driven Design (DDD)**, tuÃ¢n thá»§ nguyÃªn táº¯c **SOLID** vÃ  **12-Factor App**.

```mermaid
graph TD
    subgraph "PIKA Ecosystem"
        A[PIKA AI Companion] --> B{PIKA Memory System API}
    end

    subgraph "PIKA Memory System (Self-Hosted)"
        B -- HTTPS/gRPC --> C[API Gateway: FastAPI]
        C -- Async Calls --> D[Mem0 Python SDK]
        D -- CRUD Ops --> E[Data Stores]
        C -- Async Jobs --> F[Message Queue: RabbitMQ]
        F --> G[Workers]
        G -- Writes --> E
    end

    subgraph "Data Stores"
        E -- Vector Search --> H[Milvus/Qdrant]
        E -- Graph Traversal --> I[Neo4j]
        E -- Metadata/Jobs --> J[PostgreSQL]
        E -- Caching --> K[Redis]
    end

    subgraph "External Services"
        D -- Embedding/Extraction --> L[OpenAI/Claude API]
    end

    style B fill:#8E44AD,stroke:#000,stroke-width:2px,color:#fff
    style C fill:#2980B9,stroke:#000,stroke-width:2px,color:#fff
    style D fill:#3498DB,stroke:#000,stroke-width:2px,color:#fff
    style G fill:#3498DB,stroke:#000,stroke-width:2px,color:#fff
    style F fill:#F39C12,stroke:#000,stroke-width:2px,color:#fff
    style E fill:#16A085,stroke:#000,stroke-width:2px,color:#fff
```

### 2.2 Container Diagram (C4 Level 2)

```mermaid
graph TD
    subgraph "Application Layer"
        A[API Gateway<br/>FastAPI, Container] -- Serves API requests --> B(Client)
        A -- Enqueues Jobs --> C{Message Queue<br/>RabbitMQ, Container}
        C -- Delivers Jobs --> D[Fact Extraction Worker<br/>Python, Container]
        A -- Reads/Writes --> E{Data Stores}
        D -- Writes --> E
        D -- Calls for AI tasks --> F[External LLM API]
    end

    subgraph "Data Stores (Containers)"
        E -- Vector Search --> G[Vector DB<br/>Milvus/Qdrant]
        E -- Graph Search --> H[Graph DB<br/>Neo4j]
        E -- Metadata/Jobs --> I[Relational DB<br/>PostgreSQL]
        E -- Caching --> J[Cache<br/>Redis]
    end

    B -- HTTPS --> A

    style A fill:#2980B9,color:white
    style D fill:#2980B9,color:white
    style C fill:#F39C12,color:white
    style E fill:#16A085,color:white
```

### 2.3 Component Responsibilities

| Container                        | Technology               | Responsibilities                                                                                                                                                                                         | Key Interactions                    |
| -------------------------------- | ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------- |
| **API Gateway**            | FastAPI (Python)         | - Handle all incoming API requests`<br>`- Authentication & Authorization`<br>`- Input validation & sanitization`<br>`- Enqueue jobs for async processing`<br>`- Synchronous search orchestration | Client, Message Queue, Data Stores  |
| **Message Queue**          | RabbitMQ                 | - Decouple API from workers`<br>`- Persist jobs for reliable processing`<br>`- Provide a buffer for traffic spikes                                                                                   | API Gateway, Workers                |
| **Fact Extraction Worker** | Python (Celery/Dramatiq) | - Consume jobs from the queue`<br>`- Call LLM for fact/relationship extraction`<br>`- Call embedding models`<br>`- Write results to data stores                                                    | Message Queue, Data Stores, LLM API |
| **Vector DB**              | Milvus/Qdrant            | - Store text embeddings`<br>`- Perform fast, scalable semantic search (ANN)                                                                                                                            | API Gateway, Workers                |
| **Graph DB**               | Neo4j                    | - Store entities and their relationships`<br>`- Enable complex, multi-hop queries                                                                                                                      | API Gateway, Workers                |
| **Relational DB**          | PostgreSQL               | - Store structured metadata (users, conversations)`<br>`- Track job status`<br>`- Maintain audit logs                                                                                                | API Gateway, Workers                |
| **Cache**                  | Redis                    | - Cache search results (L2 cache)`<br>`- Store hot data to reduce DB load                                                                                                                              | API Gateway                         |

### 2.4 Technology Stack

| Component                 | Chosen Technology          | Alternatives Considered | Justification                                                                                                                                    |
| ------------------------- | -------------------------- | ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| **API Framework**   | **FastAPI**          | Flask, Django           | FastAPI's native `asyncio` support is perfect for I/O-bound tasks. Its Pydantic integration provides excellent data validation out-of-the-box. |
| **Vector Database** | **Milvus/Qdrant**    | Weaviate                | Milvus/Qdrant offer superior performance and memory safety. Simpler to deploy and manage for a self-hosted scenario.                             |
| **Graph Database**  | **Neo4j**            | Memgraph, NebulaGraph   | Neo4j is the most mature and widely-adopted graph database with a rich ecosystem and powerful Cypher query language.                             |
| **Message Queue**   | **RabbitMQ**         | Kafka, Redis Streams    | RabbitMQ is robust, mature, and feature-rich. Simpler to set up and manage than Kafka for this use case.                                         |
| **Orchestration**   | **Kubernetes (EKS)** | Docker Swarm, Nomad     | Kubernetes is the industry standard for container orchestration, offering unparalleled scalability and resilience.                               |

### 2.5 Data Flow Architecture

#### Synchronous Flow: Search Facts

```mermaid
sequenceDiagram
    participant C as Client
    participant API as API Gateway
    participant Cache as Cache Service
    participant Mem0 as Mem0 SDK
    participant VDB as Vector DB
    participant GDB as Graph DB

    C->>API: POST /search_facts
    API->>Cache: Check L0/L1/L2 Cache
    alt Cache Hit
        Cache-->>API: Return Cached Result
        API-->>C: 200 OK (< 50ms)
    else Cache Miss
        API->>Mem0: Generate Query Embedding
        Mem0-->>API: Query Vector
        API->>VDB: Vector Search
        VDB-->>API: Candidate IDs
        API->>GDB: Graph Augmentation
        GDB-->>API: Enriched Results
        API->>Cache: Store Result
        API-->>C: 200 OK (100-300ms)
    end
```

#### Asynchronous Flow: Extract Facts

```mermaid
sequenceDiagram
    participant C as Client
    participant API as API Gateway
    participant MQ as RabbitMQ
    participant W as Worker
    participant Mem0 as Mem0 SDK
    participant DB as Data Stores

    C->>API: POST /extract_facts
    API->>API: Create Job (PostgreSQL)
    API->>MQ: Enqueue Job
    API-->>C: 202 Accepted (< 50ms)
  
    MQ->>W: Deliver Job
    W->>W: Update Status: "processing"
    W->>Mem0: Extract Facts (LLM)
    Mem0-->>W: Facts & Relationships
    W->>Mem0: Generate Embeddings
    Mem0-->>W: Embeddings
    W->>DB: Store in Vector DB
    W->>DB: Store in Graph DB
    W->>DB: Update Job Status: "completed"
    W->>Cache: Invalidate Cache
```

### 2.6 Folder Structure (Clean Architecture)

```
/pika-memory-system/
â”œâ”€â”€ app/                                    # Main Application
â”‚   â”œâ”€â”€ api/                                # API Layer
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ endpoints/                  # Route handlers
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ memory.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ jobs.py
â”‚   â”‚   â”‚   â””â”€â”€ schemas/                    # Pydantic models
â”‚   â”‚   â”‚       â”œâ”€â”€ memory.py
â”‚   â”‚   â”‚       â””â”€â”€ jobs.py
â”‚   â”‚   â”œâ”€â”€ dependencies.py                 # Dependency injection
â”‚   â”‚   â””â”€â”€ router.py
â”‚   â”œâ”€â”€ core/                               # Core Configuration
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ exceptions.py
â”‚   â”‚   â”œâ”€â”€ logging.py
â”‚   â”‚   â””â”€â”€ security.py
â”‚   â”œâ”€â”€ domains/                            # Domain Layer
â”‚   â”‚   â””â”€â”€ memory/
â”‚   â”‚       â”œâ”€â”€ application/                # Application Services
â”‚   â”‚       â”‚   â”œâ”€â”€ services/
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ memory_service.py
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ job_service.py
â”‚   â”‚       â”‚   â””â”€â”€ repositories/          # Repository Interfaces
â”‚   â”‚       â”‚       â”œâ”€â”€ memory_repository.py
â”‚   â”‚       â”‚       â””â”€â”€ job_repository.py
â”‚   â”‚       â”œâ”€â”€ domain/                     # Domain Entities
â”‚   â”‚       â”‚   â”œâ”€â”€ entities.py
â”‚   â”‚       â”‚   â””â”€â”€ value_objects.py
â”‚   â”‚       â””â”€â”€ infrastructure/            # Infrastructure Implementations
â”‚   â”‚           â”œâ”€â”€ models/
â”‚   â”‚           â”‚   â””â”€â”€ job_model.py
â”‚   â”‚           â””â”€â”€ repositories/
â”‚   â”‚               â”œâ”€â”€ memory_repository_impl.py
â”‚   â”‚               â””â”€â”€ job_repository_impl.py
â”‚   â”œâ”€â”€ infrastructure/                     # Infrastructure Layer
â”‚   â”‚   â”œâ”€â”€ cache/                         # Caching Services
â”‚   â”‚   â”‚   â”œâ”€â”€ cache_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ l0_session_cache.py
â”‚   â”‚   â”‚   â”œâ”€â”€ l1_redis_cache.py
â”‚   â”‚   â”‚   â”œâ”€â”€ l2_materialized_view.py
â”‚   â”‚   â”‚   â”œâ”€â”€ l3_embedding_cache.py
â”‚   â”‚   â”‚   â””â”€â”€ proactive_cache.py
â”‚   â”‚   â”œâ”€â”€ database/                      # Database Services
â”‚   â”‚   â”‚   â””â”€â”€ postgres_session.py
â”‚   â”‚   â”œâ”€â”€ mem0/                          # Mem0 Client
â”‚   â”‚   â”‚   â””â”€â”€ mem0_client.py
â”‚   â”‚   â””â”€â”€ messaging/                     # Message Queue
â”‚   â”‚       â””â”€â”€ rabbitmq_service.py
â”‚   â”œâ”€â”€ middleware/                        # Middleware
â”‚   â”‚   â”œâ”€â”€ error_handler.py
â”‚   â”‚   â””â”€â”€ logging_middleware.py
â”‚   â”œâ”€â”€ resilience/                        # Resilience Patterns
â”‚   â”‚   â”œâ”€â”€ circuit_breaker.py
â”‚   â”‚   â””â”€â”€ retry.py
â”‚   â””â”€â”€ main.py                            # FastAPI App Entry Point
â”œâ”€â”€ workers/                                # Background Workers
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”œâ”€â”€ extraction_task.py
â”‚   â”‚   â””â”€â”€ proactive_cache_task.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ tests/                                  # Test Suite
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ load/
â””â”€â”€ docs/                                   # Documentation
```

---

## 3. CHIáº¾N LÆ¯á»¢C CACHING

### 3.1 Overview: 5-Layer Caching Strategy

Há»‡ thá»‘ng triá»ƒn khai má»™t chiáº¿n lÆ°á»£c caching **5 lá»›p** káº¿t há»£p cáº£ **reactive caching** (cache khi cÃ³ request) vÃ  **proactive caching** (tÃ­nh toÃ¡n trÆ°á»›c cho cÃ¡c query thÆ°á»ng xuyÃªn).

```mermaid
graph TD
    Start[User Query] --> L0_Check{L0: Session Cache?}
  
    L0_Check -->|Hit < 1ms| Return_L0[Return Result]
    L0_Check -->|Miss| L1_Check{L1: Redis Cache?}
  
    L1_Check -->|Hit 5-20ms| Update_L0[Update L0] --> Return_L1[Return Result]
    L1_Check -->|Miss| L2_Check{L2: Materialized View?}
  
    L2_Check -->|Hit 20-50ms| Update_L1[Update L1] --> Update_L0_2[Update L0] --> Return_L2[Return Result]
    L2_Check -->|Miss| L3_Check{L3: Embedding Cache?}
  
    L3_Check -->|Hit 50-100ms| Update_L2[Update L2] --> Update_L1_2[Update L1] --> Update_L0_3[Update L0] --> Return_L3[Return Result]
    L3_Check -->|Miss| L4_Search[L4: Vector Search<br/>100-300ms]
  
    L4_Search --> Update_L3[Update L3] --> Update_L2_2[Update L2] --> Update_L1_3[Update L1] --> Update_L0_4[Update L0] --> Return_L4[Return Result]
  
    Return_L0 --> Response[Response Delivered]
    Return_L1 --> Response
    Return_L2 --> Response
    Return_L3 --> Response
    Return_L4 --> Response
  
    subgraph "Proactive Worker"
        Worker[Proactive Worker<br/>Runs every 30 min] --> PreCompute[Pre-compute User Favorites]
        PreCompute --> Update_L2_MV[Update L2 Materialized View]
        PreCompute --> Warm_L1[Warm L1 Redis Cache]
    end
```

### 3.2 Layer Details

#### L0: Session Cache (In-Memory)

| Aspect                    | Details                                          |
| ------------------------- | ------------------------------------------------ |
| **Technology**      | Python `@lru_cache` decorator                  |
| **Scope**           | Request-scoped, in-process memory                |
| **Latency**         | **< 1ms**                                  |
| **TTL**             | Request lifetime                                 |
| **Use Case**        | Cache results within the same request/API call   |
| **Hit Rate Target** | 10-20%                                           |
| **Implementation**  | `app/infrastructure/cache/l0_session_cache.py` |

**Key Features:**

- Zero network overhead
- Thread-safe with proper locking
- Automatic cleanup after request completion
- Cache key: `hash(user_id + query)`

#### L1: Redis Cache (Distributed)

| Aspect                    | Details                                           |
| ------------------------- | ------------------------------------------------- |
| **Technology**      | Redis 7.x                                         |
| **Scope**           | Distributed, shared across all API instances      |
| **Latency**         | **5-20ms** (network + Redis processing)     |
| **TTL**             | 1 hour (configurable)                             |
| **Use Case**        | Cache search results for frequently asked queries |
| **Hit Rate Target** | 30-40%                                            |
| **Implementation**  | `app/infrastructure/cache/l1_redis_cache.py`    |

**Key Features:**

- Distributed caching across multiple API instances
- JSON serialization for complex objects
- Cache key: `search:{user_id}:{hash(query)}`
- Automatic expiration and eviction policies
- Connection pooling for performance

**Cache Invalidation Strategy:**

- **Tag-based invalidation**: Each `user_id` has a `version_tag` stored in Redis
- Cache key includes version: `search:{user_id}:{version_tag}:{hash(query)}`
- When new memories are extracted, increment `version_tag` â†’ all old cache entries become stale
- No need to manually delete cache entries

#### L2: Materialized View (PostgreSQL)

| Aspect                    | Details                                                              |
| ------------------------- | -------------------------------------------------------------------- |
| **Technology**      | PostgreSQL JSONB column                                              |
| **Scope**           | Persistent, pre-computed summaries                                   |
| **Latency**         | **20-50ms** (database query)                                   |
| **TTL**             | Updated by proactive worker (every 30 minutes)                       |
| **Use Case**        | Pre-computed results for predictable queries (e.g., "user favorite") |
| **Hit Rate Target** | 20-30% (for specific query patterns)                                 |
| **Implementation**  | `app/infrastructure/cache/l2_materialized_view.py`                 |

**Database Schema:**

```sql
CREATE TABLE user_favorite_summary (
    user_id VARCHAR(255) PRIMARY KEY,
    summary_json JSONB NOT NULL,
    last_updated TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_user_favorite_summary_user_id ON user_favorite_summary(user_id);
```

**Key Features:**

- Pre-computed summaries for common queries
- Updated by proactive worker (not on-demand)
- JSONB for flexible schema
- Indexed for fast lookups
- Used primarily for "user favorite" type queries

#### L3: Embedding Cache (Redis)

| Aspect                    | Details                                                               |
| ------------------------- | --------------------------------------------------------------------- |
| **Technology**      | Redis 7.x                                                             |
| **Scope**           | Distributed, shared across all instances                              |
| **Latency**         | **50-100ms** (includes embedding generation time if cache miss) |
| **TTL**             | 24 hours                                                              |
| **Use Case**        | Cache query embeddings to avoid redundant OpenAI API calls            |
| **Hit Rate Target** | 60-70%                                                                |
| **Implementation**  | `app/infrastructure/cache/l3_embedding_cache.py`                    |

**Key Features:**

- Cache key: `embedding:{hash(query)}`
- Stores 1536-dimensional vectors (text-embedding-3-small)
- Eliminates 100-200ms OpenAI API latency for repeated queries
- High hit rate due to query similarity (users often ask similar questions)

#### L4: Vector Search (Mem0/Milvus)

| Aspect                    | Details                                              |
| ------------------------- | ---------------------------------------------------- |
| **Technology**      | Milvus/Qdrant + Mem0 SDK                             |
| **Scope**           | Primary data source, fallback when all caches miss   |
| **Latency**         | **100-300ms** (vector search + LLM re-ranking) |
| **TTL**             | N/A (primary source)                                 |
| **Use Case**        | Full semantic search when cache misses               |
| **Hit Rate Target** | N/A (this is the fallback)                           |
| **Implementation**  | `app/infrastructure/mem0/mem0_client.py`           |

**Key Features:**

- Approximate Nearest Neighbor (ANN) search
- HNSW index for fast vector similarity search
- Graph augmentation via Neo4j for relationship context
- Optional re-ranking with LLM for better relevance

### 3.3 Proactive Caching Strategy

#### Overview

Proactive caching lÃ  má»™t chiáº¿n lÆ°á»£c **"chá»§ Ä‘á»™ng"** Ä‘á»ƒ tÃ­nh toÃ¡n trÆ°á»›c káº¿t quáº£ cho cÃ¡c query cÃ³ thá»ƒ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c, Ä‘áº·c biá»‡t lÃ  query `user favorite (...)`.

#### How It Works

1. **Proactive Worker** cháº¡y Ä‘á»‹nh ká»³ (má»—i 30 phÃºt) hoáº·c khi cÃ³ sá»± kiá»‡n (new memories extracted)
2. Worker query Mem0 Ä‘á»ƒ láº¥y táº¥t cáº£ "sá»Ÿ thÃ­ch" cá»§a má»—i user
3. Tá»•ng há»£p káº¿t quáº£ thÃ nh má»™t JSON summary
4. LÆ°u vÃ o **L2 Materialized View** (PostgreSQL)
5. "Warm up" **L1 Redis Cache** vá»›i káº¿t quáº£ nÃ y

#### Benefits

- **99% hit rate** cho "user favorite" queries â†’ **< 50ms response time**
- Giáº£m load trÃªn Vector DB vÃ  LLM APIs
- Cáº£i thiá»‡n user experience vá»›i response time cá»±c nhanh
- Cost-effective: tÃ­nh toÃ¡n má»™t láº§n, sá»­ dá»¥ng nhiá»u láº§n

#### Implementation

**Worker:** `workers/tasks/proactive_cache_task.py`

- Scheduled job (APScheduler)
- Processes all active users
- Updates L2 Materialized View
- Warms L1 Redis Cache

**Service:** `app/infrastructure/cache/proactive_cache.py`

- Business logic for cache computation
- Integration with Mem0 SDK
- Cache invalidation logic

### 3.4 Cache Invalidation Strategy

#### Tag-Based Invalidation (World-Class Approach)

**Problem:** Traditional cache invalidation requires tracking and deleting individual cache keys, which is complex and error-prone.

**Solution:** Tag-based invalidation using version tags.

**How It Works:**

1. **Tagging:** Each `user_id` has a `version_tag` (timestamp or UUID) stored in Redis

   ```
   Key: user_version:{user_id}
   Value: {version_tag: "2025-01-21T10:30:00Z"}
   ```
2. **Key Generation:** Cache keys include the version tag

   ```
   Cache Key: search:{user_id}:{version_tag}:{hash(query)}
   ```
3. **Invalidation:** When new memories are extracted:

   - Worker increments `version_tag` for that `user_id`
   - All old cache entries automatically become stale (key mismatch)
   - No need to manually delete cache entries
4. **Benefits:**

   - âœ… Simple: Only one value to update per user
   - âœ… Efficient: No need to track or delete individual keys
   - âœ… Atomic: Single Redis operation
   - âœ… Scalable: Works for millions of users

**Implementation:**

```python
# On cache read
version_tag = redis.get(f"user_version:{user_id}")
cache_key = f"search:{user_id}:{version_tag}:{hash(query)}"
result = redis.get(cache_key)

# On cache invalidation (after extraction)
new_version = datetime.utcnow().isoformat()
redis.set(f"user_version:{user_id}", new_version)
# All old cache entries are now stale
```

### 3.5 Cache Performance Metrics

| Metric                                     | Target  | Current | Status     |
| ------------------------------------------ | ------- | ------- | ---------- |
| **L0 Hit Rate**                      | 10-20%  | TBD     | ðŸŸ¡ Pending |
| **L1 Hit Rate**                      | 30-40%  | TBD     | ðŸŸ¡ Pending |
| **L2 Hit Rate**                      | 20-30%  | TBD     | ðŸŸ¡ Pending |
| **L3 Hit Rate**                      | 60-70%  | TBD     | ðŸŸ¡ Pending |
| **Overall Cache Hit Rate**           | > 40%   | TBD     | ðŸŸ¡ Pending |
| **Average Response Time (Cached)**   | < 50ms  | TBD     | ðŸŸ¡ Pending |
| **Average Response Time (Uncached)** | < 200ms | TBD     | ðŸŸ¡ Pending |

### 3.6 Cache Optimization Roadmap

#### Phase 1: Foundation (Completed âœ…)

- âœ… L0 Session Cache implementation
- âœ… L1 Redis Cache implementation
- âœ… L2 Materialized View schema
- âœ… L3 Embedding Cache implementation
- âœ… Basic cache invalidation

#### Phase 2: Proactive Caching (In Progress ðŸŸ¡)

- ðŸŸ¡ Proactive worker implementation
- ðŸŸ¡ L2 Materialized View population
- ðŸŸ¡ Cache warming strategy
- ðŸŸ¡ Monitoring and metrics

#### Phase 3: Advanced Optimization (Planned ðŸ“‹)

- ðŸ“‹ Cache preloading for hot users
- ðŸ“‹ Predictive caching based on user patterns
- ðŸ“‹ Cache compression for large objects
- ðŸ“‹ Multi-region cache replication

---

## 4. PERFORMANCE OPTIMIZATION STRATEGY

### 4.1 Latency Breakdown Analysis

Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu **P95 < 100ms**, chÃºng ta phÃ¢n tÃ­ch response time theo cÃ´ng thá»©c:

$$
RT = T_{API} + T_{Cache} + T_{Embedding} + T_{VectorSearch} + T_{Graph} + T_{Network}
$$

| Component                      | Latency (Worst Case) | Optimization Strategy             | Target Latency  |
| ------------------------------ | -------------------- | --------------------------------- | --------------- |
| **API Overhead**         | 5-10ms               | Async FastAPI, connection pooling | < 5ms           |
| **Cache Lookup**         | 1-50ms               | Multi-layer caching               | < 20ms (cached) |
| **Embedding Generation** | 100-200ms            | L3 Embedding Cache                | < 5ms (cached)  |
| **Vector Search**        | 50-150ms             | HNSW index, GPU acceleration      | < 30ms          |
| **Graph Augmentation**   | 50-100ms             | Async parallelism, caching        | < 20ms          |
| **Network Latency**      | 1-5ms                | Service colocation (same AZ)      | < 2ms           |

**Total (Optimized):** < 82ms (P95)

### 4.2 Optimization Techniques

#### Compute Optimization

- **GPU Acceleration:** Milvus with CAGRA/RAPIDS index on GPU servers
- **Asynchronous Parallelism:** `asyncio.gather` for parallel DB calls
- **Batching:** Batch embedding requests when possible

#### Data Structure Optimization

- **Vector Index Tuning:** HNSW with optimized `efConstruction` and `efSearch`
- **Scalar Filtering:** Pre-filter by `user_id` before vector search
- **Data Colocation:** Store frequently accessed fields with vectors

#### Network Optimization

- **Service Colocation:** All services in same AZ/VPC (< 1ms latency)
- **Protocol Optimization:** Protocol Buffers instead of JSON for internal calls
- **Connection Pooling:** pgBouncer for PostgreSQL, connection pools for all DBs

---

## 5. DEPLOYMENT ARCHITECTURE

### 5.1 Infrastructure Requirements

| Component                         | Instance Type    | vCPU | RAM  | Storage   | Cost/Month              |
| --------------------------------- | ---------------- | ---- | ---- | --------- | ----------------------- |
| **API Servers (3Ã—)**       | t3.large         | 2    | 8GB  | 30GB      | $90                     |
| **Worker Servers (2Ã—)**    | t3.xlarge        | 4    | 16GB | 50GB      | $120                    |
| **Milvus/Qdrant (1Ã—)**     | c5.2xlarge       | 8    | 16GB | 500GB SSD | $200                    |
| **Neo4j (1Ã—)**             | c5.2xlarge       | 8    | 16GB | 500GB SSD | $200                    |
| **PostgreSQL (1Ã—)**        | db.r5.2xlarge    | 8    | 64GB | 1TB SSD   | $400                    |
| **Redis (1Ã—)**             | cache.r6g.xlarge | 4    | 32GB | 100GB     | $100                    |
| **RabbitMQ (1Ã—)**          | t3.large         | 2    | 8GB  | 50GB      | $50                     |
| **Load Balancer**           | ALB              | -    | -    | -         | $50                     |
| **Networking & Monitoring** | -                | -    | -    | -         | $150                    |
| **Total**                   | -                | -    | -    | -         | **~$1,360/month** |

**Cost Optimization:**

- AWS Spot Instances for workers: **-30%** â†’ ~$1,000/month
- Reserved Instances for databases: **-25%** â†’ ~$850/month
- **Optimized Total: ~$850/month** (vs. $600/month Mem0 Enterprise, but with full control)

### 5.2 High Availability Architecture

```
Primary Region (ap-southeast-1, Singapore)
â”œâ”€ EKS Cluster (3 availability zones)
â”‚  â”œâ”€ API Pods (3 replicas, HPA: min 3, max 10)
â”‚  â”œâ”€ Worker Pods (2 replicas, HPA: min 2, max 5)
â”‚  â””â”€ RabbitMQ StatefulSet (1 replica)
â”œâ”€ Milvus/Qdrant (1 replica, daily snapshots to S3)
â”œâ”€ Neo4j (1 replica, daily snapshots to S3)
â”œâ”€ PostgreSQL (1 primary + 1 read replica)
â””â”€ Redis (1 primary + 1 replica)

Secondary Region (eu-central-1, Frankfurt) - For GDPR compliance
â”œâ”€ Standby EKS Cluster (can be activated in <5 minutes)
â”œâ”€ Read-only replicas of all databases
â””â”€ Automated failover via Route 53 health checks
```

---

## 6. MONITORING & OBSERVABILITY

### 6.1 Key Metrics

**API Metrics:**

- `http_requests_total` (counter)
- `http_request_duration_seconds` (histogram)
- `http_request_size_bytes` (histogram)
- `http_response_size_bytes` (histogram)

**Business Metrics:**

- `search_facts_requests_total` (counter)
- `extract_facts_requests_total` (counter)
- `facts_extracted_total` (counter)
- `facts_searched_total` (counter)

**System Metrics:**

- `milvus_query_latency_ms` (histogram)
- `neo4j_query_latency_ms` (histogram)
- `postgresql_query_latency_ms` (histogram)
- `redis_operation_latency_ms` (histogram)
- `job_processing_duration_seconds` (histogram)
- `job_success_rate` (gauge)
- `cache_hit_rate` (gauge)

**Cache Metrics:**

- `cache_hits_total` (counter, by layer)
- `cache_misses_total` (counter, by layer)
- `cache_hit_rate` (gauge, by layer)
- `cache_size_bytes` (gauge, by layer)

### 6.2 Dashboards (Grafana)

- **Overview Dashboard:** System health, throughput, error rate
- **Performance Dashboard:** Latency percentiles, cache hit rate, database query times
- **Reliability Dashboard:** Uptime, error rate by endpoint, job success rate
- **Cost Dashboard:** Infrastructure cost, cost per API call, cost per memory stored
- **Cache Dashboard:** Hit rates by layer, cache size, invalidation events

---

## 7. RISK ASSESSMENT & MITIGATION

| Risk                              | Probability | Impact | Mitigation Strategy                                                                                                               |
| --------------------------------- | ----------- | ------ | --------------------------------------------------------------------------------------------------------------------------------- |
| **Mem0 OSS Abandonment**    | Low         | High   | Fork a private version. Allocate 10% engineering time for maintenance. Have a lightweight, vector-only alternative as a fallback. |
| **Performance Bottleneck**  | Medium      | High   | Load testing early and often. Use profiling tools (Py-spy, cProfile). Optimize DB queries and index configuration.                |
| **Cost Overrun**            | Medium      | Medium | Use AWS Spot Instances for workers. Apply Reserved Instances for databases. Automate cleanup of unused resources.                 |
| **Cache Invalidation Bugs** | Medium      | Medium | Comprehensive testing of cache invalidation logic. Tag-based invalidation reduces complexity.                                     |
| **High Cache Miss Rate**    | Low         | Medium | Monitor cache hit rates. Adjust TTLs and proactive caching frequency based on metrics.                                            |

---

## 8. ROADMAP & NEXT STEPS

### Phase 1: Foundation (Weeks 1-2) âœ…

- âœ… Setup development environment
- âœ… Implement FastAPI skeleton
- âœ… Implement database integrations
- âœ… Implement RabbitMQ and workers

### Phase 2: Core APIs (Weeks 3-4) âœ…

- âœ… Implement `/search_facts` API
- âœ… Implement `/extract_facts` API
- âœ… Implement job status polling
- âœ… Implement Mem0 SDK integration

### Phase 3: Caching (Weeks 5-6) ðŸŸ¡

- âœ… L0/L1/L3 Cache implementation
- ðŸŸ¡ L2 Materialized View implementation
- ðŸŸ¡ Proactive caching worker
- ðŸŸ¡ Cache invalidation strategy

### Phase 4: Performance Optimization (Weeks 7-8) ðŸ“‹

- ðŸ“‹ Milvus index optimization (HNSW tuning)
- ðŸ“‹ Asynchronous parallelism
- ðŸ“‹ Connection pooling optimization
- ðŸ“‹ Load testing and bottleneck identification

### Phase 5: Production Deployment (Weeks 9-10) ðŸ“‹

- ðŸ“‹ Kubernetes deployment
- ðŸ“‹ CI/CD pipeline
- ðŸ“‹ Monitoring and alerting
- ðŸ“‹ Runbooks and documentation

---

## 9. CONCLUSION

PIKA Memory System Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i má»¥c tiÃªu Ä‘áº¡t Ä‘Æ°á»£c **world-class performance** trong khi giáº£m Ä‘Ã¡ng ká»ƒ chi phÃ­ váº­n hÃ nh. Chiáº¿n lÆ°á»£c caching 5 lá»›p káº¿t há»£p vá»›i proactive caching sáº½ Ä‘áº£m báº£o:

- âœ… **P95 latency < 100ms** cho search API
- âœ… **Cache hit rate > 40%** Ä‘á»ƒ giáº£m load trÃªn databases
- âœ… **99.9% uptime** vá»›i high availability architecture
- âœ… **Cost-effective** vá»›i chi phÃ­ < $850/month

Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng cho production deployment vá»›i Ä‘áº§y Ä‘á»§ monitoring, observability, vÃ  resilience patterns.

---

## APPENDICES

### A. References

- [Mem0 Documentation](https://docs.mem0.ai)
- [Milvus Documentation](https://milvus.io/docs)
- [Neo4j Documentation](https://neo4j.com/docs)
- [FastAPI Documentation](https://fastapi.tiangolo.com)

### B. Related Documents

- `docs/Step 2 - Output 1 - SDD.md` - Full Software Design Document
- `docs/Step 2 - Output 2 - SDD - HLD - LLD cÃ³ cáº£ Optimize Response Time.md` - Low-Level Design
- `docs/Step 2 - Output 3 - Caching 5 lá»›p.md` - Caching Strategy Details

---

**Document Version:** 1.0.0
**Last Updated:** 2025-01-21
**Next Review:** 2025-02-21