# V1 - manus: B√ÅO C√ÅO MECE T·ªêI ∆ØU RESPONSE TIME CHO PIKA MEMORY SYSTEM

**M·ª•c ti√™u:** ƒê·∫°t P95 Latency < 200ms cho `search_facts` API.

**T√°c gi·∫£:** Manus AI (Lead Architect) | **Ng√†y:** 2025-12-21

---

## 1. PH√ÇN T√çCH MECE C√ÅC ƒêI·ªÇM NGH·∫ºN LATENCY

ƒê·ªÉ ƒë·∫°t ƒë∆∞·ª£c m·ª•c ti√™u P95 < 200ms, ch√∫ng ta c·∫ßn ph√¢n t√≠ch Response Time (RT) theo c√¥ng th·ª©c:
$$RT = T_{API} + T_{Cache} + T_{Embedding} + T_{VectorSearch} + T_{Graph} + T_{Network}$$

C√°c ƒëi·ªÉm ngh·∫Ωn ch√≠nh ƒë∆∞·ª£c ph√¢n t√≠ch theo nguy√™n t·∫Øc MECE:

| Nh√≥m Y·∫øu T·ªë | ƒêi·ªÉm Ngh·∫Ωn C·ª• Th·ªÉ | Latency ∆Ø·ªõc T√≠nh (Worst Case) | Chi·∫øn L∆∞·ª£c T·ªëi ∆Øu |
|---|---|---|---|
| **I. External Dependency** | **Embedding API Call (OpenAI)** | 100ms - 200ms | **Semantic Caching** (Lo·∫°i b·ªè 90% cu·ªôc g·ªçi) |
| **II. Core Search** | **Vector Search (Milvus)** | 50ms - 150ms | **Index Optimization** (HNSW/CAGRA) & **GPU Acceleration** |
| **III. Data Enrichment** | **Graph Traversal (Neo4j)** | 50ms - 100ms | **Asynchronous Traversal** & **Caching** |
| **IV. Internal I/O** | **Internal Network Latency** | 1ms - 5ms | **Service Colocation** (Same AZ/VPC) |
| **V. Application Overhead** | **Serialization/Deserialization** | 5ms - 10ms | **Protocol Buffers** (thay cho JSON) |

---

## 2. ƒê√ÅNH GI√Å V√Ä C·∫¢I TI·∫æN CHI·∫æN L∆Ø·ª¢C CACHING ƒêA T·∫¶NG

Chi·∫øn l∆∞·ª£c caching ƒëa t·∫ßng (L1/L2/L3) l√† n·ªÅn t·∫£ng v·ªØng ch·∫Øc. Tuy nhi√™n, ƒë·ªÉ ƒë·∫°t P95/P99, c·∫ßn b·ªï sung **Semantic Caching cho Embedding** v√† t·ªëi ∆∞u h√≥a chi·∫øn l∆∞·ª£c Invalidation.

### 2.1. C·∫£i Ti·∫øn C·∫•u Tr√∫c Cache

| L·ªõp Cache | Lo·∫°i Cache | C·∫£i Ti·∫øn ƒê·ªÅ Xu·∫•t | M·ª•c Ti√™u Latency |
|---|---|---|---|
| **L0: Embedding Cache** | Redis (Key: `hash(query)`) | **M·ªõi:** Cache vector embedding. | **< 5ms** (Lo·∫°i b·ªè 100-200ms) |
| **L1: In-Memory** | `@lru_cache` | Cache k·∫øt qu·∫£ cu·ªëi c√πng (Final Result Cache). | **< 1ms** |
| **L2: Redis Semantic Cache** | Redis (Key: `search:{user_id}:{hash(query)}`) | Cache k·∫øt qu·∫£ t√¨m ki·∫øm cu·ªëi c√πng. | **5ms - 20ms** |
| **L3: Persistent Cache** | PostgreSQL | Cache k·∫øt qu·∫£ d√†i h·∫°n (Long-tail queries). | **50ms - 100ms** |

### 2.2. Chi·∫øn L∆∞·ª£c Cache Invalidation (World-Class)

Chi·∫øn l∆∞·ª£c **Cache Tagging** l√† gi·∫£i ph√°p t·ªëi ∆∞u nh·∫•t cho h·ªá th·ªëng ƒëa ng∆∞·ªùi d√πng:

1.  **Tagging:** M·ªói `user_id` ƒë∆∞·ª£c g√°n m·ªôt `version_tag` (v√≠ d·ª•: timestamp ho·∫∑c UUID) ƒë∆∞·ª£c l∆∞u trong Redis.
2.  **Key Generation:** `cache_key` s·∫Ω l√† `search:{user_id}:{version_tag}:{hash(query)}`.
3.  **Invalidation:** Khi `extract_facts` ho√†n th√†nh, worker ch·ªâ c·∫ßn **tƒÉng `version_tag`** c·ªßa `user_id` ƒë√≥ trong Redis.
4.  **Hi·ªáu qu·∫£:** T·∫•t c·∫£ c√°c query c≈© c·ªßa user ƒë√≥ s·∫Ω t·ª± ƒë·ªông b·ªã miss cache (stale) v√† bu·ªôc ph·∫£i ch·∫°y l·∫°i, trong khi c√°c user kh√°c kh√¥ng b·ªã ·∫£nh h∆∞·ªüng.

---

## 3. PH√ÇN T√çCH MECE C√ÅC Y·∫æU T·ªê T·ªêI ∆ØU H√ìA (ULTIMATE LATENCY REDUCTION)

C√°c k·ªπ thu·∫≠t t·ªëi ∆∞u h√≥a ƒë∆∞·ª£c ph√¢n t√≠ch theo 3 tr·ª• c·ªôt ch√≠nh: **Compute**, **Data Structure**, v√† **Network**.

### 3.1. T·ªëi ∆Øu H√≥a Compute (Gi·∫£m th·ªùi gian x·ª≠ l√Ω)

| K·ªπ Thu·∫≠t                      | M√¥ T·∫£                                                                               | ·∫¢nh H∆∞·ªüng Latency                                                |
| ----------------------------- | ----------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **GPU Acceleration (Milvus)** | Tri·ªÉn khai Milvus v·ªõi index **CAGRA** ho·∫∑c **RAPIDS** tr√™n m√°y ch·ªß c√≥ GPU (NVIDIA). | Gi·∫£m th·ªùi gian t√¨m ki·∫øm vector t·ª´ 50-150ms xu·ªëng **< 10ms** [1]. |
| **Asynchronous Parallelism**  | S·ª≠ d·ª•ng `asyncio.gather` ƒë·ªÉ g·ªçi Milvus, Neo4j, v√† Redis song song.                  | Gi·∫£m t·ªïng th·ªùi gian ch·ªù I/O.                                     |
| **Batching**                  | Gom nhi·ªÅu query nh·ªè th√†nh m·ªôt batch l·ªõn h∆°n ƒë·ªÉ g·ªçi Embedding API (n·∫øu c√≥ th·ªÉ).      | Gi·∫£m overhead c·ªßa m·ªói API call.                                  |

### 3.2. T·ªëi ∆Øu H√≥a Data Structure (TƒÉng t·ªëc ƒë·ªô truy c·∫≠p)

| K·ªπ Thu·∫≠t | M√¥ T·∫£ | ·∫¢nh H∆∞·ªüng Latency |
|---|---|---|
| **Vector Index Tuning** | S·ª≠ d·ª•ng **HNSW** (Hierarchical Navigable Small World) thay v√¨ IVF_FLAT, v·ªõi c√°c tham s·ªë `efConstruction` v√† `efSearch` ƒë∆∞·ª£c tinh ch·ªânh. | TƒÉng t·ªëc ƒë·ªô t√¨m ki·∫øm v√† ƒë·ªô ch√≠nh x√°c (Recall) [2]. |
| **Filtering/Pre-ranking** | S·ª≠ d·ª•ng **Scalar Filtering** c·ªßa Milvus (tr√™n `user_id`, `fact_type`) ƒë·ªÉ gi·∫£m kh√¥ng gian t√¨m ki·∫øm tr∆∞·ªõc khi t√≠nh to√°n vector distance. | Gi·∫£m ƒë√°ng k·ªÉ th·ªùi gian t√¨m ki·∫øm tr√™n c√°c t·∫≠p d·ªØ li·ªáu l·ªõn. |
| **Data Colocation** | ƒê·∫£m b·∫£o c√°c tr∆∞·ªùng d·ªØ li·ªáu th∆∞·ªùng xuy√™n ƒë∆∞·ª£c truy c·∫≠p (fact content, metadata) ƒë∆∞·ª£c l∆∞u tr·ªØ c√πng v·ªõi vector trong Milvus (ho·∫∑c trong Redis Cache). | Gi·∫£m thi·ªÉu c√°c cu·ªôc g·ªçi join/lookup gi·ªØa Milvus v√† PostgreSQL. |

### 3.3. T·ªëi ∆Øu H√≥a Network (Gi·∫£m th·ªùi gian truy·ªÅn t·∫£i)

| K·ªπ Thu·∫≠t | M√¥ T·∫£ | ·∫¢nh H∆∞·ªüng Latency |
|---|---|---|
| **Service Colocation** | ƒê·∫∑t API Server, Redis, Milvus trong c√πng m·ªôt **Availability Zone (AZ)** v√† **Virtual Private Cloud (VPC)**. | ƒê·∫£m b·∫£o ƒë·ªô tr·ªÖ n·ªôi b·ªô **< 1ms**. |
| **Protocol Optimization** | S·ª≠ d·ª•ng **Protocol Buffers** ho·∫∑c **MessagePack** thay v√¨ JSON cho c√°c giao ti·∫øp n·ªôi b·ªô (API Server <-> Worker/DB). | Gi·∫£m k√≠ch th∆∞·ªõc payload v√† th·ªùi gian Serialization/Deserialization. |
| **Connection Pooling** | S·ª≠ d·ª•ng Connection Pooling (v√≠ d·ª•: `pgBouncer` cho PostgreSQL, `connection pool` cho Milvus) ƒë·ªÉ tr√°nh overhead t·∫°o k·∫øt n·ªëi m·ªõi. | Gi·∫£m ƒë·ªô tr·ªÖ k·∫øt n·ªëi ban ƒë·∫ßu (handshake latency). |

---

## 4. K·∫æT LU·∫¨N V√Ä ROADMAP ƒê·ªÄ XU·∫§T

Chi·∫øn l∆∞·ª£c caching ƒëa t·∫ßng l√† c·∫ßn thi·∫øt, nh∆∞ng kh√¥ng ƒë·ªß. ƒê·ªÉ ƒë·∫°t ƒë∆∞·ª£c m·ª•c ti√™u P95 < 200ms, PIKA c·∫ßn th·ª±c hi·ªán m·ªôt chi·∫øn l∆∞·ª£c t·ªëi ∆∞u h√≥a to√†n di·ªán.

| Giai ƒêo·∫°n | H√†nh ƒê·ªông Ch√≠nh | M·ª•c Ti√™u Latency |
|---|---|---|
| **Giai ƒëo·∫°n 1 (Immediate)** | **Tri·ªÉn khai L0/L2 Semantic Caching** (Embedding & Result Caching). | **P95 < 300ms** (Lo·∫°i b·ªè ƒë·ªô tr·ªÖ OpenAI) |
| **Giai ƒëo·∫°n 2 (Short-Term)** | **T·ªëi ∆∞u Milvus Index** (HNSW tuning) v√† **Asynchronous Parallelism** (FastAPI `asyncio.gather`). | **P95 < 150ms** (T·ªëi ∆∞u h√≥a Core Search) |
| **Giai ƒëo·∫°n 3 (Long-Term)** | **ƒê√°nh gi√° GPU Acceleration** (CAGRA/RAPIDS) v√† **Service Colocation** (VPC/AZ). | **P95 < 50ms** (World-Class Latency) |

**H√†nh ƒë·ªông quan tr·ªçng nh·∫•t:** Tri·ªÉn khai **L0 Semantic Caching** ƒë·ªÉ lo·∫°i b·ªè s·ª± ph·ª• thu·ªôc v√†o ƒë·ªô tr·ªÖ c·ªßa OpenAI API.

---

## T√ÄI LI·ªÜU THAM KH·∫¢O

[1] M. Zhang, Y. He, "Zoom: Ssd-based vector search for optimizing accuracy, latency and memory," *arXiv preprint arXiv:1809.04067*, 2018.
[2] Y. Zhou, S. Lin, S. Gong, et al., "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector Nearest Neighbor Search," *arXiv preprint arXiv:2508.15694*, 2025.
[3] P99 CONF 2025, "Low-Latency Data 2025," *www.p99conf.io*.

[Zoom: Ssd-based vector search for optimizing accuracy, latency and memory](https://arxiv.org/abs/1809.04067)

‚Ä¶ with low latency and memory footprint, which existing work fails to offer. We develop, Zoom, a new vector search solution that collaboratively optimizes accuracy, latency and memory ‚Ä¶

[Optimizing Matrix-Vector Operations with CGLA for High-Performance Approximate k-NN Search](https://ieeexplore.ieee.org/abstract/document/11048859/)

‚Ä¶ movement and significantly reducing latency. Experimental evaluations demonstrate that, ‚Ä¶ conventional methods and making our solution highly suitable for real-time vector search ‚Ä¶

[Cost-Effective, Low Latency Vector Search with Azure Cosmos DB](https://arxiv.org/abs/2505.05885)

‚Ä¶ We note that the p50, p95 and p99 latencies increase by less than 2√ó despite the 100√ó increase in index size. The RU charge similarly increases less than 2√ó except in the case of Wiki-‚Ä¶

[Optimizing Large Language Model Utilization through Scheduling Strategies](https://openresearch.newcastle.edu.au/ndownloader/files/55657901)

‚Ä¶ for LLM allocation to enhance performance and reduce costs. ‚Ä¶ for providing me with world-class facilities that have supported ‚Ä¶ Adds latency and relies on LLM‚Äôs self-assessment, which ‚Ä¶

[Intelligent dispatch method for power systems based on LLM knowledge injection](https://ieeexplore.ieee.org/abstract/document/10936149/)

‚Ä¶ Networks, World-class" strategic initiative [3], providing a ‚Ä¶ the optimal power system dispatching strategy. Experimental ‚Ä¶ to control accuracy and latency in traditional power systems. ‚Ä¶

[KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and Runtime Logs Analysis](https://arxiv.org/abs/2509.04191)

‚Ä¶ : a novel LLM-based framework for dynamically hardening K8s systems based on prompt-‚Ä¶ summaries (reducing token count by 99.96%), enabling efficient LLM analysis within token ‚Ä¶

[GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector Nearest Neighbor Search](https://arxiv.org/abs/2508.15694)

‚Ä¶ a core challenge for vector database systems. Traditional exact search methods suffer from the ‚Ä¶ , HNSW achieves efficient in-memory search through a multilayer hierarchical structure, ‚Ä¶

[Efficient Data Access Paths for Mixed Vector-Relational Search](https://dl.acm.org/doi/abs/10.1145/3662010.3663448)

‚Ä¶ Small World) based on multi-layer graph structure for high accu‚Ä¶ batching the vectors to prioritize cache-local dense vector ‚Ä¶ if a vector index is not the main (vector) database structure [26]‚Ä¶


---


# Ver 2 - Genspark: D·ª±a tr√™n ph√¢n t√≠ch c√°c t√†i li·ªáu thi·∫øt k·∫ø, t√¥i s·∫Ω ƒë√°nh gi√° chi·∫øn l∆∞·ª£c caching hi·ªán t·∫°i v√† ƒë·ªÅ xu·∫•t gi·∫£i ph√°p t·ªëi ∆∞u:

---

## 1Ô∏è‚É£ PH√ÇN T√çCH V·∫§N ƒê·ªÄ CHI·∫æN L∆Ø·ª¢C CACHING HI·ªÜN T·∫†I

### üî¥ **V·∫•n ƒë·ªÅ nghi√™m tr·ªçng ph√°t hi·ªán:**

#### **A. Thi·∫øt k·∫ø Proactive Caching c√≥ l·ªói ki·∫øn tr√∫c nghi√™m tr·ªçng**

**V·∫•n ƒë·ªÅ 1: Pre-compute cho query c·ª• th·ªÉ l√† Anti-pattern**

```python
# Code hi·ªán t·∫°i (SAI L·∫¶M)
query = "user favorite (movie, character, pet, activity, friend, music, travel, toy)"
results = await self.mem0_service.search_memories(user_id=user_id, query=query, limit=50)
```

**T·∫°i sao SAI:**

- ‚ùå **Hard-coded query** - ch·ªâ cache 1 pattern c·ªë ƒë·ªãnh
- ‚ùå **Kh√¥ng scale** - n·∫øu user h·ªèi ‚ÄúWhat movies do I like?‚Äù ‚Üí cache miss
- ‚ùå **Waste resources** - pre-compute h√†ng tri·ªáu user m·ªói 30 ph√∫t cho query hi·∫øm g·∫∑p
- ‚ùå **False assumption** - gi·∫£ ƒë·ªãnh 99% query l√† ‚Äúuser favorite‚Äù (kh√¥ng th·ª±c t·∫ø)

**V·∫•n ƒë·ªÅ 2: Layer L2 Materialized View d∆∞ th·ª´a**

```sql
CREATE TABLE user_favorite_summary (
    user_id VARCHAR(255) PRIMARY KEY,
    summary_json JSONB NOT NULL,
    last_updated TIMESTAMPTZ DEFAULT NOW()
);
```

**T·∫°i sao D∆Ø TH·ª™A:**

- Redis ƒë√£ ƒë·ªß nhanh (5-20ms) cho L2
- PostgreSQL JSONB query ch·∫≠m h∆°n Redis (20-50ms)
- TƒÉng ƒë·ªô ph·ª©c t·∫°p sync data gi·ªØa Redis ‚Üî PostgreSQL
- Waste storage: duplicate data trong 2 stores

#### **B. Cache Invalidation Strategy sai**

```python
# Approach hi·ªán t·∫°i: Version Tag
cache_key = f"search:{user_id}:{version_tag}:{hash(query)}"
```

**V·∫•n ƒë·ªÅ:**

- ‚ùå **Cold start** sau invalidation - t·∫•t c·∫£ queries c·ªßa user ƒë·ªÅu miss cache
- ‚ùå **Thundering herd** - n·∫øu 1000 users v·ª´a update facts ‚Üí 1000 thundering herd problems
- ‚ùå **Kh√¥ng t·∫≠n d·ª•ng ƒë∆∞·ª£c cached results c≈©** cho queries kh√¥ng thay ƒë·ªïi

---

## 2Ô∏è‚É£ CHI·∫æN L∆Ø·ª¢C CACHING HI·ªÜU QU·∫¢ NH·∫§T (PRODUCTION-GRADE)

### **üéØ Nguy√™n t·∫Øc thi·∫øt k·∫ø:**

1. **Reactive > Proactive** cho majority c·ªßa queries
2. **Cache Result, NOT Intermediate Steps**
3. **Granular Invalidation** thay v√¨ nuke to√†n b·ªô
4. **Layered v·ªõi Clear Responsibilities**

---

### **‚úÖ KI·∫æN TR√öC CACHE T·ªêI ∆ØU (3 LAYERS)**

```mermaid
graph TB
    Query[User Query] --> L1{L1: Application<br/>LRU Cache<br/>Latency: <1ms}
    
    L1 -->|Hit| Return1[Return <1ms]
    L1 -->|Miss| L2{L2: Redis<br/>Distributed Cache<br/>Latency: 5-20ms}
    
    L2 -->|Hit| UpdateL1[Update L1]
    UpdateL1 --> Return2[Return 5-20ms]
    
    L2 -->|Miss| L3[L3: Vector Search<br/>Milvus + LLM<br/>Latency: 100-300ms]
    
    L3 --> UpdateL2[Update L2]
    UpdateL2 --> UpdateL1_2[Update L1]
    UpdateL1_2 --> Return3[Return 100-300ms]
    
    style L1 fill:#4CAF50,color:#fff
    style L2 fill:#2196F3,color:#fff
    style L3 fill:#FF9800,color:#fff
```

---

### **üìã CHI TI·∫æT T·ª™NG LAYER**

#### **Layer 0: Query Embedding Cache (NEW - Critical!)**

**M·ª•c ƒë√≠ch:** Cache embedding vectors ƒë·ªÉ tr√°nh g·ªçi OpenAI API

```python
class EmbeddingCache:
    def __init__(self):
        self.redis = redis.Redis()
        
    async def get_embedding(self, text: str) -> List[float]:
        # Normalize query ƒë·ªÉ tƒÉng hit rate
        normalized = self._normalize(text)
        cache_key = f"embed:{hash(normalized)}"
        
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        
        # Cache miss - call OpenAI
        embedding = await openai_service.get_embedding(text)
        
        # Cache v·ªõi TTL d√†i (embedding √≠t thay ƒë·ªïi)
        self.redis.setex(cache_key, 7*24*3600, json.dumps(embedding))
        return embedding
    
    def _normalize(self, text: str) -> str:
        """Chu·∫©n h√≥a query ƒë·ªÉ tƒÉng cache hit rate"""
        return text.lower().strip()
```

**L·ª£i √≠ch:**

- ‚úÖ **Gi·∫£m 80-90% OpenAI API calls** (100-200ms/call)
- ‚úÖ **Gi·∫£m chi ph√≠** $0.0001/1K tokens ‚Üí ti·∫øt ki·ªám $100+/th√°ng
- ‚úÖ **TƒÉng reliability** - gi·∫£m dependency v√†o external API

---

#### **Layer 1: Application-Level LRU Cache**

```python
from functools import lru_cache
from typing import List, Dict

class SearchService:
    @lru_cache(maxsize=1000)
    async def _cached_search(
        self, 
        user_id: str, 
        query_hash: str,  # hash c·ªßa query ƒë·ªÉ l√†m cache key
        limit: int,
        score_threshold: float
    ) -> str:  # Return JSON string (immutable for lru_cache)
        """
        In-memory cache cho results
        TTL: Implicit (LRU eviction)
        """
        # Real search logic
        results = await self._do_search(...)
        return json.dumps(results)
    
    async def search(self, user_id: str, query: str, ...) -> List[Dict]:
        query_hash = hashlib.md5(query.encode()).hexdigest()
        
        # Try L1 cache
        cached_json = self._cached_search(user_id, query_hash, limit, score_threshold)
        return json.loads(cached_json)
```

**Config:**

- **Max size:** 1,000 entries (~10MB RAM)
- **Eviction:** LRU (Least Recently Used)
- **Hit rate target:** 20-30% (hot queries trong 1 instance)

---

#### **Layer 2: Redis Distributed Cache**

```python
class RedisCacheService:
    def __init__(self):
        self.redis = redis.Redis()
        self.default_ttl = 3600  # 1 hour
    
    def get_search_result(self, user_id: str, query: str, filters: Dict) -> Optional[List]:
        """
        Distributed cache cho search results
        """
        cache_key = self._build_cache_key(user_id, query, filters)
        
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        return None
    
    def set_search_result(self, user_id: str, query: str, filters: Dict, results: List):
        cache_key = self._build_cache_key(user_id, query, filters)
        
        # Cache v·ªõi TTL dynamic based on result freshness
        ttl = self._calculate_ttl(results)
        
        self.redis.setex(cache_key, ttl, json.dumps(results))
        
        # Track cache keys cho invalidation
        self._add_to_user_cache_index(user_id, cache_key)
    
    def _build_cache_key(self, user_id: str, query: str, filters: Dict) -> str:
        """
        Key format: search:v2:{user_id}:{query_hash}:{filter_hash}
        """
        query_hash = hashlib.md5(query.encode()).hexdigest()[:12]
        filter_hash = hashlib.md5(json.dumps(filters, sort_keys=True).encode()).hexdigest()[:8]
        return f"search:v2:{user_id}:{query_hash}:{filter_hash}"
    
    def _calculate_ttl(self, results: List) -> int:
        """
        Dynamic TTL based on result characteristics
        """
        if not results:
            return 300  # 5 min cho empty results
        
        # N·∫øu c√≥ nhi·ªÅu results v·ªõi high confidence ‚Üí cache l√¢u h∆°n
        avg_score = sum(r.get('score', 0) for r in results) / len(results)
        if avg_score > 0.8:
            return 7200  # 2 hours
        elif avg_score > 0.5:
            return 3600  # 1 hour
        else:
            return 1800  # 30 min
    
    def _add_to_user_cache_index(self, user_id: str, cache_key: str):
        """
        Maintain index c·ªßa cache keys per user
        D√πng Redis Set ƒë·ªÉ track
        """
        index_key = f"cache_index:{user_id}"
        self.redis.sadd(index_key, cache_key)
        self.redis.expire(index_key, 86400)  # 24h TTL cho index
```

**Config:**

- **Memory limit:** 2GB (Redis maxmemory)
- **Eviction policy:** `allkeys-lru`
- **Hit rate target:** 40-60%

---

#### **Layer 3: Vector Search (No Cache)**

```python
async def _do_search(
    self, 
    user_id: str, 
    query: str,
    limit: int,
    score_threshold: float
) -> List[Dict]:
    """
    Th·ª±c hi·ªán search th·∫≠t t·ª´ Milvus
    """
    # 1. Get query embedding (from L0 Embedding Cache)
    query_vector = await embedding_cache.get_embedding(query)
    
    # 2. Vector search in Milvus
    results = await milvus_service.search(
        collection="memories",
        query_vector=query_vector,
        filter={"user_id": user_id},
        limit=limit,
        score_threshold=score_threshold
    )
    
    # 3. Enrich v·ªõi graph data (optional)
    enriched = await graph_service.augment(results)
    
    return enriched
```

---

### **üî• GRANULAR CACHE INVALIDATION (Best Practice)**

```python
class CacheInvalidationService:
    """
    Smart invalidation: Ch·ªâ invalidate queries b·ªã ·∫£nh h∆∞·ªüng
    """
    
    async def on_facts_extracted(self, user_id: str, new_facts: List[Dict]):
        """
        ƒê∆∞·ª£c g·ªçi khi extraction job ho√†n th√†nh
        """
        # 1. Identify affected cache keys
        affected_keys = await self._find_affected_keys(user_id, new_facts)
        
        # 2. Delete only affected keys
        if affected_keys:
            self.redis.delete(*affected_keys)
        
        # 3. Log invalidation
        logger.info(f"Invalidated {len(affected_keys)} cache keys for user {user_id}")
    
    async def _find_affected_keys(self, user_id: str, new_facts: List[Dict]) -> List[str]:
        """
        Logic ƒë·ªÉ identify keys c·∫ßn invalidate
        
        Strategy:
        - N·∫øu new_facts ch·ª©a entity "Sparky" (dog name)
          ‚Üí Invalidate c√°c queries c√≥ ch·ª©a "dog", "pet", "Sparky"
        """
        # Get all cache keys c·ªßa user n√†y
        index_key = f"cache_index:{user_id}"
        all_keys = self.redis.smembers(index_key)
        
        affected_keys = []
        
        # Extract keywords t·ª´ new facts
        keywords = self._extract_keywords(new_facts)
        
        for key in all_keys:
            # Parse query t·ª´ cache key
            query = self._extract_query_from_key(key)
            
            # Check overlap v·ªõi keywords
            if self._has_overlap(query, keywords):
                affected_keys.append(key)
        
        return affected_keys
    
    def _extract_keywords(self, facts: List[Dict]) -> Set[str]:
        """
        Extract keywords t·ª´ facts ƒë·ªÉ match v·ªõi queries
        """
        keywords = set()
        for fact in facts:
            # Simple tokenization
            tokens = fact.get('fact_value', '').lower().split()
            keywords.update(tokens)
            
            # Add category/type
            if 'fact_type' in fact:
                keywords.update(fact['fact_type'])
        
        return keywords
    
    def _has_overlap(self, query: str, keywords: Set[str]) -> bool:
        """
        Check n·∫øu query c√≥ overlap v·ªõi keywords
        """
        query_tokens = set(query.lower().split())
        return bool(query_tokens & keywords)
```

**L·ª£i √≠ch:**

- ‚úÖ **Preserve unaffected cache** - kh√¥ng nuke to√†n b·ªô
- ‚úÖ **Avoid thundering herd** - invalidate gradually
- ‚úÖ **Better hit rate** - 60-70% vs 20-30% v·ªõi version tag

---

### **üìä MONITORING & METRICS**

```python
class CacheMetrics:
    """
    Track cache performance
    """
    
    def __init__(self):
        self.prometheus_registry = CollectorRegistry()
        
        # Cache hit rate metrics
        self.hit_counter = Counter(
            'cache_hits_total',
            'Total cache hits',
            ['layer']  # L1, L2
        )
        
        self.miss_counter = Counter(
            'cache_misses_total',
            'Total cache misses',
            ['layer']
        )
        
        # Latency histograms
        self.latency_histogram = Histogram(
            'search_latency_seconds',
            'Search latency',
            ['cache_status'],  # hit_l1, hit_l2, miss
            buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5]
        )
    
    def record_hit(self, layer: str):
        self.hit_counter.labels(layer=layer).inc()
    
    def record_miss(self, layer: str):
        self.miss_counter.labels(layer=layer).inc()
    
    def record_latency(self, cache_status: str, latency_seconds: float):
        self.latency_histogram.labels(cache_status=cache_status).observe(latency_seconds)
```

**Grafana Dashboard:**

```yaml
panels:
  - title: "Cache Hit Rate"
    targets:
      - expr: |
          sum(rate(cache_hits_total[5m])) / 
          (sum(rate(cache_hits_total[5m])) + sum(rate(cache_misses_total[5m])))
    threshold: 0.4  # Alert if < 40%
  
  - title: "P95 Latency by Cache Status"
    targets:
      - expr: histogram_quantile(0.95, search_latency_seconds)
```

---

### **üí∞ COST-BENEFIT ANALYSIS**

|Metric|Without Cache|With Optimized 3-Layer|Improvement|
|---|---|---|---|
|**P95 Latency**|250ms|**<50ms**|**80% faster**|
|**OpenAI API Cost**|$500/month|**$100/month**|**80% savings**|
|**Infrastructure**|-|+$50/month (Redis)|ROI: 9x|
|**Cache Hit Rate**|0%|**60-70%**|-|
|**P99 Latency**|500ms|**<150ms**|**70% faster**|

---

### **üéØ IMPLEMENTATION ROADMAP**

**Phase 1: Foundation (Week 1-2)**

- ‚úÖ Implement L0 Embedding Cache
- ‚úÖ Implement L2 Redis Cache v·ªõi basic key structure
- ‚úÖ Add cache metrics & monitoring

**Phase 2: Optimization (Week 3-4)**

- ‚úÖ Implement L1 LRU Cache
- ‚úÖ Implement Granular Invalidation
- ‚úÖ A/B test cache TTL values

**Phase 3: Advanced (Week 5-6)**

- ‚úÖ Implement semantic similarity cho cache key matching
- ‚úÖ Auto-tune TTL d·ª±a tr√™n user behavior
- ‚úÖ Pre-warming cho VIP users

---

## üé¨ K·∫æT LU·∫¨N

**Lo·∫°i b·ªè:**

- ‚ùå Proactive Caching Worker (kh√¥ng c·∫ßn thi·∫øt)
- ‚ùå L2 PostgreSQL Materialized View (d∆∞ th·ª´a)
- ‚ùå Version Tag Invalidation (inefficient)

**Gi·ªØ l·∫°i & T·ªëi ∆∞u:**

- ‚úÖ **3-Layer Reactive Cache** (L0 Embedding + L1 LRU + L2 Redis)
- ‚úÖ **Granular Invalidation** (ch·ªâ x√≥a keys b·ªã ·∫£nh h∆∞·ªüng)
- ‚úÖ **Dynamic TTL** (based on result quality)
- ‚úÖ **Comprehensive Monitoring**

**K·∫øt qu·∫£ k·ª≥ v·ªçng:**

- üöÄ P95 latency: **<50ms** (gi·∫£m 80%)
- üí∞ Chi ph√≠: **-$400/month** ti·∫øt ki·ªám
- üìà Cache hit rate: **60-70%**
- üéØ ƒê·∫°t production-grade standards

# V3 - Perflexity: trong mem0 OSS h·ªó tr·ª£ nh·ªØng c√°ch n√†o cho shorrt term v√† long term

D·ª±a tr√™n t√†i li·ªáu v√† research, ƒë√¢y l√† c√°ch **Mem0 OSS h·ªó tr·ª£ Short-Term v√† Long-Term Memory**:

***

## üß† **MEM0 OSS: SHORT-TERM vs LONG-TERM MEMORY**

### **üìä Overview Table**

| Memory Type | Mem0 OSS Support | Storage | Lifetime | Use Case |
| :-- | :-- | :-- | :-- | :-- |
| **Short-Term Memory (STM)** | ‚úÖ Conversation/Session-scoped | In-memory context | Minutes to hours | Current conversation flow |
| **Long-Term Memory (LTM)** | ‚úÖ User/Agent-scoped | Vector DB + Graph DB | Persistent (days/months/years) | Cross-session personalization |


***

## üîç **1. SHORT-TERM MEMORY (STM) - Mem0 OSS**

### **C√°ch Mem0 H·ªó Tr·ª£:**

Mem0 **KH√îNG t·ª± ƒë·ªông qu·∫£n l√Ω STM** trong context window. B·∫°n c·∫ßn t·ª± implement:

```python
from mem0 import Memory

memory = Memory()

# Short-term memory = conversation history trong session hi·ªán t·∫°i
conversation_history = [
    {"role": "user", "content": "T√¥i th√≠ch pizza"},
    {"role": "assistant", "content": "Tuy·ªát! B·∫°n th√≠ch lo·∫°i pizza n√†o?"},
    {"role": "user", "content": "Pizza h·∫£i s·∫£n"}
]

# Mem0 KH√îNG l∆∞u conversation history n√†y
# B·∫°n ph·∫£i t·ª± qu·∫£n l√Ω trong application state ho·∫∑c Redis
```

**Implementation Pattern:**

```python
class ShortTermMemory:
    """
    Application-level short-term memory
    (Mem0 kh√¥ng handle ph·∫ßn n√†y)
    """
    def __init__(self):
        self.redis = redis.Redis()
    
    def store_conversation(self, session_id: str, messages: List[Dict]):
        """L∆∞u conversation v√†o Redis v·ªõi TTL ng·∫Øn"""
        key = f"stm:session:{session_id}"
        self.redis.setex(
            key,
            3600,  # 1 hour TTL
            json.dumps(messages)
        )
    
    def get_conversation(self, session_id: str) -> List[Dict]:
        """L·∫•y conversation t·ª´ Redis"""
        key = f"stm:session:{session_id}"
        data = self.redis.get(key)
        return json.loads(data) if data else []
```

**K·∫øt lu·∫≠n:** Mem0 OSS **KH√îNG qu·∫£n l√Ω STM**. B·∫°n c·∫ßn t·ª± implement b·∫±ng Redis ho·∫∑c in-memory cache.[^1][^2][^3]

***

## üèõÔ∏è **2. LONG-TERM MEMORY (LTM) - Mem0 OSS (CORE FEATURE)**

### **C√°ch Mem0 H·ªó Tr·ª£:**

Mem0 OSS **chuy√™n v·ªÅ LTM** - ƒë√¢y l√† core feature ch√≠nh.[^4][^2][^3][^5][^1]

```python
from mem0 import Memory

# Initialize Mem0 OSS
config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "host": "localhost",
            "port": 6333,
            "collection_name": "memories"
        }
    },
    "llm": {
        "provider": "openai",
        "config": {
            "model": "gpt-4o-mini",
            "temperature": 0.0
        }
    },
    "embedder": {
        "provider": "openai",
        "config": {
            "model": "text-embedding-3-small"
        }
    }
}

memory = Memory.from_config(config)

# ADD: L∆∞u long-term memory
memory.add(
    messages=[
        {"role": "user", "content": "T√¥i th√≠ch ƒÉn pizza"},
        {"role": "assistant", "content": "ƒê√£ l∆∞u!"}
    ],
    user_id="user_123"
)
# ‚Üí Mem0 t·ª± ƒë·ªông extract "User likes pizza" v√† l∆∞u v√†o Vector DB

# SEARCH: T√¨m long-term memory (cross-session)
results = memory.search(
    query="User th√≠ch ƒÉn g√¨?",
    user_id="user_123"
)
# ‚Üí Return: ["User likes pizza"] (ngay c·∫£ sau nhi·ªÅu ng√†y)
```


***

## üìã **3. MEM0 OSS MEMORY TYPES (Chi Ti·∫øt)**

Mem0 OSS ph√¢n lo·∫°i memory theo **4 lo·∫°i** (t·∫•t c·∫£ ƒë·ªÅu l√† LTM):[^2][^3][^1]

### **3.1. Conversation Memory (Shortest LTM)**

```python
# L∆∞u memory theo conversation_id
memory.add(
    "I'm working on project Alpha",
    user_id="user_123",
    metadata={"conversation_id": "conv_456"}
)

# Ch·ªâ √°p d·ª•ng cho conversation n√†y
# Lifetime: Minutes to hours (configurable)
```


### **3.2. Session Memory (Short LTM)**

```python
# L∆∞u memory theo session
memory.add(
    "Currently browsing electronics section",
    user_id="user_123",
    metadata={"session_id": "sess_789"}
)

# Lifetime: Hours (e.g., 1 day session)
```


### **3.3. User Memory (Long LTM)**

```python
# L∆∞u memory vƒ©nh vi·ªÖn cho user
memory.add(
    "User prefers dark mode",
    user_id="user_123"
)

# Lifetime: Weeks to forever (no expiration)
```


### **3.4. Organization Memory (Shared LTM)**

```python
# L∆∞u memory chung cho org
memory.add(
    "Company policy: remote work on Fridays",
    metadata={"org_id": "org_001"}
)

# Lifetime: Configured globally
```


***

## ‚è∞ **4. EXPIRATION / TTL (Mem0 OSS v1.0+)**

Mem0 OSS **h·ªó tr·ª£ expiration** ƒë·ªÉ implement short-term vs long-term:[^6]

```python
from datetime import datetime, timedelta

# SHORT-TERM: Expires in 7 days
expires_at = (datetime.now() + timedelta(days=7)).isoformat()
memory.add(
    "Currently browsing electronics",
    user_id="sarah",
    expiration_date=expires_at  # ‚Üê Key feature!
)

# LONG-TERM: No expiration (persists forever)
memory.add(
    "User prefers dark mode",
    user_id="sarah"
    # No expiration_date = permanent
)
```

**7 ng√†y sau:**

```python
# Short-term memory ƒë√£ expired
results = memory.search("browsing", user_id="sarah")
# ‚Üí Empty (ƒë√£ h·∫øt h·∫°n)

# Long-term memory v·∫´n c√≤n
results = memory.search("dark mode", user_id="sarah")
# ‚Üí Return: "User prefers dark mode" ‚úÖ
```


***

## üîÑ **5. WORKFLOW: PH·ªêI H·ª¢P STM + LTM V·ªöI MEM0 OSS**

### **Recommended Pattern:**

```python
class HybridMemorySystem:
    """
    K·∫øt h·ª£p STM (t·ª± qu·∫£n l√Ω) + LTM (Mem0 OSS)
    """
    
    def __init__(self):
        # STM: Redis (application-managed)
        self.stm = ShortTermMemory()
        
        # LTM: Mem0 OSS (managed by Mem0)
        self.ltm = Memory.from_config(config)
    
    async def process_message(
        self,
        user_id: str,
        session_id: str,
        message: str
    ):
        """Process user message with hybrid memory"""
        
        # 1. Get STM (recent conversation)
        conversation = self.stm.get_conversation(session_id)
        
        # 2. Get LTM (relevant long-term facts)
        ltm_facts = self.ltm.search(
            query=message,
            user_id=user_id,
            limit=5
        )
        
        # 3. Combine STM + LTM for LLM prompt
        context = {
            "conversation": conversation,  # STM
            "user_facts": ltm_facts       # LTM from Mem0
        }
        
        # 4. Generate response
        response = await llm.generate(message, context)
        
        # 5. Update STM
        conversation.append({"role": "user", "content": message})
        conversation.append({"role": "assistant", "content": response})
        self.stm.store_conversation(session_id, conversation)
        
        # 6. Extract important facts ‚Üí LTM (async)
        asyncio.create_task(
            self.ltm.add(
                messages=conversation[-2:],  # Last 2 messages
                user_id=user_id
            )
        )
        
        return response
```


***

## üìä **6. SO S√ÅNH: STM vs LTM TRONG MEM0 OSS**

| Aspect | Short-Term Memory | Long-Term Memory |
| :-- | :-- | :-- |
| **Mem0 Managed?** | ‚ùå NO (b·∫°n t·ª± implement) | ‚úÖ YES (core feature) |
| **Storage** | Redis / In-memory | Vector DB (Qdrant/Milvus) + Graph DB |
| **Lifetime** | Minutes to hours | Days to forever |
| **Scope** | Per session/conversation | Per user/agent |
| **Expiration** | Auto (TTL trong Redis) | Optional (expiration_date) |
| **Search Method** | Exact match / Key-based | Semantic vector search |
| **Use Case** | "V·ª´a n√≥i g√¨?" | "S·ªü th√≠ch t·ª´ tr∆∞·ªõc t·ªõi nay?" |


***

## ‚úÖ **K·∫æT LU·∫¨N: MEM0 OSS H·ªñ TR·ª¢ G√å?**

### **‚úÖ Mem0 OSS H·ªñ TR·ª¢:**

1. **Long-Term Memory (Core):**
    - Vector storage (Qdrant/Milvus)
    - Semantic search
    - Graph relationships (Neo4j)
    - Expiration support (TTL)
    - User/Agent/Org scoping
2. **Memory Types:**
    - Conversation memory (short LTM)
    - Session memory (medium LTM)
    - User memory (long LTM)
    - Organization memory (shared LTM)

### **‚ùå Mem0 OSS KH√îNG H·ªñ TR·ª¢:**

1. **Short-Term Memory (Conversation History):**
    - B·∫°n ph·∫£i t·ª± qu·∫£n l√Ω v·ªõi Redis/In-memory
    - Mem0 kh√¥ng store conversation history
2. **Context Window Management:**
    - Mem0 kh√¥ng replace LLM context window
    - B·∫°n v·∫´n c·∫ßn qu·∫£n l√Ω prompt engineering

***

## üéØ **RECOMMENDATION CHO PIKA:**

```python
# Chi·∫øn l∆∞·ª£c ƒë·ªÅ xu·∫•t:

# STM (Self-managed):
- Redis v·ªõi TTL 1 hour
- Key: stm:session:{session_id}
- Store: Conversation history (messages list)

# LTM (Mem0 OSS):
- Qdrant vector store
- Neo4j graph store (relationships)
- Expiration: 
  * Session facts: 7 days
  * User preferences: No expiration
```

**K·∫øt lu·∫≠n:** Mem0 OSS **chuy√™n v·ªÅ LTM**, b·∫°n c·∫ßn t·ª± implement STM b·∫±ng Redis ho·∫∑c application state.[^3][^5][^1][^2][^6]
<span style="display:none">[^10][^11][^12][^13][^14][^15][^16][^17][^18][^19][^20][^21][^22][^23][^24][^25][^7][^8][^9]</span>

<div align="center">‚ÅÇ</div>

[^1]: https://docs.mem0.ai/core-concepts/memory-types

[^2]: https://aws.amazon.com/blogs/database/build-persistent-memory-for-agentic-ai-applications-with-mem0-open-source-amazon-elasticache-for-valkey-and-amazon-neptune-analytics/

[^3]: https://dev.to/yigit-konur/mem0-the-comprehensive-guide-to-building-ai-with-persistent-memory-fbm

[^4]: https://mem0.ai

[^5]: https://github.com/mem0ai/mem0

[^6]: https://docs.mem0.ai/cookbooks/essentials/memory-expiration-short-and-long-term

[^7]: Step-2-Output-1-SDD.md

[^8]: MEM0_OSS_SETUP.md

[^9]: Step-2-Output-2-SDD-HLD-LLD-co-ca-Optimize-Response-Time.md

[^10]: report.md

[^11]: report.md

[^12]: https://viblo.asia/p/mem0-kien-truc-long-term-memory-cho-he-thong-ai-agent-G24B88pOLz3

[^13]: https://pypi.org/project/mem0ai/0.1.0/

[^14]: https://www.ycombinator.com/launches/LpA-mem0-open-source-memory-layer-for-ai-apps

[^15]: https://www.datacamp.com/tutorial/mem0-tutorial

[^16]: https://docs.mem0.ai/open-source/overview

[^17]: https://docs.crewai.com/en/concepts/memory

[^18]: https://skywork.ai/skypage/en/A-Comprehensive-Guide-to-the-Mem0-MCP-Server-Building-AI-with-Persistent-Memory/1971044006807793664

[^19]: https://arxiv.org/abs/2504.19413

[^20]: https://docs.mem0.ai/open-source/python-quickstart

[^21]: https://viblo.asia/p/gioi-thieu-ve-mem0-xay-dung-ai-agents-voi-long-term-memory-AY4qQN8q4Pw

[^22]: https://arxiv.org/pdf/2504.19413.pdf

[^23]: https://blog.stackademic.com/mem0-memo-ai-memory-layer-purpose-and-core-functionality-375cc5a2bfd0

[^24]: https://fosterfletcher.com/ai-memory-infrastructure/

[^25]: https://docs.mem0.ai/v0x/core-concepts/memory-types


---

<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# V4 - üìä **QUICK SUMMARY: STM + LTM CACHING**

### **SHORT-TERM MEMORY (STM) - 2 Layers**

```
L0: In-Memory (current request)
    ‚îî‚îÄ <1ms, 95% hit rate

L1: Redis (1 hour TTL)
    ‚îî‚îÄ 5ms, 70% hit rate
```


### **LONG-TERM MEMORY (LTM) - 4 Layers (CRITICAL!)**

```
L0: Redis Hot Facts (top 10, 5 min)
    ‚îî‚îÄ <2ms, 75% hit rate
    ‚îî‚îÄ "What do I like?" ‚Üí instant

L1: Redis Search Results (smart TTL)
    ‚îî‚îÄ 5ms, 40% hit rate
    ‚îî‚îÄ "User favorite?" ‚Üí cached

L2: PostgreSQL Metadata (24 hours)
    ‚îî‚îÄ 20ms, 30% hit rate
    ‚îî‚îÄ For filtering/ranking

L3: Mem0 Vector DB (NO CACHE)
    ‚îî‚îÄ 50-150ms, always fresh
    ‚îî‚îÄ Primary source

L4: Neo4j Graph (NO CACHE)
    ‚îî‚îÄ 50-100ms, always fresh
    ‚îî‚îÄ Relationships
```


***

## üîë **KEY INSIGHT: T·∫°i Sao LTM C≈©ng C·∫ßn Caching?**

```
N·∫øu KH√îNG cache LTM:
  User h·ªèi "What do I like?" l·∫ßn 1 ‚Üí 150ms (Mem0 search)
  User h·ªèi "What do I like?" l·∫ßn 2 (1 ph√∫t sau) ‚Üí 150ms (SAME QUERY!)
  
  Problem: T·ªën ti·ªÅn OpenAI API l·∫°i ch·∫≠m

N·∫øu cache LTM (chi·∫øn l∆∞·ª£c c·ªßa ch√∫ng ta):
  User h·ªèi "What do I like?" l·∫ßn 1 ‚Üí 150ms (Mem0 search)
  User h·ªèi "What do I like?" l·∫ßn 2 (1 ph√∫t sau) ‚Üí 5ms (CACHED!) ‚ö°
  
  Benefit: Nhanh g·∫•p 30x + ti·∫øt ki·ªám API cost
```


***

## üéØ **V√ÄI ƒêI·ªÇM QUAN TR·ªåNG**

| Aspect | STM | LTM |
| :-- | :-- | :-- |
| **Scope** | Per session | Per user (cross-session) |
| **Lifetime** | 1 hour | Days/months (configurable) |
| **Invalidation** | Session end | Granular (affected queries) |
| **Mem0 Role** | None | Primary source (L3+L4) |
| **Cache Benefit** | Avoid re-reading conversation | Avoid re-searching vector DB |
| **TTL Strategy** | Fixed (1h) | Smart (based on confidence) |


***

**File ƒë√£ s·∫µn s√†ng cho implementation! B·∫°n mu·ªën detail th√™m ph·∫ßn n√†o kh√¥ng? üöÄ**

