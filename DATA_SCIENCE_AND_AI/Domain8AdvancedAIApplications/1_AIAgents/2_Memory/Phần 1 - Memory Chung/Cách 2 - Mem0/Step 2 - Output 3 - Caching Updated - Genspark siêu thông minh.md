# V1 - manus: BÃO CÃO MECE Tá»I Æ¯U RESPONSE TIME CHO PIKA MEMORY SYSTEM

**Má»¥c tiÃªu:** Äáº¡t P95 Latency < 200ms cho `search_facts` API.

**TÃ¡c giáº£:** Manus AI (Lead Architect) | **NgÃ y:** 2025-12-21

---

## 1. PHÃ‚N TÃCH MECE CÃC ÄIá»‚M NGHáº¼N LATENCY

Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu P95 < 200ms, chÃºng ta cáº§n phÃ¢n tÃ­ch Response Time (RT) theo cÃ´ng thá»©c:
$$RT = T_{API} + T_{Cache} + T_{Embedding} + T_{VectorSearch} + T_{Graph} + T_{Network}$$

CÃ¡c Ä‘iá»ƒm ngháº½n chÃ­nh Ä‘Æ°á»£c phÃ¢n tÃ­ch theo nguyÃªn táº¯c MECE:

| NhÃ³m Yáº¿u Tá»‘ | Äiá»ƒm Ngháº½n Cá»¥ Thá»ƒ | Latency Æ¯á»›c TÃ­nh (Worst Case) | Chiáº¿n LÆ°á»£c Tá»‘i Æ¯u |
|---|---|---|---|
| **I. External Dependency** | **Embedding API Call (OpenAI)** | 100ms - 200ms | **Semantic Caching** (Loáº¡i bá» 90% cuá»™c gá»i) |
| **II. Core Search** | **Vector Search (Milvus)** | 50ms - 150ms | **Index Optimization** (HNSW/CAGRA) & **GPU Acceleration** |
| **III. Data Enrichment** | **Graph Traversal (Neo4j)** | 50ms - 100ms | **Asynchronous Traversal** & **Caching** |
| **IV. Internal I/O** | **Internal Network Latency** | 1ms - 5ms | **Service Colocation** (Same AZ/VPC) |
| **V. Application Overhead** | **Serialization/Deserialization** | 5ms - 10ms | **Protocol Buffers** (thay cho JSON) |

---

## 2. ÄÃNH GIÃ VÃ€ Cáº¢I TIáº¾N CHIáº¾N LÆ¯á»¢C CACHING ÄA Táº¦NG

Chiáº¿n lÆ°á»£c caching Ä‘a táº§ng (L1/L2/L3) lÃ  ná»n táº£ng vá»¯ng cháº¯c. Tuy nhiÃªn, Ä‘á»ƒ Ä‘áº¡t P95/P99, cáº§n bá»• sung **Semantic Caching cho Embedding** vÃ  tá»‘i Æ°u hÃ³a chiáº¿n lÆ°á»£c Invalidation.

### 2.1. Cáº£i Tiáº¿n Cáº¥u TrÃºc Cache

| Lá»›p Cache | Loáº¡i Cache | Cáº£i Tiáº¿n Äá» Xuáº¥t | Má»¥c TiÃªu Latency |
|---|---|---|---|
| **L0: Embedding Cache** | Redis (Key: `hash(query)`) | **Má»›i:** Cache vector embedding. | **< 5ms** (Loáº¡i bá» 100-200ms) |
| **L1: In-Memory** | `@lru_cache` | Cache káº¿t quáº£ cuá»‘i cÃ¹ng (Final Result Cache). | **< 1ms** |
| **L2: Redis Semantic Cache** | Redis (Key: `search:{user_id}:{hash(query)}`) | Cache káº¿t quáº£ tÃ¬m kiáº¿m cuá»‘i cÃ¹ng. | **5ms - 20ms** |
| **L3: Persistent Cache** | PostgreSQL | Cache káº¿t quáº£ dÃ i háº¡n (Long-tail queries). | **50ms - 100ms** |

### 2.2. Chiáº¿n LÆ°á»£c Cache Invalidation (World-Class)

Chiáº¿n lÆ°á»£c **Cache Tagging** lÃ  giáº£i phÃ¡p tá»‘i Æ°u nháº¥t cho há»‡ thá»‘ng Ä‘a ngÆ°á»i dÃ¹ng:

1.  **Tagging:** Má»—i `user_id` Ä‘Æ°á»£c gÃ¡n má»™t `version_tag` (vÃ­ dá»¥: timestamp hoáº·c UUID) Ä‘Æ°á»£c lÆ°u trong Redis.
2.  **Key Generation:** `cache_key` sáº½ lÃ  `search:{user_id}:{version_tag}:{hash(query)}`.
3.  **Invalidation:** Khi `extract_facts` hoÃ n thÃ nh, worker chá»‰ cáº§n **tÄƒng `version_tag`** cá»§a `user_id` Ä‘Ã³ trong Redis.
4.  **Hiá»‡u quáº£:** Táº¥t cáº£ cÃ¡c query cÅ© cá»§a user Ä‘Ã³ sáº½ tá»± Ä‘á»™ng bá»‹ miss cache (stale) vÃ  buá»™c pháº£i cháº¡y láº¡i, trong khi cÃ¡c user khÃ¡c khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng.

---

## 3. PHÃ‚N TÃCH MECE CÃC Yáº¾U Tá» Tá»I Æ¯U HÃ“A (ULTIMATE LATENCY REDUCTION)

CÃ¡c ká»¹ thuáº­t tá»‘i Æ°u hÃ³a Ä‘Æ°á»£c phÃ¢n tÃ­ch theo 3 trá»¥ cá»™t chÃ­nh: **Compute**, **Data Structure**, vÃ  **Network**.

### 3.1. Tá»‘i Æ¯u HÃ³a Compute (Giáº£m thá»i gian xá»­ lÃ½)

| Ká»¹ Thuáº­t                      | MÃ´ Táº£                                                                               | áº¢nh HÆ°á»Ÿng Latency                                                |
| ----------------------------- | ----------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **GPU Acceleration (Milvus)** | Triá»ƒn khai Milvus vá»›i index **CAGRA** hoáº·c **RAPIDS** trÃªn mÃ¡y chá»§ cÃ³ GPU (NVIDIA). | Giáº£m thá»i gian tÃ¬m kiáº¿m vector tá»« 50-150ms xuá»‘ng **< 10ms** [1]. |
| **Asynchronous Parallelism**  | Sá»­ dá»¥ng `asyncio.gather` Ä‘á»ƒ gá»i Milvus, Neo4j, vÃ  Redis song song.                  | Giáº£m tá»•ng thá»i gian chá» I/O.                                     |
| **Batching**                  | Gom nhiá»u query nhá» thÃ nh má»™t batch lá»›n hÆ¡n Ä‘á»ƒ gá»i Embedding API (náº¿u cÃ³ thá»ƒ).      | Giáº£m overhead cá»§a má»—i API call.                                  |

### 3.2. Tá»‘i Æ¯u HÃ³a Data Structure (TÄƒng tá»‘c Ä‘á»™ truy cáº­p)

| Ká»¹ Thuáº­t | MÃ´ Táº£ | áº¢nh HÆ°á»Ÿng Latency |
|---|---|---|
| **Vector Index Tuning** | Sá»­ dá»¥ng **HNSW** (Hierarchical Navigable Small World) thay vÃ¬ IVF_FLAT, vá»›i cÃ¡c tham sá»‘ `efConstruction` vÃ  `efSearch` Ä‘Æ°á»£c tinh chá»‰nh. | TÄƒng tá»‘c Ä‘á»™ tÃ¬m kiáº¿m vÃ  Ä‘á»™ chÃ­nh xÃ¡c (Recall) [2]. |
| **Filtering/Pre-ranking** | Sá»­ dá»¥ng **Scalar Filtering** cá»§a Milvus (trÃªn `user_id`, `fact_type`) Ä‘á»ƒ giáº£m khÃ´ng gian tÃ¬m kiáº¿m trÆ°á»›c khi tÃ­nh toÃ¡n vector distance. | Giáº£m Ä‘Ã¡ng ká»ƒ thá»i gian tÃ¬m kiáº¿m trÃªn cÃ¡c táº­p dá»¯ liá»‡u lá»›n. |
| **Data Colocation** | Äáº£m báº£o cÃ¡c trÆ°á»ng dá»¯ liá»‡u thÆ°á»ng xuyÃªn Ä‘Æ°á»£c truy cáº­p (fact content, metadata) Ä‘Æ°á»£c lÆ°u trá»¯ cÃ¹ng vá»›i vector trong Milvus (hoáº·c trong Redis Cache). | Giáº£m thiá»ƒu cÃ¡c cuá»™c gá»i join/lookup giá»¯a Milvus vÃ  PostgreSQL. |

### 3.3. Tá»‘i Æ¯u HÃ³a Network (Giáº£m thá»i gian truyá»n táº£i)

| Ká»¹ Thuáº­t | MÃ´ Táº£ | áº¢nh HÆ°á»Ÿng Latency |
|---|---|---|
| **Service Colocation** | Äáº·t API Server, Redis, Milvus trong cÃ¹ng má»™t **Availability Zone (AZ)** vÃ  **Virtual Private Cloud (VPC)**. | Äáº£m báº£o Ä‘á»™ trá»… ná»™i bá»™ **< 1ms**. |
| **Protocol Optimization** | Sá»­ dá»¥ng **Protocol Buffers** hoáº·c **MessagePack** thay vÃ¬ JSON cho cÃ¡c giao tiáº¿p ná»™i bá»™ (API Server <-> Worker/DB). | Giáº£m kÃ­ch thÆ°á»›c payload vÃ  thá»i gian Serialization/Deserialization. |
| **Connection Pooling** | Sá»­ dá»¥ng Connection Pooling (vÃ­ dá»¥: `pgBouncer` cho PostgreSQL, `connection pool` cho Milvus) Ä‘á»ƒ trÃ¡nh overhead táº¡o káº¿t ná»‘i má»›i. | Giáº£m Ä‘á»™ trá»… káº¿t ná»‘i ban Ä‘áº§u (handshake latency). |

---

## 4. Káº¾T LUáº¬N VÃ€ ROADMAP Äá»€ XUáº¤T

Chiáº¿n lÆ°á»£c caching Ä‘a táº§ng lÃ  cáº§n thiáº¿t, nhÆ°ng khÃ´ng Ä‘á»§. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu P95 < 200ms, PIKA cáº§n thá»±c hiá»‡n má»™t chiáº¿n lÆ°á»£c tá»‘i Æ°u hÃ³a toÃ n diá»‡n.

| Giai Äoáº¡n | HÃ nh Äá»™ng ChÃ­nh | Má»¥c TiÃªu Latency |
|---|---|---|
| **Giai Ä‘oáº¡n 1 (Immediate)** | **Triá»ƒn khai L0/L2 Semantic Caching** (Embedding & Result Caching). | **P95 < 300ms** (Loáº¡i bá» Ä‘á»™ trá»… OpenAI) |
| **Giai Ä‘oáº¡n 2 (Short-Term)** | **Tá»‘i Æ°u Milvus Index** (HNSW tuning) vÃ  **Asynchronous Parallelism** (FastAPI `asyncio.gather`). | **P95 < 150ms** (Tá»‘i Æ°u hÃ³a Core Search) |
| **Giai Ä‘oáº¡n 3 (Long-Term)** | **ÄÃ¡nh giÃ¡ GPU Acceleration** (CAGRA/RAPIDS) vÃ  **Service Colocation** (VPC/AZ). | **P95 < 50ms** (World-Class Latency) |

**HÃ nh Ä‘á»™ng quan trá»ng nháº¥t:** Triá»ƒn khai **L0 Semantic Caching** Ä‘á»ƒ loáº¡i bá» sá»± phá»¥ thuá»™c vÃ o Ä‘á»™ trá»… cá»§a OpenAI API.

---

## TÃ€I LIá»†U THAM KHáº¢O

[1] M. Zhang, Y. He, "Zoom: Ssd-based vector search for optimizing accuracy, latency and memory," *arXiv preprint arXiv:1809.04067*, 2018.
[2] Y. Zhou, S. Lin, S. Gong, et al., "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector Nearest Neighbor Search," *arXiv preprint arXiv:2508.15694*, 2025.
[3] P99 CONF 2025, "Low-Latency Data 2025," *www.p99conf.io*.

[Zoom: Ssd-based vector search for optimizing accuracy, latency and memory](https://arxiv.org/abs/1809.04067)

â€¦ with low latency and memory footprint, which existing work fails to offer. We develop, Zoom, a new vector search solution that collaboratively optimizes accuracy, latency and memory â€¦

[Optimizing Matrix-Vector Operations with CGLA for High-Performance Approximate k-NN Search](https://ieeexplore.ieee.org/abstract/document/11048859/)

â€¦ movement and significantly reducing latency. Experimental evaluations demonstrate that, â€¦ conventional methods and making our solution highly suitable for real-time vector search â€¦

[Cost-Effective, Low Latency Vector Search with Azure Cosmos DB](https://arxiv.org/abs/2505.05885)

â€¦ We note that the p50, p95 and p99 latencies increase by less than 2Ã— despite the 100Ã— increase in index size. The RU charge similarly increases less than 2Ã— except in the case of Wiki-â€¦

[Optimizing Large Language Model Utilization through Scheduling Strategies](https://openresearch.newcastle.edu.au/ndownloader/files/55657901)

â€¦ for LLM allocation to enhance performance and reduce costs. â€¦ for providing me with world-class facilities that have supported â€¦ Adds latency and relies on LLMâ€™s self-assessment, which â€¦

[Intelligent dispatch method for power systems based on LLM knowledge injection](https://ieeexplore.ieee.org/abstract/document/10936149/)

â€¦ Networks, World-class" strategic initiative [3], providing a â€¦ the optimal power system dispatching strategy. Experimental â€¦ to control accuracy and latency in traditional power systems. â€¦

[KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and Runtime Logs Analysis](https://arxiv.org/abs/2509.04191)

â€¦ : a novel LLM-based framework for dynamically hardening K8s systems based on prompt-â€¦ summaries (reducing token count by 99.96%), enabling efficient LLM analysis within token â€¦

[GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector Nearest Neighbor Search](https://arxiv.org/abs/2508.15694)

â€¦ a core challenge for vector database systems. Traditional exact search methods suffer from the â€¦ , HNSW achieves efficient in-memory search through a multilayer hierarchical structure, â€¦

[Efficient Data Access Paths for Mixed Vector-Relational Search](https://dl.acm.org/doi/abs/10.1145/3662010.3663448)

â€¦ Small World) based on multi-layer graph structure for high accuâ€¦ batching the vectors to prioritize cache-local dense vector â€¦ if a vector index is not the main (vector) database structure [26]â€¦


---


# Ver 2 - Genspark: Dá»±a trÃªn phÃ¢n tÃ­ch cÃ¡c tÃ i liá»‡u thiáº¿t káº¿, tÃ´i sáº½ Ä‘Ã¡nh giÃ¡ chiáº¿n lÆ°á»£c caching hiá»‡n táº¡i vÃ  Ä‘á» xuáº¥t giáº£i phÃ¡p tá»‘i Æ°u:

---

## 1ï¸âƒ£ PHÃ‚N TÃCH Váº¤N Äá»€ CHIáº¾N LÆ¯á»¢C CACHING HIá»†N Táº I

### ğŸ”´ **Váº¥n Ä‘á» nghiÃªm trá»ng phÃ¡t hiá»‡n:**

#### **A. Thiáº¿t káº¿ Proactive Caching cÃ³ lá»—i kiáº¿n trÃºc nghiÃªm trá»ng**

**Váº¥n Ä‘á» 1: Pre-compute cho query cá»¥ thá»ƒ lÃ  Anti-pattern**

```python
# Code hiá»‡n táº¡i (SAI Láº¦M)
query = "user favorite (movie, character, pet, activity, friend, music, travel, toy)"
results = await self.mem0_service.search_memories(user_id=user_id, query=query, limit=50)
```

**Táº¡i sao SAI:**

- âŒ **Hard-coded query** - chá»‰ cache 1 pattern cá»‘ Ä‘á»‹nh
- âŒ **KhÃ´ng scale** - náº¿u user há»i â€œWhat movies do I like?â€ â†’ cache miss
- âŒ **Waste resources** - pre-compute hÃ ng triá»‡u user má»—i 30 phÃºt cho query hiáº¿m gáº·p
- âŒ **False assumption** - giáº£ Ä‘á»‹nh 99% query lÃ  â€œuser favoriteâ€ (khÃ´ng thá»±c táº¿)

**Váº¥n Ä‘á» 2: Layer L2 Materialized View dÆ° thá»«a**

```sql
CREATE TABLE user_favorite_summary (
    user_id VARCHAR(255) PRIMARY KEY,
    summary_json JSONB NOT NULL,
    last_updated TIMESTAMPTZ DEFAULT NOW()
);
```

**Táº¡i sao DÆ¯ THá»ªA:**

- Redis Ä‘Ã£ Ä‘á»§ nhanh (5-20ms) cho L2
- PostgreSQL JSONB query cháº­m hÆ¡n Redis (20-50ms)
- TÄƒng Ä‘á»™ phá»©c táº¡p sync data giá»¯a Redis â†” PostgreSQL
- Waste storage: duplicate data trong 2 stores

#### **B. Cache Invalidation Strategy sai**

```python
# Approach hiá»‡n táº¡i: Version Tag
cache_key = f"search:{user_id}:{version_tag}:{hash(query)}"
```

**Váº¥n Ä‘á»:**

- âŒ **Cold start** sau invalidation - táº¥t cáº£ queries cá»§a user Ä‘á»u miss cache
- âŒ **Thundering herd** - náº¿u 1000 users vá»«a update facts â†’ 1000 thundering herd problems
- âŒ **KhÃ´ng táº­n dá»¥ng Ä‘Æ°á»£c cached results cÅ©** cho queries khÃ´ng thay Ä‘á»•i

---

## 2ï¸âƒ£ CHIáº¾N LÆ¯á»¢C CACHING HIá»†U QUáº¢ NHáº¤T (PRODUCTION-GRADE)

### **ğŸ¯ NguyÃªn táº¯c thiáº¿t káº¿:**

1. **Reactive > Proactive** cho majority cá»§a queries
2. **Cache Result, NOT Intermediate Steps**
3. **Granular Invalidation** thay vÃ¬ nuke toÃ n bá»™
4. **Layered vá»›i Clear Responsibilities**

---

### **âœ… KIáº¾N TRÃšC CACHE Tá»I Æ¯U (3 LAYERS)**

```mermaid
graph TB
    Query[User Query] --> L1{L1: Application<br/>LRU Cache<br/>Latency: <1ms}
    
    L1 -->|Hit| Return1[Return <1ms]
    L1 -->|Miss| L2{L2: Redis<br/>Distributed Cache<br/>Latency: 5-20ms}
    
    L2 -->|Hit| UpdateL1[Update L1]
    UpdateL1 --> Return2[Return 5-20ms]
    
    L2 -->|Miss| L3[L3: Vector Search<br/>Milvus + LLM<br/>Latency: 100-300ms]
    
    L3 --> UpdateL2[Update L2]
    UpdateL2 --> UpdateL1_2[Update L1]
    UpdateL1_2 --> Return3[Return 100-300ms]
    
    style L1 fill:#4CAF50,color:#fff
    style L2 fill:#2196F3,color:#fff
    style L3 fill:#FF9800,color:#fff
```

---

### **ğŸ“‹ CHI TIáº¾T Tá»ªNG LAYER**

#### **Layer 0: Query Embedding Cache (NEW - Critical!)**

**Má»¥c Ä‘Ã­ch:** Cache embedding vectors Ä‘á»ƒ trÃ¡nh gá»i OpenAI API

```python
class EmbeddingCache:
    def __init__(self):
        self.redis = redis.Redis()
        
    async def get_embedding(self, text: str) -> List[float]:
        # Normalize query Ä‘á»ƒ tÄƒng hit rate
        normalized = self._normalize(text)
        cache_key = f"embed:{hash(normalized)}"
        
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        
        # Cache miss - call OpenAI
        embedding = await openai_service.get_embedding(text)
        
        # Cache vá»›i TTL dÃ i (embedding Ã­t thay Ä‘á»•i)
        self.redis.setex(cache_key, 7*24*3600, json.dumps(embedding))
        return embedding
    
    def _normalize(self, text: str) -> str:
        """Chuáº©n hÃ³a query Ä‘á»ƒ tÄƒng cache hit rate"""
        return text.lower().strip()
```

**Lá»£i Ã­ch:**

- âœ… **Giáº£m 80-90% OpenAI API calls** (100-200ms/call)
- âœ… **Giáº£m chi phÃ­** $0.0001/1K tokens â†’ tiáº¿t kiá»‡m $100+/thÃ¡ng
- âœ… **TÄƒng reliability** - giáº£m dependency vÃ o external API

---

#### **Layer 1: Application-Level LRU Cache**

```python
from functools import lru_cache
from typing import List, Dict

class SearchService:
    @lru_cache(maxsize=1000)
    async def _cached_search(
        self, 
        user_id: str, 
        query_hash: str,  # hash cá»§a query Ä‘á»ƒ lÃ m cache key
        limit: int,
        score_threshold: float
    ) -> str:  # Return JSON string (immutable for lru_cache)
        """
        In-memory cache cho results
        TTL: Implicit (LRU eviction)
        """
        # Real search logic
        results = await self._do_search(...)
        return json.dumps(results)
    
    async def search(self, user_id: str, query: str, ...) -> List[Dict]:
        query_hash = hashlib.md5(query.encode()).hexdigest()
        
        # Try L1 cache
        cached_json = self._cached_search(user_id, query_hash, limit, score_threshold)
        return json.loads(cached_json)
```

**Config:**

- **Max size:** 1,000 entries (~10MB RAM)
- **Eviction:** LRU (Least Recently Used)
- **Hit rate target:** 20-30% (hot queries trong 1 instance)

---

#### **Layer 2: Redis Distributed Cache**

```python
class RedisCacheService:
    def __init__(self):
        self.redis = redis.Redis()
        self.default_ttl = 3600  # 1 hour
    
    def get_search_result(self, user_id: str, query: str, filters: Dict) -> Optional[List]:
        """
        Distributed cache cho search results
        """
        cache_key = self._build_cache_key(user_id, query, filters)
        
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        return None
    
    def set_search_result(self, user_id: str, query: str, filters: Dict, results: List):
        cache_key = self._build_cache_key(user_id, query, filters)
        
        # Cache vá»›i TTL dynamic based on result freshness
        ttl = self._calculate_ttl(results)
        
        self.redis.setex(cache_key, ttl, json.dumps(results))
        
        # Track cache keys cho invalidation
        self._add_to_user_cache_index(user_id, cache_key)
    
    def _build_cache_key(self, user_id: str, query: str, filters: Dict) -> str:
        """
        Key format: search:v2:{user_id}:{query_hash}:{filter_hash}
        """
        query_hash = hashlib.md5(query.encode()).hexdigest()[:12]
        filter_hash = hashlib.md5(json.dumps(filters, sort_keys=True).encode()).hexdigest()[:8]
        return f"search:v2:{user_id}:{query_hash}:{filter_hash}"
    
    def _calculate_ttl(self, results: List) -> int:
        """
        Dynamic TTL based on result characteristics
        """
        if not results:
            return 300  # 5 min cho empty results
        
        # Náº¿u cÃ³ nhiá»u results vá»›i high confidence â†’ cache lÃ¢u hÆ¡n
        avg_score = sum(r.get('score', 0) for r in results) / len(results)
        if avg_score > 0.8:
            return 7200  # 2 hours
        elif avg_score > 0.5:
            return 3600  # 1 hour
        else:
            return 1800  # 30 min
    
    def _add_to_user_cache_index(self, user_id: str, cache_key: str):
        """
        Maintain index cá»§a cache keys per user
        DÃ¹ng Redis Set Ä‘á»ƒ track
        """
        index_key = f"cache_index:{user_id}"
        self.redis.sadd(index_key, cache_key)
        self.redis.expire(index_key, 86400)  # 24h TTL cho index
```

**Config:**

- **Memory limit:** 2GB (Redis maxmemory)
- **Eviction policy:** `allkeys-lru`
- **Hit rate target:** 40-60%

---

#### **Layer 3: Vector Search (No Cache)**

```python
async def _do_search(
    self, 
    user_id: str, 
    query: str,
    limit: int,
    score_threshold: float
) -> List[Dict]:
    """
    Thá»±c hiá»‡n search tháº­t tá»« Milvus
    """
    # 1. Get query embedding (from L0 Embedding Cache)
    query_vector = await embedding_cache.get_embedding(query)
    
    # 2. Vector search in Milvus
    results = await milvus_service.search(
        collection="memories",
        query_vector=query_vector,
        filter={"user_id": user_id},
        limit=limit,
        score_threshold=score_threshold
    )
    
    # 3. Enrich vá»›i graph data (optional)
    enriched = await graph_service.augment(results)
    
    return enriched
```

---

### **ğŸ”¥ GRANULAR CACHE INVALIDATION (Best Practice)**

```python
class CacheInvalidationService:
    """
    Smart invalidation: Chá»‰ invalidate queries bá»‹ áº£nh hÆ°á»Ÿng
    """
    
    async def on_facts_extracted(self, user_id: str, new_facts: List[Dict]):
        """
        ÄÆ°á»£c gá»i khi extraction job hoÃ n thÃ nh
        """
        # 1. Identify affected cache keys
        affected_keys = await self._find_affected_keys(user_id, new_facts)
        
        # 2. Delete only affected keys
        if affected_keys:
            self.redis.delete(*affected_keys)
        
        # 3. Log invalidation
        logger.info(f"Invalidated {len(affected_keys)} cache keys for user {user_id}")
    
    async def _find_affected_keys(self, user_id: str, new_facts: List[Dict]) -> List[str]:
        """
        Logic Ä‘á»ƒ identify keys cáº§n invalidate
        
        Strategy:
        - Náº¿u new_facts chá»©a entity "Sparky" (dog name)
          â†’ Invalidate cÃ¡c queries cÃ³ chá»©a "dog", "pet", "Sparky"
        """
        # Get all cache keys cá»§a user nÃ y
        index_key = f"cache_index:{user_id}"
        all_keys = self.redis.smembers(index_key)
        
        affected_keys = []
        
        # Extract keywords tá»« new facts
        keywords = self._extract_keywords(new_facts)
        
        for key in all_keys:
            # Parse query tá»« cache key
            query = self._extract_query_from_key(key)
            
            # Check overlap vá»›i keywords
            if self._has_overlap(query, keywords):
                affected_keys.append(key)
        
        return affected_keys
    
    def _extract_keywords(self, facts: List[Dict]) -> Set[str]:
        """
        Extract keywords tá»« facts Ä‘á»ƒ match vá»›i queries
        """
        keywords = set()
        for fact in facts:
            # Simple tokenization
            tokens = fact.get('fact_value', '').lower().split()
            keywords.update(tokens)
            
            # Add category/type
            if 'fact_type' in fact:
                keywords.update(fact['fact_type'])
        
        return keywords
    
    def _has_overlap(self, query: str, keywords: Set[str]) -> bool:
        """
        Check náº¿u query cÃ³ overlap vá»›i keywords
        """
        query_tokens = set(query.lower().split())
        return bool(query_tokens & keywords)
```

**Lá»£i Ã­ch:**

- âœ… **Preserve unaffected cache** - khÃ´ng nuke toÃ n bá»™
- âœ… **Avoid thundering herd** - invalidate gradually
- âœ… **Better hit rate** - 60-70% vs 20-30% vá»›i version tag

---

### **ğŸ“Š MONITORING & METRICS**

```python
class CacheMetrics:
    """
    Track cache performance
    """
    
    def __init__(self):
        self.prometheus_registry = CollectorRegistry()
        
        # Cache hit rate metrics
        self.hit_counter = Counter(
            'cache_hits_total',
            'Total cache hits',
            ['layer']  # L1, L2
        )
        
        self.miss_counter = Counter(
            'cache_misses_total',
            'Total cache misses',
            ['layer']
        )
        
        # Latency histograms
        self.latency_histogram = Histogram(
            'search_latency_seconds',
            'Search latency',
            ['cache_status'],  # hit_l1, hit_l2, miss
            buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5]
        )
    
    def record_hit(self, layer: str):
        self.hit_counter.labels(layer=layer).inc()
    
    def record_miss(self, layer: str):
        self.miss_counter.labels(layer=layer).inc()
    
    def record_latency(self, cache_status: str, latency_seconds: float):
        self.latency_histogram.labels(cache_status=cache_status).observe(latency_seconds)
```

**Grafana Dashboard:**

```yaml
panels:
  - title: "Cache Hit Rate"
    targets:
      - expr: |
          sum(rate(cache_hits_total[5m])) / 
          (sum(rate(cache_hits_total[5m])) + sum(rate(cache_misses_total[5m])))
    threshold: 0.4  # Alert if < 40%
  
  - title: "P95 Latency by Cache Status"
    targets:
      - expr: histogram_quantile(0.95, search_latency_seconds)
```

---

### **ğŸ’° COST-BENEFIT ANALYSIS**

|Metric|Without Cache|With Optimized 3-Layer|Improvement|
|---|---|---|---|
|**P95 Latency**|250ms|**<50ms**|**80% faster**|
|**OpenAI API Cost**|$500/month|**$100/month**|**80% savings**|
|**Infrastructure**|-|+$50/month (Redis)|ROI: 9x|
|**Cache Hit Rate**|0%|**60-70%**|-|
|**P99 Latency**|500ms|**<150ms**|**70% faster**|

---

### **ğŸ¯ IMPLEMENTATION ROADMAP**

**Phase 1: Foundation (Week 1-2)**

- âœ… Implement L0 Embedding Cache
- âœ… Implement L2 Redis Cache vá»›i basic key structure
- âœ… Add cache metrics & monitoring

**Phase 2: Optimization (Week 3-4)**

- âœ… Implement L1 LRU Cache
- âœ… Implement Granular Invalidation
- âœ… A/B test cache TTL values

**Phase 3: Advanced (Week 5-6)**

- âœ… Implement semantic similarity cho cache key matching
- âœ… Auto-tune TTL dá»±a trÃªn user behavior
- âœ… Pre-warming cho VIP users

---

## ğŸ¬ Káº¾T LUáº¬N

**Loáº¡i bá»:**

- âŒ Proactive Caching Worker (khÃ´ng cáº§n thiáº¿t)
- âŒ L2 PostgreSQL Materialized View (dÆ° thá»«a)
- âŒ Version Tag Invalidation (inefficient)

**Giá»¯ láº¡i & Tá»‘i Æ°u:**

- âœ… **3-Layer Reactive Cache** (L0 Embedding + L1 LRU + L2 Redis)
- âœ… **Granular Invalidation** (chá»‰ xÃ³a keys bá»‹ áº£nh hÆ°á»Ÿng)
- âœ… **Dynamic TTL** (based on result quality)
- âœ… **Comprehensive Monitoring**

**Káº¿t quáº£ ká»³ vá»ng:**

- ğŸš€ P95 latency: **<50ms** (giáº£m 80%)
- ğŸ’° Chi phÃ­: **-$400/month** tiáº¿t kiá»‡m
- ğŸ“ˆ Cache hit rate: **60-70%**
- ğŸ¯ Äáº¡t production-grade standards

# V3 - Perflexity: trong mem0 OSS há»— trá»£ nhá»¯ng cÃ¡ch nÃ o cho shorrt term vÃ  long term

Dá»±a trÃªn tÃ i liá»‡u vÃ  research, Ä‘Ã¢y lÃ  cÃ¡ch **Mem0 OSS há»— trá»£ Short-Term vÃ  Long-Term Memory**:

***

## ğŸ§  **MEM0 OSS: SHORT-TERM vs LONG-TERM MEMORY**

### **ğŸ“Š Overview Table**

| Memory Type | Mem0 OSS Support | Storage | Lifetime | Use Case |
| :-- | :-- | :-- | :-- | :-- |
| **Short-Term Memory (STM)** | âœ… Conversation/Session-scoped | In-memory context | Minutes to hours | Current conversation flow |
| **Long-Term Memory (LTM)** | âœ… User/Agent-scoped | Vector DB + Graph DB | Persistent (days/months/years) | Cross-session personalization |


***

## ğŸ” **1. SHORT-TERM MEMORY (STM) - Mem0 OSS**

### **CÃ¡ch Mem0 Há»— Trá»£:**

Mem0 **KHÃ”NG tá»± Ä‘á»™ng quáº£n lÃ½ STM** trong context window. Báº¡n cáº§n tá»± implement:

```python
from mem0 import Memory

memory = Memory()

# Short-term memory = conversation history trong session hiá»‡n táº¡i
conversation_history = [
    {"role": "user", "content": "TÃ´i thÃ­ch pizza"},
    {"role": "assistant", "content": "Tuyá»‡t! Báº¡n thÃ­ch loáº¡i pizza nÃ o?"},
    {"role": "user", "content": "Pizza háº£i sáº£n"}
]

# Mem0 KHÃ”NG lÆ°u conversation history nÃ y
# Báº¡n pháº£i tá»± quáº£n lÃ½ trong application state hoáº·c Redis
```

**Implementation Pattern:**

```python
class ShortTermMemory:
    """
    Application-level short-term memory
    (Mem0 khÃ´ng handle pháº§n nÃ y)
    """
    def __init__(self):
        self.redis = redis.Redis()
    
    def store_conversation(self, session_id: str, messages: List[Dict]):
        """LÆ°u conversation vÃ o Redis vá»›i TTL ngáº¯n"""
        key = f"stm:session:{session_id}"
        self.redis.setex(
            key,
            3600,  # 1 hour TTL
            json.dumps(messages)
        )
    
    def get_conversation(self, session_id: str) -> List[Dict]:
        """Láº¥y conversation tá»« Redis"""
        key = f"stm:session:{session_id}"
        data = self.redis.get(key)
        return json.loads(data) if data else []
```

**Káº¿t luáº­n:** Mem0 OSS **KHÃ”NG quáº£n lÃ½ STM**. Báº¡n cáº§n tá»± implement báº±ng Redis hoáº·c in-memory cache.[^1][^2][^3]

***

## ğŸ›ï¸ **2. LONG-TERM MEMORY (LTM) - Mem0 OSS (CORE FEATURE)**

### **CÃ¡ch Mem0 Há»— Trá»£:**

Mem0 OSS **chuyÃªn vá» LTM** - Ä‘Ã¢y lÃ  core feature chÃ­nh.[^4][^2][^3][^5][^1]

```python
from mem0 import Memory

# Initialize Mem0 OSS
config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "host": "localhost",
            "port": 6333,
            "collection_name": "memories"
        }
    },
    "llm": {
        "provider": "openai",
        "config": {
            "model": "gpt-4o-mini",
            "temperature": 0.0
        }
    },
    "embedder": {
        "provider": "openai",
        "config": {
            "model": "text-embedding-3-small"
        }
    }
}

memory = Memory.from_config(config)

# ADD: LÆ°u long-term memory
memory.add(
    messages=[
        {"role": "user", "content": "TÃ´i thÃ­ch Äƒn pizza"},
        {"role": "assistant", "content": "ÄÃ£ lÆ°u!"}
    ],
    user_id="user_123"
)
# â†’ Mem0 tá»± Ä‘á»™ng extract "User likes pizza" vÃ  lÆ°u vÃ o Vector DB

# SEARCH: TÃ¬m long-term memory (cross-session)
results = memory.search(
    query="User thÃ­ch Äƒn gÃ¬?",
    user_id="user_123"
)
# â†’ Return: ["User likes pizza"] (ngay cáº£ sau nhiá»u ngÃ y)
```


***

## ğŸ“‹ **3. MEM0 OSS MEMORY TYPES (Chi Tiáº¿t)**

Mem0 OSS phÃ¢n loáº¡i memory theo **4 loáº¡i** (táº¥t cáº£ Ä‘á»u lÃ  LTM):[^2][^3][^1]

### **3.1. Conversation Memory (Shortest LTM)**

```python
# LÆ°u memory theo conversation_id
memory.add(
    "I'm working on project Alpha",
    user_id="user_123",
    metadata={"conversation_id": "conv_456"}
)

# Chá»‰ Ã¡p dá»¥ng cho conversation nÃ y
# Lifetime: Minutes to hours (configurable)
```


### **3.2. Session Memory (Short LTM)**

```python
# LÆ°u memory theo session
memory.add(
    "Currently browsing electronics section",
    user_id="user_123",
    metadata={"session_id": "sess_789"}
)

# Lifetime: Hours (e.g., 1 day session)
```


### **3.3. User Memory (Long LTM)**

```python
# LÆ°u memory vÄ©nh viá»…n cho user
memory.add(
    "User prefers dark mode",
    user_id="user_123"
)

# Lifetime: Weeks to forever (no expiration)
```


### **3.4. Organization Memory (Shared LTM)**

```python
# LÆ°u memory chung cho org
memory.add(
    "Company policy: remote work on Fridays",
    metadata={"org_id": "org_001"}
)

# Lifetime: Configured globally
```


***

## â° **4. EXPIRATION / TTL (Mem0 OSS v1.0+)**

Mem0 OSS **há»— trá»£ expiration** Ä‘á»ƒ implement short-term vs long-term:[^6]

```python
from datetime import datetime, timedelta

# SHORT-TERM: Expires in 7 days
expires_at = (datetime.now() + timedelta(days=7)).isoformat()
memory.add(
    "Currently browsing electronics",
    user_id="sarah",
    expiration_date=expires_at  # â† Key feature!
)

# LONG-TERM: No expiration (persists forever)
memory.add(
    "User prefers dark mode",
    user_id="sarah"
    # No expiration_date = permanent
)
```

**7 ngÃ y sau:**

```python
# Short-term memory Ä‘Ã£ expired
results = memory.search("browsing", user_id="sarah")
# â†’ Empty (Ä‘Ã£ háº¿t háº¡n)

# Long-term memory váº«n cÃ²n
results = memory.search("dark mode", user_id="sarah")
# â†’ Return: "User prefers dark mode" âœ…
```


***

## ğŸ”„ **5. WORKFLOW: PHá»I Há»¢P STM + LTM Vá»šI MEM0 OSS**

### **Recommended Pattern:**

```python
class HybridMemorySystem:
    """
    Káº¿t há»£p STM (tá»± quáº£n lÃ½) + LTM (Mem0 OSS)
    """
    
    def __init__(self):
        # STM: Redis (application-managed)
        self.stm = ShortTermMemory()
        
        # LTM: Mem0 OSS (managed by Mem0)
        self.ltm = Memory.from_config(config)
    
    async def process_message(
        self,
        user_id: str,
        session_id: str,
        message: str
    ):
        """Process user message with hybrid memory"""
        
        # 1. Get STM (recent conversation)
        conversation = self.stm.get_conversation(session_id)
        
        # 2. Get LTM (relevant long-term facts)
        ltm_facts = self.ltm.search(
            query=message,
            user_id=user_id,
            limit=5
        )
        
        # 3. Combine STM + LTM for LLM prompt
        context = {
            "conversation": conversation,  # STM
            "user_facts": ltm_facts       # LTM from Mem0
        }
        
        # 4. Generate response
        response = await llm.generate(message, context)
        
        # 5. Update STM
        conversation.append({"role": "user", "content": message})
        conversation.append({"role": "assistant", "content": response})
        self.stm.store_conversation(session_id, conversation)
        
        # 6. Extract important facts â†’ LTM (async)
        asyncio.create_task(
            self.ltm.add(
                messages=conversation[-2:],  # Last 2 messages
                user_id=user_id
            )
        )
        
        return response
```


***

## ğŸ“Š **6. SO SÃNH: STM vs LTM TRONG MEM0 OSS**

| Aspect | Short-Term Memory | Long-Term Memory |
| :-- | :-- | :-- |
| **Mem0 Managed?** | âŒ NO (báº¡n tá»± implement) | âœ… YES (core feature) |
| **Storage** | Redis / In-memory | Vector DB (Qdrant/Milvus) + Graph DB |
| **Lifetime** | Minutes to hours | Days to forever |
| **Scope** | Per session/conversation | Per user/agent |
| **Expiration** | Auto (TTL trong Redis) | Optional (expiration_date) |
| **Search Method** | Exact match / Key-based | Semantic vector search |
| **Use Case** | "Vá»«a nÃ³i gÃ¬?" | "Sá»Ÿ thÃ­ch tá»« trÆ°á»›c tá»›i nay?" |


***

## âœ… **Káº¾T LUáº¬N: MEM0 OSS Há»– TRá»¢ GÃŒ?**

### **âœ… Mem0 OSS Há»– TRá»¢:**

1. **Long-Term Memory (Core):**
    - Vector storage (Qdrant/Milvus)
    - Semantic search
    - Graph relationships (Neo4j)
    - Expiration support (TTL)
    - User/Agent/Org scoping
2. **Memory Types:**
    - Conversation memory (short LTM)
    - Session memory (medium LTM)
    - User memory (long LTM)
    - Organization memory (shared LTM)

### **âŒ Mem0 OSS KHÃ”NG Há»– TRá»¢:**

1. **Short-Term Memory (Conversation History):**
    - Báº¡n pháº£i tá»± quáº£n lÃ½ vá»›i Redis/In-memory
    - Mem0 khÃ´ng store conversation history
2. **Context Window Management:**
    - Mem0 khÃ´ng replace LLM context window
    - Báº¡n váº«n cáº§n quáº£n lÃ½ prompt engineering

***

## ğŸ¯ **RECOMMENDATION CHO PIKA:**

```python
# Chiáº¿n lÆ°á»£c Ä‘á» xuáº¥t:

# STM (Self-managed):
- Redis vá»›i TTL 1 hour
- Key: stm:session:{session_id}
- Store: Conversation history (messages list)

# LTM (Mem0 OSS):
- Qdrant vector store
- Neo4j graph store (relationships)
- Expiration: 
  * Session facts: 7 days
  * User preferences: No expiration
```

**Káº¿t luáº­n:** Mem0 OSS **chuyÃªn vá» LTM**, báº¡n cáº§n tá»± implement STM báº±ng Redis hoáº·c application state.[^3][^5][^1][^2][^6]
<span style="display:none">[^10][^11][^12][^13][^14][^15][^16][^17][^18][^19][^20][^21][^22][^23][^24][^25][^7][^8][^9]</span>

<div align="center">â‚</div>

[^1]: https://docs.mem0.ai/core-concepts/memory-types

[^2]: https://aws.amazon.com/blogs/database/build-persistent-memory-for-agentic-ai-applications-with-mem0-open-source-amazon-elasticache-for-valkey-and-amazon-neptune-analytics/

[^3]: https://dev.to/yigit-konur/mem0-the-comprehensive-guide-to-building-ai-with-persistent-memory-fbm

[^4]: https://mem0.ai

[^5]: https://github.com/mem0ai/mem0

[^6]: https://docs.mem0.ai/cookbooks/essentials/memory-expiration-short-and-long-term

[^7]: Step-2-Output-1-SDD.md

[^8]: MEM0_OSS_SETUP.md

[^9]: Step-2-Output-2-SDD-HLD-LLD-co-ca-Optimize-Response-Time.md

[^10]: report.md

[^11]: report.md

[^12]: https://viblo.asia/p/mem0-kien-truc-long-term-memory-cho-he-thong-ai-agent-G24B88pOLz3

[^13]: https://pypi.org/project/mem0ai/0.1.0/

[^14]: https://www.ycombinator.com/launches/LpA-mem0-open-source-memory-layer-for-ai-apps

[^15]: https://www.datacamp.com/tutorial/mem0-tutorial

[^16]: https://docs.mem0.ai/open-source/overview

[^17]: https://docs.crewai.com/en/concepts/memory

[^18]: https://skywork.ai/skypage/en/A-Comprehensive-Guide-to-the-Mem0-MCP-Server-Building-AI-with-Persistent-Memory/1971044006807793664

[^19]: https://arxiv.org/abs/2504.19413

[^20]: https://docs.mem0.ai/open-source/python-quickstart

[^21]: https://viblo.asia/p/gioi-thieu-ve-mem0-xay-dung-ai-agents-voi-long-term-memory-AY4qQN8q4Pw

[^22]: https://arxiv.org/pdf/2504.19413.pdf

[^23]: https://blog.stackademic.com/mem0-memo-ai-memory-layer-purpose-and-core-functionality-375cc5a2bfd0

[^24]: https://fosterfletcher.com/ai-memory-infrastructure/

[^25]: https://docs.mem0.ai/v0x/core-concepts/memory-types


---

<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# V4 - ğŸ“Š **QUICK SUMMARY: STM + LTM CACHING**

### **SHORT-TERM MEMORY (STM) - 2 Layers**

```
L0: In-Memory (current request)
    â””â”€ <1ms, 95% hit rate

L1: Redis (1 hour TTL)
    â””â”€ 5ms, 70% hit rate
```


### **LONG-TERM MEMORY (LTM) - 4 Layers (CRITICAL!)**

```
L0: IN-MEMORY (Python @lru_cache)
    â”œâ”€ Latency: <1ms
    â”œâ”€ Hit Rate: 10-20%
    â”œâ”€ Scope: Per request
    â””â”€ Use: Repeated queries in same request

L1: EMBEDDING CACHE (Redis)
    â”œâ”€ Latency: 5ms
    â”œâ”€ Hit Rate: 60-70%
    â”œâ”€ TTL: 24 hours
    â””â”€ Use: Avoid OpenAI API calls (100-200ms saved!)

L2: RESULT CACHE (Redis)
    â”œâ”€ Latency: 5-20ms
    â”œâ”€ Hit Rate: 40%
    â”œâ”€ TTL: Smart (based on confidence)
    â””â”€ Use: Cache full search results

L3: MATERIALIZED VIEW (PostgreSQL)
    â”œâ”€ Latency: 20-50ms
    â”œâ”€ Hit Rate: 20-30% (for favorites)
    â”œâ”€ TTL: Proactive worker updates
    â””â”€ Use: Pre-computed user favorites

L4: VECTOR SEARCH (Mem0 + Milvus)
    â”œâ”€ Latency: 100-300ms
    â”œâ”€ Hit Rate: N/A (always fresh)
    â”œâ”€ TTL: No cache (primary source)
    â””â”€ Use: Fallback when all caches miss

```


***


---

# Sau khi káº¿t thÃºc 1 cuá»™c há»™i thoáº¡i -> Ä‘Æ°á»£c báº¯n Ä‘i xá»­ lÃ½ extract cÃ¡c kiá»ƒu -> save memory  
+, L4 thá»±c hiá»‡n ngay query user_favorite_summary => Ä‘áº©y xuá»‘ng L3  
+, L3 thá»±c hiá»‡n ngay Ä‘á»ƒ lÆ°u vÃ o DB Postgres  
=> L2 thá»±c hiá»‡n ngay Ä‘á»ƒ cache vÃ o trong Redis  
  
----  
trong lÃºc quÃ¡ trÃ¬nh nÃ y chÆ°a thá»±c hiá»‡n xong thÃ¬ náº¿u user há»i sáº½ dÃ¹ng short term memory


| Aspect          | **Milvus**                      | **Neo4j**                  |
| --------------- | ------------------------------- | -------------------------- |
| **Type**        | Vector Database                 | Graph Database             |
| **Primary Use** | Semantic similarity search      | Relationship traversal     |
| **Data Stored** | Embeddings (vectors 1536-dim)   | Entities + Relationships   |
| **Query Type**  | "Find similar memories"         | "Who owns what?"           |
| **Latency**     | 50-150ms                        | 50-100ms                   |
| **Best For**    | Fuzzy matching, semantic search | Structured knowledge graph |


Khi end cuá»™c há»™i thoáº¡i, bÃªn BE chá»§ Ä‘á»™ng báº¯n end cho bÃªn phÃ­a AI thá»±c hiá»‡n extract (á»Ÿ Module Context Handling rá»“i).  
  
+, Trong Module Memory nÃ y chá»‰ cáº§n:  
1. Thá»±c hiá»‡n extract xong thÃ¬ lÆ°u vÃ o Long Term Memory  
2. Thá»±c hiá»‡n query L4 (Vector Search: Milvus, vÃ  Graph Search: Neo4J Ä‘Æ°á»£c tÃ­ch há»£p sáºµn trong Mem0 OSS


```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          CONTEXT HANDLING MODULE                     â”‚
â”‚  (Already handles conversation end & extraction)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Triggers extraction job
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          MEMORY MODULE (Your focus)                  â”‚
â”‚                                                      â”‚
â”‚  âœ… 1. Receive extraction results                   â”‚
â”‚  âœ… 2. Save to Long-Term Memory (L4)                â”‚
â”‚        â”œâ”€ Vector Search (Milvus)                    â”‚
â”‚        â””â”€ Graph Search (Neo4j)                      â”‚
â”‚        â†’ Both handled by Mem0 OSS                   â”‚
â”‚                                                      â”‚
â”‚  âœ… 3. Proactive Cache Warming (After save)         â”‚
â”‚        â”œâ”€ Query L4 (user_favorite_summary)          â”‚
â”‚        â”œâ”€ Save to L3 (PostgreSQL)                   â”‚
â”‚        â””â”€ Warm L2 (Redis)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

## Flow sau khi end conversation

- BE bÃ¡o end â†’ Context Handling cháº¡y extract, rá»“i gá»­iÂ `extracted_facts`Â sang Memory Module.[](https://www.perplexity.ai/search/cai-tai-lieu-nao-ma-co-full-co-DnFYpZp7Tzaf_teH.xHLkw)â€‹
    
- Memory Module lÃ m 3 bÆ°á»›c ná»‘i tiáº¿p (background, async):
    
    1. **Save LTM (L4)**: dÃ¹ng Mem0 OSS Ä‘á»ƒ lÆ°u facts vÃ o Milvus + Neo4j.
        
    2. **Query L4 choÂ `user_favorite_summary`**: gá»i má»™t query canonical Ä‘á»ƒ gom Ä‘á»§ â€œfavorite (movie, character, pet, activity, friend, music, travel, toy)â€.
        
    3. **Äáº©y xuá»‘ng cÃ¡c táº§ng cache**:
        
        - L3: ghi summary vÃ o Postgres (`user_favorite_summary`).
            
        - L2: cache ngay 1â€“2 response canonical trong Redis cho cÃ¡c cÃ¢u favorite phá»• biáº¿n.[](https://www.perplexity.ai/search/cai-tai-lieu-nao-ma-co-full-co-DnFYpZp7Tzaf_teH.xHLkw)â€‹

Trong lÃºc pipeline nÃ y chÆ°a xong  
Náº¿u user há»i láº¡i ngay sau khi end:  
Front/Context Handling dÃ¹ngÂ shortâ€‘term memoryÂ + L1/L2 nhÆ° bÃ¬nh thÆ°á»ng Ä‘á»ƒ tráº£ lá»i, khÃ´ng Ä‘á»£i job ná»n.  
â€‹  
Khi job ná»n hoÃ n táº¥t:  
L4 sáº½ cÃ³  
L3 Ä‘Ã£ cÃ³ profile dÃ i háº¡n.  
L2 Ä‘Ã£ Ä‘Æ°á»£c warm sáºµn cho cÃ¡c favorite query â†’ láº§n sau user há»i sáº½ hit cache nhanh.â€‹


Ã nÃ y há»£p lÃ½, vÃ  cÃ³ thá»ƒ rÃºt láº¡i thÃ nh rule Ä‘Æ¡n giáº£n cho pipeline trongâ€‘ngÃ y:

## 1. Online path trong ngÃ y

- Vá»›i cÃ¡c cÃ¢u há»i â€œbÃ¬nh thÆ°á»ngâ€ trong 1 ngÃ y Ä‘Ã³:
    
    - Æ¯u tiÃªn dÃ¹ng **Shortâ€‘Term Memory** (STM) lÃ m nguá»“n chÃ­nh vÃ¬:
        
        - ÄÃ£ chá»©a full lá»‹ch sá»­ cÃ¡c cuá»™c há»™i thoáº¡i gáº§n Ä‘Ã¢y.
            
        - ÄÆ°á»£c lÆ°u Redis 1 ngÃ y nÃªn coi nhÆ° â€œshortâ€‘term nhÆ°ng Ä‘á»§ dÃ iâ€.[perplexity](https://www.perplexity.ai/search/cai-tai-lieu-nao-ma-co-full-co-DnFYpZp7Tzaf_teH.xHLkw)â€‹
            
- Khi cáº§n LTM trong pipeline:
    - Check L0 **L0 â€“ Inâ€‘memory (per request / per process)**
		- Python dict / nhá» gá»n, latency <1ms.
		    
		- DÃ¹ng Ä‘á»ƒ trÃ¡nh láº·p láº¡i cÃ¹ng má»™t phÃ©p tÃ­nh trong cÃ¹ng request hoáº·c ráº¥t ngáº¯n háº¡n. 
    - **Check L1 (embedding cache)** Ä‘á»ƒ trÃ¡nh gá»i embedding.
        
    - **Check L2 (result cache)** cho cÃ¡c query LTM Ä‘Ã£ warm.
        
    - Náº¿u cáº£ L1/L2 Ä‘á»u á»•n thÃ¬ **thÆ°á»ng khÃ´ng cáº§n cháº¡m L3/L4** trong Ä‘a sá»‘ case trong ngÃ y.[perplexity](https://www.perplexity.ai/search/cai-tai-lieu-nao-ma-co-full-co-DnFYpZp7Tzaf_teH.xHLkw)â€‹
        

## 2. L3/L4 nhÆ° fallback & offline source

- **L3 (Postgres summary)**: chá»‰ dÃ¹ng khi:
    
    - L2 gáº·p lá»—i / miss mÃ  cáº§n profile á»•n Ä‘á»‹nh, hoáº·c trong cÃ¡c job offline build/cáº­p nháº­t profile.[perplexity](https://www.perplexity.ai/search/cai-tai-lieu-nao-ma-co-full-co-DnFYpZp7Tzaf_teH.xHLkw)â€‹
        
- **L4 (Mem0 â€“ Milvus + Neo4j)**: giá»¯ vai trÃ²:
    
    - Source of truth Ä‘á»ƒ:
        
        - Build / rebuild L3.
            
        - Serve cÃ¡c query LTM â€œká»³ láº¡â€, Ã­t gáº·p, hoáº·c khi STM + L2/L3 khÃ´ng Ä‘á»§ thÃ´ng tin.[perplexity](https://www.perplexity.ai/search/cai-tai-lieu-nao-ma-co-full-co-DnFYpZp7Tzaf_teH.xHLkw)â€‹
            

## 3. Tá»‘i Æ°u thá»±c táº¿

- VÃ¬ STM Ä‘Ã£ giá»¯ Ä‘Æ°á»£c 1 ngÃ y, nÃªn vá»›i user hoáº¡t Ä‘á»™ng nhiá»u trong ngÃ y:
    
    - 90%+ cÃ¢u tráº£ lá»i sáº½ Ä‘áº¿n tá»« **STM + L1/L2**.
        
    - L3/L4 gáº§n nhÆ° chá»‰ cháº¡y:
        
        - Khi káº¿t thÃºc há»™i thoáº¡i (pipeline ná»n).
            
        - Hoáº·c cho cÃ¡c truy váº¥n â€œráº¥t lÃ¢u rá»“i khÃ´ng Ä‘á»¥ng tá»›iâ€.[perplexity](https://www.perplexity.ai/search/cai-tai-lieu-nao-ma-co-full-co-DnFYpZp7Tzaf_teH.xHLkw)â€‹
            

TÃ³m láº¡i: pipeline online dÃ¹ng **STM + L1/L2** lÃ m â€œfirst lineâ€, cÃ²n **L3/L4 lÃ  longâ€‘term backup + offline computation layer**, Ä‘Ãºng vá»›i cÃ¡ch báº¡n Ä‘ang nghÄ©.[perplexity](https://www.perplexity.ai/search/cai-tai-lieu-nao-ma-co-full-co-DnFYpZp7Tzaf_teH.xHLkw)â€‹

1. [https://www.perplexity.ai/search/cai-tai-lieu-nao-ma-co-full-co-DnFYpZp7Tzaf_teH.xHLkw](https://www.perplexity.ai/search/cai-tai-lieu-nao-ma-co-full-co-DnFYpZp7Tzaf_teH.xHLkw)


