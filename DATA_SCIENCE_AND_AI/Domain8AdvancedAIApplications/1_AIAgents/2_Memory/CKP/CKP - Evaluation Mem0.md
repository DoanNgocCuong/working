
```
VI·∫æT CHO M√åNH M·ª§C L·ª§C C·ª¶A ALL IN ONE EVAL MEMORY 

# 1. V·ªÅ c√°ch ƒë√°nh gi√° MECE c√°c ch·ªâ s·ªë v√† ph∆∞∆°ng ph√°p 
l·∫•y d·∫´n ch·ª©ng ƒë·∫ßy ƒë·ªß c√°c framework v√† best practices

MECE b·ªô test v√† best practices 
....

# 2. c√°ch tri·ªÉn khai ƒë√°nh gi√° ?? MECE ? v√† best practices 
```


<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# so s√°nh v·ªõi b·ªô long mem eval

**LongMemEval** ƒë∆∞·ª£c coi l√† m·ªôt "b·∫£n n√¢ng c·∫•p" kh√≥ h∆°n v√† to√†n di·ªán h∆°n so v·ªõi **LOCOMO**. N·∫øu b·∫°n mu·ªën Mem0 OSS c·ªßa m√¨nh th·ª±c s·ª± m·∫°nh m·∫Ω (ho·∫∑c mu·ªën so s√°nh c√¥ng b·∫±ng v·ªõi c√°c h·ªá th·ªëng m·ªõi nh∆∞ Zep, MemGPT), b·∫°n n√™n c√¢n nh·∫Øc c·∫£ hai.

D∆∞·ªõi ƒë√¢y l√† b·∫£ng so s√°nh chi ti·∫øt v√† c√°ch ƒë√°nh gi√° tr√™n LongMemEval.

### 1. B·∫£ng So S√°nh Tr·ª±c Ti·∫øp: LOCOMO vs. LongMemEval

| Ti√™u ch√≠                 | LOCOMO (Mem0 d√πng)                                                                           | LongMemEval (Zep/M·ªõi d√πng)                                                                                                                               |
| :----------------------- | :------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ƒê·ªô d√†i l·ªãch s·ª≠**       | ~26k tokens (10 h·ªôi tho·∫°i d√†i).                                                              | **~115k tokens** (LongMemEval_S) ƒë·∫øn **1.5M tokens** (LongMemEval_M).                                                                                    |
| **ƒê·ªô ph·ª©c t·∫°p**          | T·∫≠p trung v√†o *conversational recall*. Kh√° "d·ªÖ th·ªü" v·ªõi c√°c LLM c·ª≠a s·ªï context l·ªõn hi·ªán nay. | R·∫•t kh√≥. Thi·∫øt k·∫ø ƒë·ªÉ **ƒë√°nh l·ª´a** h·ªá th·ªëng memory b·∫±ng th√¥ng tin g√¢y nhi·ªÖu v√† kho·∫£ng c√°ch th·ªùi gian l·ªõn.                                                 |
| **Lo·∫°i c√¢u h·ªèi (Tasks)** | 4 lo·∫°i: Single-hop, Multi-hop, Temporal, Open-domain.                                        | **5 lo·∫°i ch√≠nh:** Info Extraction, Multi-session Reasoning, Temporal Reasoning, **Knowledge Updates** (quan tr·ªçng!), Abstention (bi·∫øt n√≥i "kh√¥ng bi·∫øt"). |
| **ƒêi·ªÉm y·∫øu**             | Zep ch·ªâ tr√≠ch LOCOMO l√† "qu√° ng·∫Øn" v√† kh√¥ng ƒë·ªß ƒë·ªÉ test gi·ªõi h·∫°n c·ªßa RAG/Vector DB.           | C·∫•u tr√∫c d·ªØ li·ªáu JSON r·∫•t ph·ª©c t·∫°p (~300MB), kh√≥ setup h∆°n.                                                                                              |
| **Ai ƒëang d√πng?**        | Mem0, Cognee.                                                                                | Zep, MemGPT (Letta), LiCoMemory.                                                                                                                         |

**T√≥m l·∫°i:**

* **LOCOMO** ph√π h·ª£p ƒë·ªÉ test **t·ªëc ƒë·ªô** v√† **ƒë·ªô ch√≠nh x√°c c∆° b·∫£n**.
* **LongMemEval** ph√π h·ª£p ƒë·ªÉ test **s·ª©c ch·ªãu ƒë·ª±ng (stress test)** v√† kh·∫£ nƒÉng **x·ª≠ l√Ω m√¢u thu·∫´n/c·∫≠p nh·∫≠t** (v√≠ d·ª•: h√¥m qua th√≠ch A, nay th√≠ch B -> Memory c√≥ update kh√¥ng?).

***

### 2. C·∫•u tr√∫c Dataset c·ªßa LongMemEval

Kh√°c v·ªõi h·ªôi tho·∫°i ph·∫≥ng, LongMemEval c√≥ c·∫•u tr√∫c l·ªìng nhau (Nested JSON):

```json
{
  "history": [
    {
      "session_id": "s1",
      "timestamp": "2023-01-01",
      "conversation": [ ... ] // 1 session h·ªôi tho·∫°i
    },
    {
      "session_id": "s2",
      "timestamp": "2023-02-15", // C√°ch nhau 1.5 th√°ng
      "conversation": [ ... ]
    },
    // ... 50-500 sessions kh√°c
  ],
  "question": "M√≥n ƒÉn y√™u th√≠ch c·ªßa user l√† g√¨?",
  "answer": "Sushi (c·∫≠p nh·∫≠t m·ªõi nh·∫•t), tr∆∞·ªõc ƒë√≥ l√† Pizza",
  "evidence": ["s1", "s50"] // Ch·ªâ r√µ session n√†o ch·ª©a th√¥ng tin ƒë√∫ng
}
```


### 3. C√°ch Benchmark Mem0 tr√™n LongMemEval

N·∫øu b·∫°n mu·ªën ch·∫°y benchmark n√†y cho Mem0 OSS, quy tr√¨nh s·∫Ω h∆°i kh√°c LOCOMO m·ªôt ch√∫t do ƒë·ªô l·ªõn c·ªßa d·ªØ li·ªáu:

1. **T·∫£i Dataset:**
    * Repo: `xiaowu0162/LongMemEval`
    * File: `longmemeval_s.json` (b·∫£n S l√† ƒë·ªß, b·∫£n M qu√° n·∫∑ng cho m√°y c√° nh√¢n).
2. **Pipeline Ch·∫°y (Adapter):**
    * **Ingest (N·∫°p d·ªØ li·ªáu):** B·∫°n ph·∫£i loop qua t·ª´ng `session` trong `history` v√† g·ªçi API `add` c·ªßa Mem0.
        * *L∆∞u √Ω:* Ph·∫£i truy·ªÅn `timestamp` v√†o metadata c·ªßa Mem0. ƒê√¢y l√† ch√¨a kh√≥a ƒë·ªÉ Mem0 x·ª≠ l√Ω Temporal Reasoning (c√¢u h·ªèi v·ªÅ th·ªùi gian).
    * **Retrieval \& Answer:**
        * G·ªçi API `search` v·ªõi c√¢u h·ªèi.
        * ƒê∆∞a k·∫øt qu·∫£ search v√†o LLM ƒë·ªÉ tr·∫£ l·ªùi.
    * **Evaluation:**
        * D√πng script ch·∫•m ƒëi·ªÉm c√≥ s·∫µn trong repo LongMemEval (h·ªç c≈©ng d√πng LLM-as-a-Judge ho·∫∑c string matching cho c√¢u tr·∫£ l·ªùi ng·∫Øn).
3. **Metrics c·∫ßn quan t√¢m tr√™n LongMemEval:**
    * **Accuracy:** ƒê·∫∑c bi·ªát l√† ·ªü nh√≥m c√¢u h·ªèi **Knowledge Updates** (Update th√¥ng tin c≈©) - ƒë√¢y l√† n∆°i Mem0 th∆∞·ªùng b·ªã fail n·∫øu kh√¥ng config Graph Memory t·ªët.
    * **Abstention Rate:** T·ª∑ l·ªá AI bi·∫øt tr·∫£ l·ªùi "T√¥i kh√¥ng c√≥ th√¥ng tin n√†y" thay v√¨ b·ªãa (Hallucination).

### 4. L·ªùi khuy√™n cho b·∫°n

V·ªõi dataset 20 d√≤ng v√† Mem0 OSS hi·ªán t·∫°i:

1. **ƒê·ª´ng ch·∫°y LongMemEval ngay:** N√≥ qu√° l·ªõn (115k tokens) v√† ph·ª©c t·∫°p so v·ªõi quy m√¥ 20 d√≤ng c·ªßa b·∫°n. K·∫øt qu·∫£ s·∫Ω kh√¥ng c√≥ √Ω nghƒ©a th·ªëng k√™.
2. **H·ªçc t·∫≠p logic c·ªßa LongMemEval:** H√£y th√™m v√†o b·ªô test 20 d√≤ng c·ªßa b·∫°n c√°c case sau (l·∫•y c·∫£m h·ª©ng t·ª´ LongMemEval):
    * **Update Case:** D√≤ng 1 n√≥i "Th√≠ch ch√≥", D√≤ng 10 n√≥i "M·ªõi b·ªã ch√≥ c·∫Øn, gi·ªù gh√©t ch√≥". -> H·ªèi: "User th√≠ch con g√¨?".
    * **Temporal Case:** D√≤ng 1 (Ng√†y 1/1): "ƒêang ·ªü H√† N·ªôi". D√≤ng 5 (Ng√†y 5/1): "ƒê√£ bay v√†o S√†i G√≤n". -> H·ªèi: "Ng√†y 3/1 user ·ªü ƒë√¢u?".
3. **Khi n√†o scale up:** Khi n√†o b·∫°n c√≥ h·ªá th·ªëng production v·ªõi h√†ng ngh√¨n user, l√∫c ƒë√≥ h√£y k√©o LongMemEval v·ªÅ ƒë·ªÉ stress test h·ªá th·ªëng.

**Actionable:** B·∫°n c√≥ th·ªÉ gi·ªØ nguy√™n b·ªô test 20 d√≤ng nh∆∞ng b·ªï sung th√™m 2-3 d√≤ng k·ªãch b·∫£n "Update" v√† "Temporal" nh∆∞ tr√™n ƒë·ªÉ b·ªô benchmark c·ªßa b·∫°n "x·ªãn" h∆°n (LongMemEval-style).
<span style="display:none">[^1][^10][^11][^12][^13][^14][^15][^16][^17][^18][^19][^2][^20][^3][^4][^5][^6][^7][^8][^9]</span>

<div align="center">‚ÅÇ</div>

[^1]: https://github.com/letta-ai/letta/issues/3115

[^2]: https://arxiv.org/html/2410.10813v1

[^3]: https://www.linkedin.com/posts/richmondalake_100daysofagentmemory-memoryevaluation-activity-7375895418322296832-r77i

[^4]: https://openreview.net/forum?id=pZiyCaVuti

[^5]: https://www.emergentmind.com/topics/locomo-and-longmemeval-_s-benchmarks

[^6]: https://www.emergentmind.com/topics/longmemeval-dataset

[^7]: https://arxiv.org/html/2511.17208v2

[^8]: https://github.com/getzep/zep-papers/issues/5

[^9]: https://www.emergence.ai/post/state-of-the-art-results-in-agentic-memory

[^10]: https://www.emergentmind.com/topics/locomo-benchmark-d06cff1a-d4a5-4df8-ab85-fdca157d190b

[^11]: https://blog.getzep.com/lies-damn-lies-statistics-is-mem0-really-sota-in-agent-memory/

[^12]: https://nicoloboschi.com/posts/20251110/

[^13]: https://mem0.ai/blog/benchmarked-openai-memory-vs-langmem-vs-memgpt-vs-mem0-for-long-term-memory-here-s-how-they-stacked-up

[^14]: https://www.alphaxiv.org/overview/2508.19828v4

[^15]: https://www.alphaxiv.org/overview/2410.10813v2

[^16]: https://liner.com/review/lightmem-lightweight-and-efficient-memoryaugmented-generation

[^17]: https://evermind.ai/blogs/a-unified-evaluation-framework-for-ai-memory-systems

[^18]: https://github.com/xiaowu0162/LongMemEval

[^19]: https://aclanthology.org/2025.acl-long.413.pdf

[^20]: https://huggingface.co/papers?q=LOCOMO+benchmark


---


<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# 1. MECE xem c√°c b·ªçn ƒë√°nh gi√° memory n√≥ d√πng c√°i g√¨ ƒë·ªÉ ƒë√°nh gi√° v√† c√°ch ch·∫°y ?

D∆∞·ªõi ƒë√¢y l√† b·∫£ng ph√¢n t√≠ch **MECE (Mutually Exclusive, Collectively Exhaustive)** v·ªÅ c√°c ph∆∞∆°ng ph√°p v√† c√¥ng c·ª• m√† c√°c h·ªá th·ªëng Memory h√†ng ƒë·∫ßu (Mem0, Zep, MemGPT/Letta, LangMem) ƒëang s·ª≠ d·ª•ng ƒë·ªÉ benchmark.

ƒê·ªÉ ƒë√°nh gi√° to√†n di·ªán m·ªôt h·ªá th·ªëng Memory, c·ªông ƒë·ªìng hi·ªán t·∫°i chia l√†m **4 tr·ª• c·ªôt ƒë√°nh gi√° (Pillars of Evaluation)** ri√™ng bi·ªát.

***

### 1. Tr·ª• c·ªôt: RETRIEVAL QUALITY (Ch·∫•t l∆∞·ª£ng T√¨m ki·∫øm)

*ƒê√°nh gi√° xem h·ªá th·ªëng c√≥ "l√¥i" ƒë∆∞·ª£c ƒë√∫ng k√Ω ·ª©c c·∫ßn thi·∫øt ra kh·ªèi kho l∆∞u tr·ªØ hay kh√¥ng? (Ch∆∞a quan t√¢m c√¢u tr·∫£ l·ªùi c·ªßa AI).*


| Metric (Ch·ªâ s·ªë) | √ù nghƒ©a | C√°ch ch·∫°y / Tool |
| :-- | :-- | :-- |
| **Recall@k** | Trong top-k (vd: top 5) k·∫øt qu·∫£ tr·∫£ v·ªÅ, c√≥ ch·ª©a k√Ω ·ª©c ƒë√∫ng (Gold Memory) kh√¥ng? | **C√°ch ch·∫°y:**<br>1. Chu·∫©n b·ªã dataset: `(Query, [Gold_Memory_IDs])`.<br>2. G·ªçi API `search`.<br>3. So s√°nh ID tr·∫£ v·ªÅ vs Gold ID.<br>**Tool:** Ragas, TruLens, custom scripts. |
| **MRR (Mean Reciprocal Rank)** | K√Ω ·ª©c ƒë√∫ng n·∫±m ·ªü v·ªã tr√≠ n√†o? (H·∫°ng 1 t·ªët h∆°n H·∫°ng 5). | **C√°ch ch·∫°y:** T√≠nh `1/rank` c·ªßa k·∫øt qu·∫£ ƒë√∫ng ƒë·∫ßu ti√™n. |
| **Precision** | T·ª∑ l·ªá k√Ω ·ª©c h·ªØu √≠ch tr√™n t·ªïng s·ªë k√Ω ·ª©c l·∫•y ra (tr√°nh l·∫•y r√°c l√†m nhi·ªÖu context). | **C√°ch ch·∫°y:** ƒê·∫øm s·ªë `Relevant_Memories / Total_Retrieved`. |

> **Ai d√πng?** Zep, Mem0 (ph·∫ßn retrieval benchmark), c√°c h·ªá th·ªëng RAG truy·ªÅn th·ªëng.

***

### 2. Tr·ª• c·ªôt: GENERATION QUALITY (Ch·∫•t l∆∞·ª£ng H·ªìi ƒë√°p)

*ƒê√°nh gi√° xem sau khi l·∫•y ƒë∆∞·ª£c k√Ω ·ª©c, AI c√≥ tr·∫£ l·ªùi ƒë√∫ng c√¢u h·ªèi d·ª±a tr√™n k√Ω ·ª©c ƒë√≥ kh√¥ng?*


| Metric (Ch·ªâ s·ªë) | √ù nghƒ©a | C√°ch ch·∫°y / Tool |
| :-- | :-- | :-- |
| **Correctness (LLM-as-a-Judge)** | C√¢u tr·∫£ l·ªùi c√≥ ƒë√∫ng s·ª± th·∫≠t (Ground Truth) kh√¥ng? | **C√°ch ch·∫°y:**<br>D√πng GPT-4o l√†m gi√°m kh·∫£o v·ªõi prompt: *"Ch·∫•m ƒëi·ªÉm c√¢u tr·∫£ l·ªùi A d·ª±a tr√™n s·ª± th·∫≠t B theo thang 1-5"*. |
| **Faithfulness / Hallucination** | C√¢u tr·∫£ l·ªùi c√≥ b·ªãa ra th√¥ng tin kh√¥ng c√≥ trong memory kh√¥ng? | **Tool:** Ragas (`faithfulness` metric), DeepEval. |
| **Context Adherence** | C√¢u tr·∫£ l·ªùi c√≥ b√°m s√°t ng·ªØ c·∫£nh th·ªùi gian/m√¢u thu·∫´n kh√¥ng? (V√≠ d·ª•: "NƒÉm ngo√°i th√≠ch A, nƒÉm nay th√≠ch B" -> ph·∫£i tr·∫£ l·ªùi B). | **Benchmark:** **LOCOMO** (Mem0 d√πng c√°i n√†y), **LongMemEval**. |

> **Ai d√πng?** **Mem0** (d√πng LOCOMO ƒë·ªÉ ƒëo c√°i n√†y), MemGPT (Letta).

***

### 3. Tr·ª• c·ªôt: SYSTEM PERFORMANCE (Hi·ªáu nƒÉng H·ªá th·ªëng)

*ƒê√°nh gi√° xem h·ªá th·ªëng c√≥ ch·∫°y nhanh v√† ·ªïn ƒë·ªãnh kh√¥ng? (Quan tr·ªçng cho OSS/Production)*.


| Metric (Ch·ªâ s·ªë) | √ù nghƒ©a | C√°ch ch·∫°y / Tool |
| :-- | :-- | :-- |
| **Latency P95 / P99** | Th·ªùi gian ph·∫£n h·ªìi m√† 95% request ƒë·∫°t ƒë∆∞·ª£c (lo·∫°i b·ªè ngo·∫°i l·ªá). Chia l√†m:<br>1. **Ingest Latency:** T·ªëc ƒë·ªô l∆∞u/update.<br>2. **Search Latency:** T·ªëc ƒë·ªô t√¨m.<br>3. **E2E Latency:** T·ªïng th·ªùi gian (Search + LLM). | **Tool:** Locust, JMeter, Python `time` module.<br>**Mem0 Benchmark:** So s√°nh Search Latency vs Vector DB thu·∫ßn. |
| **Throughput (QPS)** | H·ªá th·ªëng ch·ªãu ƒë∆∞·ª£c bao nhi√™u query/gi√¢y? | **C√°ch ch·∫°y:** Stress test b·∫Øn li√™n t·ª•c request v√†o API search. |
| **Storage Efficiency** | T·ªën bao nhi√™u RAM/Disk cho 1 tri·ªáu memories? | ƒêo dung l∆∞·ª£ng index c·ªßa Vector DB (Milvus/Qdrant). |

> **Ai d√πng?** C√°c team DevOps, Zep (qu·∫£ng c√°o latency th·∫•p), Mem0 (so s√°nh t·ªëc ƒë·ªô vs LangMem).

***

### 4. Tr·ª• c·ªôt: ROBUSTNESS \& EDGE CASES (Kh·∫£ nƒÉng X·ª≠ l√Ω Ca kh√≥)

*ƒê√°nh gi√° ƒë·ªô th√¥ng minh c·ªßa logic Memory (Memory Management)*.


| Metric (Ch·ªâ s·ªë) | √ù nghƒ©a | C√°ch ch·∫°y / Tool |
| :-- | :-- | :-- |
| **Conflict Resolution** | User update th√¥ng tin (A -> B). H·ªá th·ªëng tr·∫£ l·ªùi A hay B? | **Test:** "Update test". G·ª≠i 2 fact m√¢u thu·∫´n theo th·ªùi gian -> Query l·∫°i tr·∫°ng th√°i hi·ªán t·∫°i. |
| **Needle In A Haystack** | T√¨m 1 th√¥ng tin nh·ªè x√≠u trong ƒë·ªëng l·ªãch s·ª≠ kh·ªïng l·ªì (100k tokens). | **Benchmark:** **NIAH (Needle In A Haystack)**. Ch√®n 1 fact random v√†o gi·ªØa h·ªôi tho·∫°i d√†i -> Ask. |
| **Negation Handling** | Hi·ªÉu "T√¥i KH√îNG th√≠ch X". | **Test:** G·ª≠i c√¢u ph·ªß ƒë·ªãnh -> Search "S·ªü th√≠ch" -> N·∫øu ra X l√† Fail. |

> **Ai d√πng?** MemGPT (ƒë√°nh gi√° kh·∫£ nƒÉng qu·∫£n l√Ω context window), Mem0 (ƒë√°nh gi√° kh·∫£ nƒÉng update graph).

***

### T·ªîNG H·ª¢P: C√ÅC B·ªò BENCHMARK CHU·∫®N (State-of-the-Art)

N·∫øu b·∫°n mu·ªën benchmark Mem0 OSS c·ªßa m√¨nh "chu·∫©n b√†i", h√£y ch·∫°y 3 b√†i test n√†y theo th·ª© t·ª± ∆∞u ti√™n:

1. **LOCOMO (Long-term Conversational Memory Benchmark)**
    * *Do ai t·∫°o:* Snap Research (ƒë∆∞·ª£c Mem0 d√πng ch√≠nh).
    * *T·∫≠p trung:* Kh·∫£ nƒÉng nh·ªõ d√†i h·∫°n, suy lu·∫≠n theo th·ªùi gian.
    * *C√°ch ch·∫°y:* Ingest h·ªôi tho·∫°i d√†i -> H·ªèi -> GPT-4o ch·∫•m ƒëi·ªÉm.
2. **Needle In A Haystack (NIAH)**
    * *Do ai t·∫°o:* C·ªông ƒë·ªìng LLM (Greg Kamradt).
    * *T·∫≠p trung:* Kh·∫£ nƒÉng t√¨m ki·∫øm ch√≠nh x√°c (Retrieval).
    * *C√°ch ch·∫°y:* Ch√®n fact ·∫£o v√†o v·ªã tr√≠ 10%, 50%, 90% ƒë·ªô d√†i context -> Ki·ªÉm tra xem t√¨m ƒë∆∞·ª£c kh√¥ng.
3. **MemoryAgentBench**
    * *Do ai t·∫°o:* M·ªôt s·ªë paper g·∫ßn ƒë√¢y v·ªÅ Agent.
    * *T·∫≠p trung:* Kh·∫£ nƒÉng Agent *s·ª≠ d·ª•ng* memory ƒë·ªÉ h√†nh ƒë·ªông (Tool use + Memory).

### Quy tr√¨nh ch·∫°y Benchmark chu·∫©n (Pipeline)

ƒê·ªÉ ch·∫°y b·∫•t k·ª≥ b·ªô n√†o ·ªü tr√™n cho Mem0 OSS, b·∫°n c·∫ßn d·ª±ng pipeline code nh∆∞ sau:

1. **Adapter Layer:** Vi·∫øt 1 file Python wrap API c·ªßa b·∫°n (`POST /add`, `GET /search`) th√†nh class chu·∫©n (`class MyMem0OSS`).
2. **Data Loader:** Script ƒë·ªçc file dataset (LOCOMO json / Custom json).
3. **Execution Loop:**
    * Loop qua t·ª´ng h·ªôi tho·∫°i:
        * G·ª≠i `add` request (ƒëo Ingest Latency).
        * G·ª≠i `search` request (ƒëo Search Latency).
        * L·∫•y k·∫øt qu·∫£ search -> G·ª≠i v√†o LLM -> L·∫•y Answer (ƒëo E2E Latency).
4. **Evaluator:**
    * So s√°nh `Retrieved_IDs` vs `Gold_IDs` -> T√≠nh Recall.
    * G·ª≠i `(Question, Answer, Ground_Truth)` cho GPT-4o ch·∫•m -> T√≠nh Correctness Score.

B·∫°n c√≥ mu·ªën m√¨nh **vi·∫øt khung code Python (Skeleton Code)** cho pipeline n√†y ƒë·ªÉ b·∫°n ch·ªâ c·∫ßn ƒëi·ªÅn URL API c·ªßa b·∫°n v√†o l√† ch·∫°y kh√¥ng?


---



<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# C√°c ch·ªâ s·ªë ƒë·ªÉ ƒë√°nh gi√° b√†i benchmark cho b√†i Memory Mem0

ƒê·ªÉ ƒë√°nh gi√° hi·ªáu qu·∫£ c·ªßa th∆∞ vi·ªán **Mem0** (m·ªôt l·ªõp b·ªô nh·ªõ d√†i h·∫°n cho LLM), c√°c b√†i benchmark ti√™u chu·∫©n (ƒëi·ªÉn h√¨nh l√† **LOCOMO**) t·∫≠p trung v√†o 3 nh√≥m ch·ªâ s·ªë ch√≠nh: **Ch·∫•t l∆∞·ª£ng** (ƒë·ªô ch√≠nh x√°c c·ªßa th√¥ng tin h·ªìi t∆∞·ªüng), **Hi·ªáu nƒÉng** (t·ªëc ƒë·ªô ph·∫£n h·ªìi) v√† **Chi ph√≠** (m·ª©c ti√™u th·ª• t√†i nguy√™n).

D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt c√°c ch·ªâ s·ªë ƒë√°nh gi√° d·ª±a tr√™n c√°c b√°o c√°o k·ªπ thu·∫≠t v√† paper t·ª´ Mem0 v√† c√°c h·ªá th·ªëng t∆∞∆°ng t·ª±.

### 1. Ch·ªâ s·ªë v·ªÅ Ch·∫•t l∆∞·ª£ng (Quality \& Accuracy)

ƒê√¢y l√† nh√≥m ch·ªâ s·ªë quan tr·ªçng nh·∫•t ƒë·ªÉ x√°c ƒë·ªãnh xem AI c√≥ "nh·ªõ ƒë√∫ng" v√† "hi·ªÉu ƒë√∫ng" ng·ªØ c·∫£nh hay kh√¥ng. Mem0 s·ª≠ d·ª•ng benchmark **LOCOMO** (Long-term Conversational Memory) l√†m chu·∫©n ƒë√°nh gi√° ch√≠nh.


| Ch·ªâ s·ªë                     | √ù nghƒ©a \& C√°ch ƒëo l∆∞·ªùng                                                                                                                                                                                                                                | Ti√™u chu·∫©n c·ªßa Mem0                                                                        |
| :------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :----------------------------------------------------------------------------------------- |
| **LLM-as-a-Judge Score**   | S·ª≠ d·ª•ng m·ªôt model m·∫°nh (th∆∞·ªùng l√† GPT-4o) l√†m gi√°m kh·∫£o ƒë·ªÉ ch·∫•m ƒëi·ªÉm c√¢u tr·∫£ l·ªùi c·ªßa Mem0 d·ª±a tr√™n 3 ti√™u ch√≠: **T√≠nh ch√≠nh x√°c** (Correctness), **T√≠nh li√™n quan** (Relevance), v√† **T√≠nh ƒë·∫ßy ƒë·ªß** (Completeness).                                     | Mem0 ƒë·∫°t ƒëi·ªÉm cao h∆°n ~26% so v·ªõi OpenAI Memory tr√™n benchmark n√†y[^1][^2].                |
| **F1 Score / Exact Match** | ƒêo l∆∞·ªùng m·ª©c ƒë·ªô kh·ªõp ch√≠nh x√°c gi·ªØa th√¥ng tin ƒë∆∞·ª£c tr√≠ch xu·∫•t v√† s·ª± th·∫≠t ng·∫ßm ƒë·ªãnh (ground truth) trong d·ªØ li·ªáu. Ch·ªâ s·ªë n√†y d√πng ƒë·ªÉ ki·ªÉm tra c√°c s·ª± ki·ªán c·ª• th·ªÉ (v√≠ d·ª•: "Ng∆∞·ªùi d√πng th√≠ch u·ªëng g√¨?").                                                   | ƒê∆∞·ª£c s·ª≠ d·ª•ng trong c√°c t√°c v·ª• **QA** (H·ªèi ƒë√°p) c·ªßa b√†i test.                               |
| **Reasoning Accuracy**     | ƒê√°nh gi√° kh·∫£ nƒÉng suy lu·∫≠n tr√™n d·ªØ li·ªáu nh·ªõ:<br>- **Single-hop**: Truy xu·∫•t 1 s·ª± ki·ªán ƒë∆°n l·∫ª.<br>- **Multi-hop**: K·∫øt n·ªëi nhi·ªÅu s·ª± ki·ªán r·ªùi r·∫°c ƒë·ªÉ tr·∫£ l·ªùi.<br>- **Temporal**: Hi·ªÉu v·ªÅ th·ªùi gian (v√≠ d·ª•: ph√¢n bi·ªát s·ªü th√≠ch "nƒÉm ngo√°i" vs "hi·ªán t·∫°i"). | Mem0 m·∫°nh v·ªÅ *Temporal reasoning* (suy lu·∫≠n theo th·ªùi gian) nh·ªù c·∫•u tr√∫c Graph Memory[^3]. |
| **Recall@k**               | ƒêo l∆∞·ªùng kh·∫£ nƒÉng t√¨m ki·∫øm: Trong $k$ ƒëo·∫°n k√Ω ·ª©c ƒë∆∞·ª£c truy xu·∫•t, c√≥ bao nhi√™u ƒëo·∫°n th·ª±c s·ª± ch·ª©a th√¥ng tin c·∫ßn thi·∫øt?                                                                                                                                    | ƒê√°nh gi√° ri√™ng cho module Retriever c·ªßa h·ªá th·ªëng.                                          |

### 2. Ch·ªâ s·ªë v·ªÅ Hi·ªáu nƒÉng (Latency \& Speed)

V√¨ Mem0 ƒë∆∞·ª£c thi·∫øt k·∫ø cho c√°c ·ª©ng d·ª•ng th·ª±c t·∫ø (production), t·ªëc ƒë·ªô l√† y·∫øu t·ªë s·ªëng c√≤n. C√°c ch·ªâ s·ªë n√†y ƒëo l∆∞·ªùng ƒë·ªô tr·ªÖ t·ª´ l√∫c ng∆∞·ªùi d√πng h·ªèi ƒë·∫øn l√∫c nh·∫≠n ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi c√≥ k√®m k√Ω ·ª©c.

* **P95 Latency (95th Percentile)**: Th·ªùi gian ph·∫£n h·ªìi m√† 95% c√°c y√™u c·∫ßu ƒë·ªÅu ƒë·∫°t ƒë∆∞·ª£c ho·∫∑c nhanh h∆°n. ƒê√¢y l√† ch·ªâ s·ªë quan tr·ªçng nh·∫•t ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô ·ªïn ƒë·ªãnh.
    * *K·∫øt qu·∫£ tham kh·∫£o:* Mem0 gi·∫£m **91% ƒë·ªô tr·ªÖ P95** so v·ªõi c√°c ph∆∞∆°ng ph√°p full-context (kho·∫£ng 1.44s so v·ªõi 16.5s).[^1][^2]
* **Median Latency (P50)**: Th·ªùi gian ph·∫£n h·ªìi trung b√¨nh.
* **Search Latency vs. End-to-End Latency**:
    * *Search Latency*: Th·ªùi gian ch·ªâ ƒë·ªÉ t√¨m ki·∫øm k√Ω ·ª©c trong Vector/Graph Database.
    * *End-to-End*: T·ªïng th·ªùi gian (T√¨m ki·∫øm + LLM x·ª≠ l√Ω + Ph·∫£n h·ªìi).


### 3. Ch·ªâ s·ªë v·ªÅ Chi ph√≠ \& T√†i nguy√™n (Efficiency)

Nh√≥m ch·ªâ s·ªë n√†y ƒë√°nh gi√° t√≠nh kinh t·∫ø khi tri·ªÉn khai Mem0 quy m√¥ l·ªõn.

* **Token Usage per Query**: S·ªë l∆∞·ª£ng token c·∫ßn n·∫°p v√†o context window cho m·ªói c√¢u h·ªèi.
    * Mem0 gi√∫p **ti·∫øt ki·ªám ~90% token** so v·ªõi vi·ªác n·∫°p to√†n b·ªô l·ªãch s·ª≠ h·ªôi tho·∫°i v√†o LLM. Ch·ªâ s·ªë n√†y t·ª∑ l·ªá thu·∫≠n v·ªõi chi ph√≠ API (v√≠ d·ª•: chi ph√≠ tr·∫£ cho OpenAI/Anthropic).[^4][^1]
* **Storage Overhead**: Dung l∆∞·ª£ng l∆∞u tr·ªØ c·∫ßn thi·∫øt cho Vector Database (Qdrant/Milvus) ho·∫∑c Graph Database (Neo4j).


### T·ªïng h·ª£p: B·∫£ng Benchmark LOCOMO cho Mem0

Khi b·∫°n ƒë·ªçc c√°c b√°o c√°o benchmark v·ªÅ Mem0, b·∫°n s·∫Ω th∆∞·ªùng th·∫•y b·∫£ng so s√°nh t√≥m t·∫Øt nh∆∞ sau (s·ªë li·ªáu minh h·ªça t·ª´ b√°o c√°o c·ªßa Mem0):


| H·ªá th·ªëng | Accuracy (LLM Judge) | P95 Latency | Tokens / Query | ƒêi·ªÉm m·∫°nh |
| :-- | :-- | :-- | :-- | :-- |
| **Mem0** | **66.9%** | **~1.4s** | **Th·∫•p** | C√¢n b·∫±ng t·ªët nh·∫•t gi·ªØa T·ªëc ƒë·ªô v√† Ch√≠nh x√°c. |
| OpenAI Memory | 52.9% | 0.9s | Trung b√¨nh | C√†i ƒë·∫∑t nhanh nh∆∞ng kh·∫£ nƒÉng h·ªìi t∆∞·ªüng k√©m h∆°n. |
| LangMem | 58.1% | ~60s | R·∫•t th·∫•p | Qu√° ch·∫≠m cho chat, ph√π h·ª£p ph√¢n t√≠ch offline. |
| Full Context | - | Cao | R·∫•t cao | Chi ph√≠ ƒë·∫Øt ƒë·ªè v√† ƒë·ªô tr·ªÖ l·ªõn khi h·ªôi tho·∫°i d√†i. |

### L·ªùi khuy√™n cho vi·ªác Benchmark c·ªßa ri√™ng b·∫°n

N·∫øu b·∫°n mu·ªën t·ª± ƒë√°nh gi√° Mem0 cho ·ª©ng d·ª•ng c·ªßa m√¨nh, h√£y t·∫≠p trung v√†o 3 b√†i test sau:

1. **Needle in a Haystack (Kim ƒë√°y b·ªÉ)**: Ch√®n m·ªôt th√¥ng tin ng·∫´u nhi√™n v√†o l·ªãch s·ª≠ d√†i v√† y√™u c·∫ßu Mem0 t√¨m l·∫°i.
2. **Conflicting Information Update**: N√≥i v·ªõi AI "T√¥i th√≠ch m√†u ƒë·ªè", sau ƒë√≥ 10 l∆∞·ª£t h·ªôi tho·∫°i n√≥i "T√¥i ƒë·ªïi √Ω, t√¥i th√≠ch m√†u xanh". Ki·ªÉm tra xem Mem0 c√≥ tr·∫£ v·ªÅ m√†u xanh (m·ªõi nh·∫•t) hay kh√¥ng.
3. **Cross-session Recall**: T·∫Øt session hi·ªán t·∫°i, m·ªü session m·ªõi v√† h·ªèi l·∫°i th√¥ng tin t·ª´ session c≈©.

**Ngu·ªìn tham kh·∫£o ch√≠nh:**

* **LOCOMO Benchmark**: B·ªô d·ªØ li·ªáu chu·∫©n g·ªìm 10 ƒëo·∫°n h·ªôi tho·∫°i r·∫•t d√†i (avg 26k tokens) ƒë·ªÉ test kh·∫£ nƒÉng nh·ªõ d√†i h·∫°n.[^2][^5][^6]
* **LongMemEval**: Benchmark ƒë√°nh gi√° kh·∫£ nƒÉng suy lu·∫≠n ƒëa phi√™n v√† c·∫≠p nh·∫≠t tri th·ª©c.[^7]
<span style="display:none">[^10][^11][^12][^13][^14][^15][^16][^17][^18][^19][^20][^21][^22][^23][^24][^25][^26][^27][^28][^29][^30][^31][^32][^8][^9]</span>

<div align="center">‚ÅÇ</div>

[^1]: https://apidog.com/vi/blog/mem0-memory-llm-agents-vi/

[^2]: https://mem0.ai/research

[^3]: https://mem0.ai/blog/benchmarked-openai-memory-vs-langmem-vs-memgpt-vs-mem0-for-long-term-memory-here-s-how-they-stacked-up

[^4]: https://mem0.ai

[^5]: https://www.emergentmind.com/topics/locomo-benchmark-scores

[^6]: https://github.com/snap-research/locomo

[^7]: https://www.emergentmind.com/topics/longmemeval

[^8]: https://viblo.asia/p/gioi-thieu-ve-mem0-xay-dung-ai-agents-voi-long-term-memory-AY4qQN8q4Pw

[^9]: https://www.facebook.com/cung.AI.VN/photos/memos-m·ªü-r·ªông-tr√≠-nh·ªõ-c√°-v√†ng-c·ªßa-aiteam-c·ªßa-ƒë·∫°i-h·ªçc-chi·∫øt-giang-zhejiang-univer/686009927768097/

[^10]: https://viblo.asia/p/mem0-kien-truc-long-term-memory-cho-he-thong-ai-agent-G24B88pOLz3

[^11]: https://nama.vn/blogs/tin-cong-nghe/benchmark-la-gi

[^12]: https://www.reddit.com/r/singularity/comments/1oe1v70/whats_the_best_overall_ai_model_benchmark/

[^13]: https://www.cognee.ai/blog/deep-dives/ai-memory-evals-0825

[^14]: https://arxiv.org/abs/2402.17753

[^15]: https://www.flowhunt.io/vi/tu-dien/benchmarking/

[^16]: https://www.southbridge.ai/blog/mem0-technical-analysis-report

[^17]: https://github.com/mem0ai/mem0

[^18]: https://arxiv.org/abs/2506.13356

[^19]: https://aichatbot.com.vn/context-engineering-ky-nang-so-1-nam-2025-trong-linh-vuc-ai/

[^20]: https://www.zeroentropy.dev/articles/mem0-improves-memory-retrieval-accuracy-with-zeroentropy

[^21]: https://www.datacamp.com/tutorial/mem0-tutorial

[^22]: https://aclanthology.org/2025.findings-acl.1014.pdf

[^23]: https://www.marktechpost.com/2025/11/10/comparing-memory-systems-for-llm-agents-vector-graph-and-event-logs/

[^24]: https://www.emergentmind.com/topics/locomo-benchmark

[^25]: https://www.emergentmind.com/topics/memoryagentbench

[^26]: https://arxiv.org/html/2501.13956v1

[^27]: https://www.letta.com/blog/letta-leaderboard

[^28]: https://memmachine.ai/blog/2025/09/memmachine-reaches-new-heights-on-locomo/

[^29]: https://www.bachdyon.com/thiet-ke-chatbot-voi-tri-nho-dai-han-kien-truc-va-giai-phap-ky-thuat/

[^30]: https://blog.getzep.com/state-of-the-art-agent-memory/

[^31]: https://snap-research.github.io/locomo/

[^32]: https://www.studocu.vn/vn/document/uef-dai-hoc-kinh-te-tai-chinh-thanh-pho-ho-chi-minh/quan-tri-kinh-doanh/benchmarking-tom-tat/120303676




----




B√¢y gi·ªù t√¥i s·∫Ω t·∫°o m·ªôt t√†i li·ªáu ALL IN ONE EVAL MEMORY to√†n di·ªán cho b·∫°n:

# üìä ALL IN ONE EVAL MEMORY - T√ÄI LI·ªÜU TO√ÄN DI·ªÜN

## M·ª§C L·ª§C

### PH·∫¶N 1: KHUNG L√ù THUY·∫æT - C√ÅCH ƒê√ÅNH GI√Å MEMORY (MECE FRAMEWORK)

1. [4 Tr·ª• C·ªôt ƒê√°nh Gi√° Memory](https://www.genspark.ai/agents?id=9c9e3c48-9db8-41ec-8364-ba37898929f2#1-4-tr%E1%BB%A5-c%E1%BB%99t-%C4%91%C3%A1nh-gi%C3%A1-memory)
2. [C√°c Framework Memory H√†ng ƒê·∫ßu v√† C√°ch ƒê√°nh Gi√°](https://www.genspark.ai/agents?id=9c9e3c48-9db8-41ec-8364-ba37898929f2#2-c%C3%A1c-framework-memory-h%C3%A0ng-%C4%91%E1%BA%A7u)
3. [B·ªô Benchmark Chu·∫©n v√† Best Practices](https://www.genspark.ai/agents?id=9c9e3c48-9db8-41ec-8364-ba37898929f2#3-b%E1%BB%99-benchmark-chu%E1%BA%A9n)

### PH·∫¶N 2: TRI·ªÇN KHAI TH·ª∞C T·∫æ

4. [Pipeline Evaluation - Ki·∫øn Tr√∫c v√† Quy Tr√¨nh](https://www.genspark.ai/agents?id=9c9e3c48-9db8-41ec-8364-ba37898929f2#4-pipeline-evaluation)
5. [Code Implementation - Skeleton Code cho API c·ªßa B·∫°n](https://www.genspark.ai/agents?id=9c9e3c48-9db8-41ec-8364-ba37898929f2#5-code-implementation)
6. [Testing Scenarios - B·ªô Test Cases ƒê·∫ßy ƒê·ªß](https://www.genspark.ai/agents?id=9c9e3c48-9db8-41ec-8364-ba37898929f2#6-testing-scenarios)

---

## PH·∫¶N 1: KHUNG L√ù THUY·∫æT

## 1. 4 TR·ª§ C·ªòT ƒê√ÅNH GI√Å MEMORY (MECE FRAMEWORK)

ƒê·ªÉ ƒë√°nh gi√° to√†n di·ªán m·ªôt h·ªá th·ªëng Memory, c·ªông ƒë·ªìng AI hi·ªán t·∫°i ƒë√£ th·ªëng nh·∫•t **4 tr·ª• c·ªôt (Pillars)** ƒë·ªôc l·∫≠p v√† bao tr√πm (MECE):

### üéØ 1.1. RETRIEVAL QUALITY (Ch·∫•t L∆∞·ª£ng T√¨m Ki·∫øm)

**M·ª•c ƒë√≠ch:** ƒê√°nh gi√° kh·∫£ nƒÉng ‚Äúl·∫•y ƒë√∫ng k√Ω ·ª©c‚Äù t·ª´ kho l∆∞u tr·ªØ (ch∆∞a quan t√¢m ƒë·∫øn c√¢u tr·∫£ l·ªùi c·ªßa LLM).

|Metric|C√¥ng Th·ª©c|√ù Nghƒ©a|Tool/Framework|
|---|---|---|---|
|**Recall@k**|`# Relevant_in_Top_K / # Total_Relevant`|Trong top-k k·∫øt qu·∫£, c√≥ bao nhi√™u % k√Ω ·ª©c ƒë√∫ng?|Ragas, TruLens, Custom Scripts|
|**Precision@k**|`# Relevant_in_Top_K / K`|T·ª∑ l·ªá k√Ω ·ª©c h·ªØu √≠ch trong top-k (tr√°nh nhi·ªÖu)|Standard IR metrics|
|**MRR (Mean Reciprocal Rank)**|`1/rank_of_first_relevant`|K√Ω ·ª©c ƒë√∫ng xu·∫•t hi·ªán ·ªü v·ªã tr√≠ n√†o? (c√†ng s·ªõm c√†ng t·ªët)|Standard IR metrics|
|**NDCG@k**|Normalized Discounted Cumulative Gain|ƒê√°nh gi√° th·ª© t·ª± x·∫øp h·∫°ng c√≥ h·ª£p l√Ω kh√¥ng?|Advanced IR|

**Ai s·ª≠ d·ª•ng:** Zep, Mem0 (ph·∫ßn retrieval benchmark), LangMem, c√°c h·ªá th·ªëng RAG.

**D·∫´n ch·ª©ng:**

- **Mem0**: B√°o c√°o retrieval performance tr√™n LOCOMO, so s√°nh v·ªõi baseline RAG systems
- **Zep**: T·∫≠p trung v√†o low-latency retrieval v·ªõi P95 latency ~1.4s

---

### üß† 1.2. GENERATION QUALITY (Ch·∫•t L∆∞·ª£ng H·ªìi ƒê√°p)

**M·ª•c ƒë√≠ch:** ƒê√°nh gi√° xem sau khi l·∫•y ƒë∆∞·ª£c k√Ω ·ª©c, AI c√≥ tr·∫£ l·ªùi ƒë√∫ng c√¢u h·ªèi kh√¥ng?

|Metric|Ph∆∞∆°ng Ph√°p ƒêo|Framework D√πng|Baseline Accuracy|
|---|---|---|---|
|**LLM-as-a-Judge Score**|GPT-4o/Claude ch·∫•m ƒëi·ªÉm theo rubric 1-5 ho·∫∑c CORRECT/WRONG|Ragas, DeepEval, TruLens|Mem0: 66.9%, OpenAI Memory: 52.9%|
|**F1 Score / Exact Match**|So s√°nh text overlap gi·ªØa answer v√† ground truth|Standard NLP metrics|Bi·∫øn thi√™n theo domain|
|**Faithfulness**|C√¢u tr·∫£ l·ªùi c√≥ b·ªãa th√™m th√¥ng tin kh√¥ng c√≥ trong memory?|Ragas (`faithfulness` metric)|Target: >0.9|
|**Context Adherence**|C√≥ b√°m s√°t ng·ªØ c·∫£nh th·ªùi gian/logic kh√¥ng?|LOCOMO benchmark|Temporal reasoning: Mem0 >70%, OpenAI <30%|

**Best Practices - LLM-as-a-Judge:**

```python
# Prompt template chu·∫©n (theo LOCOMO benchmark)
JUDGE_PROMPT = """
Your task is to label an answer as 'CORRECT' or 'WRONG'.

Data:
- Question: {question}
- Gold Answer (Ground Truth): {expected_answer}
- Generated Answer: {ai_response}

Rules:
1. Be GENEROUS - if answer touches same topic as gold answer, mark CORRECT
2. For time questions: Accept relative time ("last Tuesday") if matches gold date
3. Format differences OK (e.g., "May 7th" vs "7 May")

Output JSON:
{
  "reasoning": "one sentence explanation",
  "label": "CORRECT" or "WRONG"
}
"""
```

**D·∫´n ch·ª©ng:**

- **Mem0**: S·ª≠ d·ª•ng GPT-4o l√†m judge v·ªõi temperature=0.1, ƒë·∫°t 26% higher accuracy so v·ªõi OpenAI Memory
- **Backboard**: ƒê·∫°t 90% overall accuracy tr√™n LOCOMO v·ªõi LLM judge evaluation

---

### ‚ö° 1.3. SYSTEM PERFORMANCE (Hi·ªáu NƒÉng H·ªá Th·ªëng)

**M·ª•c ƒë√≠ch:** ƒê√°nh gi√° t·ªëc ƒë·ªô v√† chi ph√≠ (quan tr·ªçng cho production).

|Metric|Target Benchmark|C√¥ng C·ª• ƒêo|Framework Comparison|
|---|---|---|---|
|**P50 Latency** (Median)|<2s cho retrieval + generation|Python `time`, Locust|Mem0: 1.4s, OpenAI: 0.9s|
|**P95 Latency** (95th Percentile)|<5s (lo·∫°i b·ªè outliers)|Locust, JMeter|Mem0: 1.44s, Full-context: 16.5s|
|**P99 Latency**|<10s|Load testing tools|Critical for SLA|
|**Throughput (QPS)**|S·ªë query/gi√¢y h·ªá th·ªëng ch·ªãu ƒë∆∞·ª£c|Stress testing|Depends on infra|
|**Token Usage**|S·ªë token/query (·∫£nh h∆∞·ªüng cost)|LLM API counter|Mem0 ti·∫øt ki·ªám ~90% vs full-context|
|**Storage Overhead**|MB/1K memories|Vector DB metrics|Vector: ~500MB/1M, Graph: th√™m ~200MB|

**Benchmark Setup:**

```python
# P95 Latency measurement
import time
import numpy as np

latencies = []
for _ in range(100):
    start = time.time()
    response = memory_system.search_and_answer(query)
    latencies.append(time.time() - start)

p50 = np.percentile(latencies, 50)
p95 = np.percentile(latencies, 95)
p99 = np.percentile(latencies, 99)
```

**D·∫´n ch·ª©ng:**

- **Mem0**: 91% gi·∫£m P95 latency (1.44s vs 16.5s c·ªßa full-context)
- **Zep**: Tuy√™n b·ªë low-latency architecture v·ªõi optimized vector search

---

### üõ°Ô∏è 1.4. ROBUSTNESS & EDGE CASES (Kh·∫£ NƒÉng X·ª≠ L√Ω Ca Kh√≥)

**M·ª•c ƒë√≠ch:** ƒê√°nh gi√° logic th√¥ng minh c·ªßa Memory Management.

|Test Case|K·ªãch B·∫£n|Expected Behavior|Framework Test|
|---|---|---|---|
|**Conflict Resolution**|User n√≥i ‚Äúth√≠ch m√†u ƒë·ªè‚Äù ‚Üí 10 turns sau ‚Üí ‚Äúƒë·ªïi √Ω, th√≠ch m√†u xanh‚Äù|Tr·∫£ v·ªÅ m√†u xanh (m·ªõi nh·∫•t)|Manual test|
|**Temporal Reasoning**|‚ÄúNƒÉm ngo√°i th√≠ch A, nƒÉm nay th√≠ch B‚Äù|Ph√¢n bi·ªát ƒë∆∞·ª£c timeline|LOCOMO temporal QA|
|**Needle In Haystack**|Ch√®n 1 fact v√†o gi·ªØa 100K tokens|T√¨m ƒë∆∞·ª£c fact ƒë√≥|NIAH benchmark|
|**Negation Handling**|‚ÄúT√¥i KH√îNG th√≠ch X‚Äù|Kh√¥ng tr·∫£ v·ªÅ X trong s·ªü th√≠ch|Custom test|
|**Cross-session Memory**|Th√¥ng tin t·ª´ session 1 ƒë∆∞·ª£c nh·ªõ ·ªü session 10|Persistent memory|LOCOMO multi-session|

**D·∫´n ch·ª©ng:**

- **MemGPT (Letta)**: ƒê√°nh gi√° kh·∫£ nƒÉng context window management v·ªõi NIAH
- **Mem0**: Graph Memory gi√∫p x·ª≠ l√Ω temporal reasoning t·ªët h∆°n 70% so v·ªõi OpenAI

---

## 2. C√ÅC FRAMEWORK MEMORY H√ÄNG ƒê·∫¶U V√Ä C√ÅCH ƒê√ÅNH GI√Å

### üìä So S√°nh T·ªïng Quan

|Framework|Ki·∫øn Tr√∫c Memory|Benchmark Ch√≠nh|ƒêi·ªÉm M·∫°nh|ƒêi·ªÉm Y·∫øu|Accuracy (LOCOMO)|
|---|---|---|---|---|---|
|**Mem0**|Hybrid (Vector + Graph)|LOCOMO, Custom|Balance gi·ªØa accuracy v√† speed|Ph·ª©c t·∫°p khi setup|**66.9%**|
|**OpenAI Memory**|Black-box (kh√¥ng c√¥ng b·ªë)|LOCOMO|D·ªÖ d√πng, t√≠ch h·ª£p s·∫µn|Accuracy th·∫•p, kh√¥ng customize|52.9%|
|**LangMem**|Vector + Compression|LOCOMO|Chi ph√≠ th·∫•p|Qu√° ch·∫≠m (P95: ~60s)|58.1%|
|**MemGPT (Letta)**|Filesystem + Tiered Memory|LOCOMO, Custom|Transparent, controllable|Ph·ª©c t·∫°p, c·∫ßn config nhi·ªÅu|~74% (v·ªõi filesystem)|
|**Zep**|Vector + Session Management|LOCOMO, Custom|Low latency|Accuracy trung b√¨nh|~75%|
|**Backboard**|Multi-layer Memory|LOCOMO|SOTA accuracy|Closed-source|**90%**|

### üî¨ Chi Ti·∫øt Ph∆∞∆°ng Ph√°p ƒê√°nh Gi√° T·ª´ng Framework

#### 2.1. Mem0 - C√°ch ƒê√°nh Gi√° Chi Ti·∫øt

**B·ªô Benchmark Ch√≠nh:**

1. **LOCOMO** (Long-Term Conversational Memory)
    
    - 10 conversations, avg 600 turns/conversation (~26K tokens)
    - 4 lo·∫°i c√¢u h·ªèi: Single-hop, Multi-hop, Temporal, Open-domain
    - ƒê√°nh gi√°: LLM-as-Judge (GPT-4o, temp=0.1)
2. **Custom Latency Benchmark**
    
    - So s√°nh search latency v·ªõi Vector DB thu·∫ßn
    - ƒêo E2E latency (search + LLM generation)

**K·∫øt Qu·∫£ C√¥ng B·ªë:**

```
Mem0 Performance (LOCOMO):
- Overall Accuracy: 66.9%
- Single-hop: 67.13%
- Multi-hop: 51.15%
- Temporal: 55.51%  ‚Üê ƒêi·ªÉm m·∫°nh nh·ªù Graph Memory
- Open-domain: 72.93%

- P95 Latency: 1.44s
- Token Usage: Gi·∫£m 90% vs full-context
```

**Source Code:**

- GitHub: [mem0ai/mem0](https://github.com/mem0ai/mem0)
- Benchmark code: C√≥ trong examples, nh∆∞ng kh√¥ng public evaluation pipeline ƒë·∫ßy ƒë·ªß
- Paper: [arXiv:2504.19413](https://arxiv.org/html/2504.19413v1)

#### 2.2. Zep - C√°ch ƒê√°nh Gi√°

**Focus:** Low-latency v√† Production-ready

**Benchmark:**

- LOCOMO benchmark (c√≥ tranh c√£i v·ªÅ accuracy reporting - xem [GitHub issue #5](https://github.com/getzep/zep-papers/issues/5))
- Deep Memory benchmark (custom)

**ƒê√°nh Gi√° T·ª´ Community:**

- Initial claim: 84% accuracy ‚Üí Corrected: ~58% (c√≥ dispute)
- P95 Latency: ~1.5-2s
- ƒêi·ªÉm m·∫°nh: Session management t·ªët

#### 2.3. LangMem - C√°ch ƒê√°nh Gi√°

**Ki·∫øn Tr√∫c:** Vector DB + Aggressive Compression

**Performance:**

```
LangMem (LOCOMO):
- Overall: 58.1%
- Single-hop: 62.23%
- Multi-hop: 47.92%
- P95 Latency: ~60s (qu√° ch·∫≠m cho chat)
```

**Use Case:** Ph√π h·ª£p offline analysis, kh√¥ng ph√π h·ª£p real-time chat.

#### 2.4. MemGPT (Letta) - C√°ch ƒê√°nh Gi√°

**ƒê·∫∑c Bi·ªát:** Transparent memory v·ªõi filesystem approach

**Benchmark Approach:**

- LOCOMO: Filesystem-based ƒë·∫°t 74% (beat c·∫£ specialized systems)
- NIAH (Needle In A Haystack): ƒê√°nh gi√° kh·∫£ nƒÉng context window management

**D·∫´n Ch·ª©ng:**  
[Letta Benchmark Blog](https://www.letta.com/blog/benchmarking-ai-agent-memory)

---

## 3. B·ªò BENCHMARK CHU·∫®N V√Ä BEST PRACTICES

### üèÜ 3.1. LOCOMO - Benchmark SOTA

**Th√¥ng Tin:**

- **T√™n ƒë·∫ßy ƒë·ªß:** Long-Term Conversational Memory Benchmark
- **T·ªï ch·ª©c:** Snap Research + UNC Chapel Hill
- **C√¥ng b·ªë:** ACL 2024
- **Paper:** [arXiv:2402.17753](https://arxiv.org/abs/2402.17753)

**Dataset Structure:**

```json
{
  "sample_id": "conv-001",
  "conversation": {
    "speaker_a": "Alice",
    "speaker_b": "Bob",
    "session_1": [...],  // List of turns
    "session_1_date_time": "2023-05-08T13:56:00Z",
    "session_2": [...],
    ...
  },
  "qa": [
    {
      "question": "What did Alice order for lunch?",
      "answer": "Caesar salad",
      "category": 1,  // 1=single-hop, 2=temporal, 3=multi-hop, 4=open-domain
      "evidence": ["dia_id_123"]  // Dialog IDs containing answer
    }
  ],
  "event_summary": [...],  // Ground truth event graphs
  "observation": [...]  // Generated observations (for RAG)
}
```

**3 Tasks:**

1. **Question Answering** (ch√≠nh)
2. **Event Graph Summarization**
3. **Multimodal Dialog Generation**

**GitHub:**

- [snap-research/locomo](https://github.com/snap-research/locomo)
- Dataset: `data/locomo10.json` (10 conversations)
- Evaluation scripts: C√≥ s·∫µn cho GPT, Claude, Gemini, HuggingFace models

**C√°ch Ch·∫°y LOCOMO:**

```bash
# 1. Clone repo
git clone https://github.com/snap-research/locomo.git

# 2. Install dependencies
pip install -r requirements.txt

# 3. Set API keys
export OPENAI_API_KEY="your-key"

# 4. Run evaluation
bash scripts/evaluate_gpts.sh  # Cho OpenAI models
bash scripts/evaluate_rag_gpts.sh  # Cho RAG-based
```

---

### üéØ 3.2. NIAH (Needle In A Haystack)

**M·ª•c ƒë√≠ch:** Test kh·∫£ nƒÉng t√¨m ki·∫øm ch√≠nh x√°c trong context d√†i.

**C√°ch Ho·∫°t ƒê·ªông:**

1. Ch√®n 1 ‚Äúneedle‚Äù (fact ng·∫´u nhi√™n) v√†o v·ªã tr√≠ X% trong context d√†i
2. H·ªèi v·ªÅ needle ƒë√≥
3. Ki·ªÉm tra xem model c√≥ t√¨m ƒë∆∞·ª£c kh√¥ng

**Bi·∫øn Th·ªÉ:**

```
Needle Position: [10%, 30%, 50%, 70%, 90%]
Context Length: [1K, 10K, 50K, 100K, 200K tokens]
‚Üí T·∫°o ra heatmap 5x5 ƒë·ªÉ ƒë√°nh gi√°
```

**Implementation:**

```python
def needle_in_haystack_test(memory_system, needle_position=0.5):
    # 1. T·∫°o haystack (context d√†i)
    haystack = generate_long_context(num_tokens=100000)
    
    # 2. Ch√®n needle
    needle = "The secret code is XJ-9274"
    insert_pos = int(len(haystack) * needle_position)
    full_context = haystack[:insert_pos] + needle + haystack[insert_pos:]
    
    # 3. Ingest v√†o memory
    memory_system.add(full_context)
    
    # 4. Query
    query = "What is the secret code?"
    response = memory_system.search(query)
    
    # 5. Check
    return "XJ-9274" in response
```

**Frameworks S·ª≠ D·ª•ng:**

- MemGPT/Letta: Primary benchmark
- OpenCompass: [NIAH Evaluation Guide](https://opencompass.readthedocs.io/en/latest/advanced_guides/needleinahaystack_eval.html)

---

### üß© 3.3. MemoryAgentBench

**ƒê·∫∑c ƒêi·ªÉm:** Benchmark cho Agent v·ªõi Memory (kh√¥ng ch·ªâ chatbot)

**4 Competencies:**

1. **Memory Formation:** C√≥ extract ƒë∆∞·ª£c info quan tr·ªçng kh√¥ng?
2. **Memory Retrieval:** T√¨m l·∫°i ƒë√∫ng l√∫c kh√¥ng?
3. **Memory Update:** X·ª≠ l√Ω conflicting info th·∫ø n√†o?
4. **Memory Application:** S·ª≠ d·ª•ng memory ƒë·ªÉ ra quy·∫øt ƒë·ªãnh

**Paper:** [arXiv:2507.05257](https://arxiv.org/html/2507.05257v1)

**Use Case:** Ph√π h·ª£p v·ªõi multi-step agent tasks (booking, planning, research).

---

### üìã 3.4. Best Practices - Quy Tr√¨nh Benchmark Chu·∫©n

**Pipeline 5 B∆∞·ªõc:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Step 1: DATA PREPARATION                                       ‚îÇ
‚îÇ  ‚Ä¢ Load benchmark dataset (LOCOMO/Custom)                       ‚îÇ
‚îÇ  ‚Ä¢ Parse conversations + questions + ground truth               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Step 2: MEMORY INGESTION                                       ‚îÇ
‚îÇ  ‚Ä¢ Create isolated assistant per conversation (optional)        ‚îÇ
‚îÇ  ‚Ä¢ Ingest turns with metadata (timestamps, speaker_id)          ‚îÇ
‚îÇ  ‚Ä¢ Measure: Ingest latency, storage overhead                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Step 3: RETRIEVAL EVALUATION                                   ‚îÇ
‚îÇ  ‚Ä¢ For each question: search(query)                             ‚îÇ
‚îÇ  ‚Ä¢ Compare retrieved IDs vs gold evidence IDs                   ‚îÇ
‚îÇ  ‚Ä¢ Metrics: Recall@k, Precision@k, MRR                          ‚îÇ
‚îÇ  ‚Ä¢ Measure: Search latency (P50/P95/P99)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Step 4: GENERATION EVALUATION                                  ‚îÇ
‚îÇ  ‚Ä¢ Generate answer using LLM + retrieved memory                 ‚îÇ
‚îÇ  ‚Ä¢ Measure: E2E latency, token usage                            ‚îÇ
‚îÇ  ‚Ä¢ Compare with ground truth                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Step 5: SCORING & ANALYSIS                                     ‚îÇ
‚îÇ  ‚Ä¢ LLM-as-Judge: GPT-4o ch·∫•m CORRECT/WRONG                      ‚îÇ
‚îÇ  ‚Ä¢ Calculate accuracy by category (single/multi/temporal)       ‚îÇ
‚îÇ  ‚Ä¢ Generate report: JSON + visualization                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## PH·∫¶N 2: TRI·ªÇN KHAI TH·ª∞C T·∫æ

## 4. PIPELINE EVALUATION - KI·∫æN TR√öC V√Ä QUY TR√åNH

### üèóÔ∏è 4.1. Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      YOUR API ENDPOINTS                          ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  POST /memories      ‚Üê Ingest conversation                      ‚îÇ
‚îÇ  POST /search        ‚Üê Retrieve relevant memories               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  EVALUATION FRAMEWORK                            ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ Data Loader  ‚îÇ‚Üí ‚îÇ Test Runner  ‚îÇ‚Üí ‚îÇ  Evaluator   ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                   ‚îÇ               ‚îÇ
‚îÇ         ‚ñº                   ‚ñº                   ‚ñº               ‚îÇ
‚îÇ   LOCOMO JSON        API Calls          LLM Judge              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      RESULTS OUTPUT                              ‚îÇ
‚îÇ  ‚Ä¢ Per-conversation JSON files                                  ‚îÇ
‚îÇ  ‚Ä¢ Aggregate metrics (accuracy, latency)                        ‚îÇ
‚îÇ  ‚Ä¢ Visualization (charts, heatmaps)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 5. CODE IMPLEMENTATION - SKELETON CODE CHO API C·ª¶A B·∫†N

### üöÄ 5.1. Full Evaluation Pipeline

T√¥i s·∫Ω vi·∫øt skeleton code cho API c·ªßa b·∫°n:

```python
#!/usr/bin/env python3
"""
Memory System Evaluation Framework
H·ªó tr·ª£ ƒë√°nh gi√° API Memory System tr√™n LOCOMO benchmark

Author: AI Assistant
Date: 2026-01-05
"""

import asyncio
import aiohttp
import json
import time
import numpy as np
from typing import List, Dict, Optional
from dataclasses import dataclass
from datetime import datetime

# ============================================================================
# CONFIGURATION
# ============================================================================

# API Endpoints c·ªßa b·∫°n
API_BASE_URL = "http://103.253.20.30:8889"
ADD_MEMORY_ENDPOINT = f"{API_BASE_URL}/memories"
SEARCH_ENDPOINT = f"{API_BASE_URL}/search"

# OpenAI API cho LLM Judge
OPENAI_API_KEY = "your-openai-key"

# Dataset path
LOCOMO_DATA_PATH = "./data/locomo10.json"

# Output directory
OUTPUT_DIR = "./results"

# Test parameters
TOP_K = 3
SCORE_THRESHOLD = 0.7
TIMEOUT = 30.0

# ============================================================================
# DATA MODELS
# ============================================================================

@dataclass
class Turn:
    """M·ªôt turn trong conversation"""
    speaker: str
    text: str
    dia_id: str
    img_url: Optional[str] = None
    
@dataclass
class Question:
    """M·ªôt c√¢u h·ªèi benchmark"""
    question: str
    answer: str
    category: int  # 1=single-hop, 2=temporal, 3=multi-hop, 4=open-domain
    evidence: List[str]  # Dialog IDs ch·ª©a c√¢u tr·∫£ l·ªùi

@dataclass
class Conversation:
    """M·ªôt conversation ƒë·∫ßy ƒë·ªß"""
    sample_id: str
    speaker_a: str
    speaker_b: str
    sessions: List[List[Turn]]
    session_timestamps: List[str]
    questions: List[Question]
    
# ============================================================================
# 1. DATA LOADER
# ============================================================================

class LOCOMODataLoader:
    """Load v√† parse LOCOMO dataset"""
    
    @staticmethod
    def load_dataset(path: str) -> List[Conversation]:
        """Load LOCOMO JSON file"""
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        conversations = []
        for sample in data:
            conv = LOCOMODataLoader._parse_conversation(sample)
            conversations.append(conv)
        
        return conversations
    
    @staticmethod
    def _parse_conversation(sample: dict) -> Conversation:
        """Parse m·ªôt conversation sample"""
        conv_data = sample['conversation']
        
        # Extract sessions
        sessions = []
        timestamps = []
        session_num = 1
        
        while f'session_{session_num}' in conv_data:
            session_key = f'session_{session_num}'
            timestamp_key = f'session_{session_num}_date_time'
            
            # Parse turns trong session
            turns = []
            for turn_data in conv_data[session_key]:
                turn = Turn(
                    speaker=turn_data['speaker'],
                    text=turn_data['text'],
                    dia_id=turn_data['dia_id'],
                    img_url=turn_data.get('img_url')
                )
                turns.append(turn)
            
            sessions.append(turns)
            timestamps.append(conv_data.get(timestamp_key, ''))
            session_num += 1
        
        # Parse questions
        questions = []
        for qa in sample.get('qa', []):
            # Filter category 5 (adversarial) nh∆∞ c√°c framework kh√°c
            if qa['category'] != 5:
                question = Question(
                    question=qa['question'],
                    answer=qa['answer'],
                    category=qa['category'],
                    evidence=qa.get('evidence', [])
                )
                questions.append(question)
        
        return Conversation(
            sample_id=sample['sample_id'],
            speaker_a=conv_data['speaker_a'],
            speaker_b=conv_data['speaker_b'],
            sessions=sessions,
            session_timestamps=timestamps,
            questions=questions
        )

# ============================================================================
# 2. API CLIENT - ADAPTER CHO API C·ª¶A B·∫†N
# ============================================================================

class MemoryAPIClient:
    """Wrapper cho API c·ªßa b·∫°n"""
    
    def __init__(self, base_url: str, timeout: float = 30.0):
        self.base_url = base_url
        self.timeout = aiohttp.ClientTimeout(total=timeout)
        
    async def add_memory(self, 
                         messages: List[Dict],
                         user_id: str,
                         run_id: str) -> Dict:
        """
        G·ªçi POST /memories ƒë·ªÉ ingest conversation
        
        Args:
            messages: List of {"role": "user/assistant", "content": "..."}
            user_id: ID c·ªßa user (v√≠ d·ª•: "ƒêo√†n Ng·ªçc C∆∞·ªùng")
            run_id: ID c·ªßa session/conversation (v√≠ d·ª•: "conv_001_session_1")
        
        Returns:
            Response t·ª´ API
        """
        payload = {
            "messages": messages,
            "user_id": user_id,
            "run_id": run_id
        }
        
        async with aiohttp.ClientSession(timeout=self.timeout) as session:
            async with session.post(
                f"{self.base_url}/memories",
                json=payload,
                headers={"Content-Type": "application/json"}
            ) as response:
                return await response.json()
    
    async def search(self,
                     query: str,
                     user_id: str,
                     top_k: int = 3,
                     limit: int = 10,
                     score_threshold: float = 0.7) -> Dict:
        """
        G·ªçi POST /search ƒë·ªÉ t√¨m ki·∫øm memories
        
        Args:
            query: C√¢u h·ªèi c·∫ßn t√¨m
            user_id: ID c·ªßa user
            top_k: S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ
            limit: Limit cho internal retrieval
            score_threshold: Ng∆∞·ª°ng ƒëi·ªÉm similarity
        
        Returns:
            Response t·ª´ API v·ªõi list of memories
        """
        payload = {
            "query": query,
            "user_id": user_id,
            "top_k": top_k,
            "limit": limit,
            "score_threshold": score_threshold
        }
        
        start_time = time.time()
        
        async with aiohttp.ClientSession(timeout=self.timeout) as session:
            async with session.post(
                f"{self.base_url}/search",
                json=payload,
                headers={"Content-Type": "application/json"}
            ) as response:
                result = await response.json()
                
        latency = time.time() - start_time
        result['_latency'] = latency
        
        return result

# ============================================================================
# 3. LLM JUDGE EVALUATOR
# ============================================================================

class LLMJudge:
    """GPT-4o l√†m gi√°m kh·∫£o ch·∫•m ƒëi·ªÉm"""
    
    JUDGE_PROMPT_TEMPLATE = """
Your task is to label an answer to a question as 'CORRECT' or 'WRONG'. 

You will be given:
(1) a question
(2) a 'gold' (ground truth) answer
(3) a generated answer

The gold answer is usually concise. The generated answer might be much longer, 
but you should be GENEROUS with your grading - as long as it touches on the 
same topic as the gold answer, it should be counted as CORRECT.

For time-related questions:
- The gold answer will be a specific date/time
- The generated answer might use relative time ("last Tuesday", "next month")
- Be generous - if it refers to the same time period, mark CORRECT
- Format differences are OK (e.g., "May 7th" vs "7 May")

Now evaluate this:

Question: {question}
Gold Answer: {expected_answer}
Generated Answer: {ai_response}

Provide a short (one sentence) explanation of your reasoning, then finish with 
CORRECT or WRONG.

Return your response in JSON format with two keys:
- "reasoning": your explanation
- "label": "CORRECT" or "WRONG"

Do NOT include both CORRECT and WRONG in your response.
"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o", temperature: float = 0.1):
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
        
    async def evaluate(self,
                       question: str,
                       expected_answer: str,
                       ai_response: str) -> Dict:
        """
        Ch·∫•m ƒëi·ªÉm m·ªôt c√¢u tr·∫£ l·ªùi
        
        Returns:
            {
                "label": "CORRECT" or "WRONG",
                "reasoning": "...",
                "is_correct": True/False
            }
        """
        prompt = self.JUDGE_PROMPT_TEMPLATE.format(
            question=question,
            expected_answer=expected_answer,
            ai_response=ai_response
        )
        
        async with aiohttp.ClientSession() as session:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "You are a precise evaluator."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": self.temperature,
                "response_format": {"type": "json_object"}
            }
            
            async with session.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload
            ) as response:
                result = await response.json()
                
        content = result['choices'][0]['message']['content']
        evaluation = json.loads(content)
        
        # Th√™m flag is_correct
        evaluation['is_correct'] = evaluation['label'].upper() == 'CORRECT'
        
        return evaluation

# ============================================================================
# 4. TEST RUNNER
# ============================================================================

class MemoryBenchmarkRunner:
    """Ch·∫°y benchmark ƒë·∫ßy ƒë·ªß"""
    
    def __init__(self,
                 api_client: MemoryAPIClient,
                 judge: LLMJudge,
                 output_dir: str = "./results"):
        self.api_client = api_client
        self.judge = judge
        self.output_dir = output_dir
        
        # Statistics
        self.results = []
        self.latencies = []
        
    async def run_conversation(self, conversation: Conversation) -> Dict:
        """
        Ch·∫°y benchmark cho m·ªôt conversation
        
        Pipeline:
        1. Ingest t·∫•t c·∫£ sessions
        2. Run questions
        3. Evaluate v·ªõi LLM Judge
        
        Returns:
            Results dict v·ªõi metrics
        """
        print(f"\n{'='*80}")
        print(f" Conversation: {conversation.sample_id}")
        print(f"{'='*80}\n")
        
        user_id = f"{conversation.speaker_a}_{conversation.speaker_b}"
        
        # Step 1: Ingest sessions
        print(f"üì• Ingesting {len(conversation.sessions)} sessions...")
        
        for idx, (session, timestamp) in enumerate(zip(
            conversation.sessions, 
            conversation.session_timestamps
        ), 1):
            run_id = f"{conversation.sample_id}_session_{idx}"
            
            # Convert turns to messages format
            messages = []
            for turn in session:
                role = "user" if turn.speaker == conversation.speaker_a else "assistant"
                messages.append({
                    "role": role,
                    "content": turn.text
                })
            
            # Ingest
            await self.api_client.add_memory(
                messages=messages,
                user_id=user_id,
                run_id=run_id
            )
            
            print(f"   ‚úì Session {idx}/{len(conversation.sessions)} - {len(messages)} turns")
        
        # Step 2: Run questions
        print(f"\n‚ùì Running {len(conversation.questions)} questions...\n")
        
        question_results = []
        
        for q_idx, question in enumerate(conversation.questions, 1):
            print(f"   Question {q_idx}/{len(conversation.questions)} [category {question.category}]")
            print(f"   Q: {question.question}")
            
            # Search
            search_result = await self.api_client.search(
                query=question.question,
                user_id=user_id,
                top_k=TOP_K,
                score_threshold=SCORE_THRESHOLD
            )
            
            search_latency = search_result.get('_latency', 0)
            self.latencies.append(search_latency)
            
            # Extract answer t·ª´ memories (gi·∫£ s·ª≠ API tr·∫£ v·ªÅ list memories)
            # TODO: ƒêi·ªÅu ch·ªânh theo format response th·ª±c t·∫ø c·ªßa API b·∫°n
            memories = search_result.get('results', [])
            ai_response = self._generate_answer_from_memories(
                question.question, 
                memories
            )
            
            print(f"   A: {ai_response}")
            print(f"   Expected: {question.answer}")
            print(f"   Latency: {search_latency:.2f}s")
            
            # Evaluate
            evaluation = await self.judge.evaluate(
                question=question.question,
                expected_answer=question.answer,
                ai_response=ai_response
            )
            
            result_symbol = "‚úÖ CORRECT" if evaluation['is_correct'] else "‚ùå WRONG"
            print(f"   {result_symbol}")
            print(f"   Reasoning: {evaluation['reasoning']}\n")
            
            # Save result
            question_result = {
                "question": question.question,
                "expected_answer": question.answer,
                "category": question.category,
                "ai_response": ai_response,
                "evaluation": evaluation,
                "latency": search_latency,
                "memories_retrieved": len(memories)
            }
            question_results.append(question_result)
        
        # Calculate metrics
        correct_count = sum(1 for r in question_results if r['evaluation']['is_correct'])
        accuracy = correct_count / len(question_results) * 100
        
        print(f"\n{'='*80}")
        print(f" Conversation {conversation.sample_id} Results:")
        print(f"   Accuracy: {correct_count}/{len(question_results)} ({accuracy:.1f}%)")
        print(f"   Avg Latency: {np.mean(self.latencies[-len(question_results):]):.2f}s")
        print(f"{'='*80}\n")
        
        return {
            "sample_id": conversation.sample_id,
            "total_questions": len(question_results),
            "correct": correct_count,
            "accuracy": accuracy,
            "results": question_results
        }
    
    def _generate_answer_from_memories(self, 
                                       question: str, 
                                       memories: List[Dict]) -> str:
        """
        Sinh c√¢u tr·∫£ l·ªùi t·ª´ memories retrieved
        
        TODO: 
        - N·∫øu API c·ªßa b·∫°n ƒë√£ tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi s·∫µn ‚Üí d√πng tr·ª±c ti·∫øp
        - N·∫øu ch·ªâ tr·∫£ v·ªÅ memories ‚Üí c·∫ßn g·ªçi LLM ƒë·ªÉ sinh answer
        
        V√≠ d·ª• ƒë∆°n gi·∫£n: Concatenate memories
        """
        if not memories:
            return "I don't have enough information to answer that question."
        
        # Gi·∫£ s·ª≠ m·ªói memory c√≥ field 'content' ho·∫∑c 'text'
        context = "\n".join([m.get('content', m.get('text', '')) for m in memories])
        
        # Option 1: Tr·∫£ v·ªÅ context tr·ª±c ti·∫øp (simple)
        # return context
        
        # Option 2: G·ªçi LLM ƒë·ªÉ sinh answer (recommended)
        # answer = await call_llm_to_generate_answer(question, context)
        # return answer
        
        # Placeholder
        return f"Based on the memories: {context[:200]}..."
    
    async def run_full_benchmark(self, conversations: List[Conversation]) -> Dict:
        """Ch·∫°y benchmark cho t·∫•t c·∫£ conversations"""
        
        print(f"\nüöÄ Starting LOCOMO Benchmark")
        print(f"   Total conversations: {len(conversations)}")
        print(f"   API: {self.api_client.base_url}\n")
        
        all_results = []
        
        for idx, conv in enumerate(conversations, 1):
            print(f"\n>>> Conversation {idx}/{len(conversations)}")
            
            try:
                result = await self.run_conversation(conv)
                all_results.append(result)
            except Exception as e:
                print(f"‚ùå Error processing conversation {conv.sample_id}: {e}")
                continue
        
        # Aggregate metrics
        return self._compute_aggregate_metrics(all_results)
    
    def _compute_aggregate_metrics(self, results: List[Dict]) -> Dict:
        """T√≠nh to√°n metrics t·ªïng h·ª£p"""
        
        total_questions = sum(r['total_questions'] for r in results)
        total_correct = sum(r['correct'] for r in results)
        overall_accuracy = total_correct / total_questions * 100
        
        # Per-category breakdown
        category_stats = {1: [], 2: [], 3: [], 4: []}  # single, temporal, multi, open-domain
        
        for result in results:
            for q_result in result['results']:
                cat = q_result['category']
                category_stats[cat].append(q_result['evaluation']['is_correct'])
        
        category_accuracy = {}
        for cat, correct_list in category_stats.items():
            if correct_list:
                acc = sum(correct_list) / len(correct_list) * 100
                category_accuracy[cat] = {
                    "category_name": self._get_category_name(cat),
                    "questions": len(correct_list),
                    "correct": sum(correct_list),
                    "accuracy": acc
                }
        
        # Latency stats
        p50 = np.percentile(self.latencies, 50)
        p95 = np.percentile(self.latencies, 95)
        p99 = np.percentile(self.latencies, 99)
        
        aggregate = {
            "total_conversations": len(results),
            "total_questions": total_questions,
            "total_correct": total_correct,
            "overall_accuracy": overall_accuracy,
            "category_breakdown": category_accuracy,
            "latency_stats": {
                "mean": np.mean(self.latencies),
                "p50": p50,
                "p95": p95,
                "p99": p99
            },
            "per_conversation_results": results
        }
        
        # Print summary
        print(f"\n{'='*80}")
        print(f" üìä FINAL RESULTS")
        print(f"{'='*80}\n")
        print(f"Overall Accuracy: {total_correct}/{total_questions} ({overall_accuracy:.1f}%)")
        print(f"\nPer-Category Accuracy:")
        for cat, stats in category_accuracy.items():
            print(f"   {stats['category_name']:20s}: {stats['correct']}/{stats['questions']:3d} ({stats['accuracy']:5.1f}%)")
        
        print(f"\nLatency Statistics:")
        print(f"   Mean:  {np.mean(self.latencies):.2f}s")
        print(f"   P50:   {p50:.2f}s")
        print(f"   P95:   {p95:.2f}s")
        print(f"   P99:   {p99:.2f}s")
        print(f"\n{'='*80}\n")
        
        # Save to file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"{self.output_dir}/benchmark_results_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(aggregate, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Results saved to: {output_file}\n")
        
        return aggregate
    
    @staticmethod
    def _get_category_name(cat: int) -> str:
        names = {
            1: "Single-hop",
            2: "Temporal",
            3: "Multi-hop",
            4: "Open-domain"
        }
        return names.get(cat, f"Category-{cat}")

# ============================================================================
# 5. MAIN - ENTRY POINT
# ============================================================================

async def main():
    """Main entry point"""
    
    # 1. Load dataset
    print("üìÇ Loading LOCOMO dataset...")
    loader = LOCOMODataLoader()
    conversations = loader.load_dataset(LOCOMO_DATA_PATH)
    print(f"   Loaded {len(conversations)} conversations\n")
    
    # 2. Initialize components
    api_client = MemoryAPIClient(base_url=API_BASE_URL, timeout=TIMEOUT)
    judge = LLMJudge(api_key=OPENAI_API_KEY)
    runner = MemoryBenchmarkRunner(
        api_client=api_client,
        judge=judge,
        output_dir=OUTPUT_DIR
    )
    
    # 3. Run benchmark
    results = await runner.run_full_benchmark(conversations)
    
    print("‚úÖ Benchmark completed!")
    
    return results

if __name__ == "__main__":
    # Run
    asyncio.run(main())
```

---

## 6. TESTING SCENARIOS - B·ªò TEST CASES ƒê·∫¶Y ƒê·ª¶

### üß™ 6.1. Unit Tests cho T·ª´ng Component

```python
"""
Unit tests cho Memory System
tests/test_memory_components.py
"""

import pytest
import asyncio
from memory_evaluator import MemoryAPIClient, LLMJudge

# ============================================================================
# 1. Test Conflict Resolution
# ============================================================================

@pytest.mark.asyncio
async def test_conflict_resolution():
    """
    Test: User thay ƒë·ªïi s·ªü th√≠ch
    
    Timeline:
    - Turn 1: "T√¥i th√≠ch m√†u ƒë·ªè"
    - Turn 10: "T√¥i ƒë·ªïi √Ω, t√¥i th√≠ch m√†u xanh"
    - Query: "S·ªü th√≠ch m√†u s·∫Øc c·ªßa t√¥i l√† g√¨?"
    
    Expected: Tr·∫£ v·ªÅ "m√†u xanh" (latest)
    """
    client = MemoryAPIClient(API_BASE_URL)
    
    # Ingest
    messages = [
        {"role": "user", "content": "T√¥i th√≠ch m√†u ƒë·ªè"},
        {"role": "assistant", "content": "ƒê∆∞·ª£c r·ªìi, t√¥i nh·ªõ r·ªìi!"},
        # ... 8 turns kh√°c
        {"role": "user", "content": "T√¥i ƒë·ªïi √Ω, t√¥i th√≠ch m√†u xanh"},
        {"role": "assistant", "content": "OK, c·∫≠p nh·∫≠t r·ªìi"}
    ]
    
    await client.add_memory(
        messages=messages,
        user_id="test_user_conflict",
        run_id="conflict_test_1"
    )
    
    # Query
    result = await client.search(
        query="S·ªü th√≠ch m√†u s·∫Øc c·ªßa t√¥i l√† g√¨?",
        user_id="test_user_conflict"
    )
    
    # Assert
    response_text = str(result).lower()
    assert "xanh" in response_text, "Should return latest preference (xanh)"
    assert "ƒë·ªè" not in response_text or response_text.index("xanh") < response_text.index("ƒë·ªè"), \
        "xanh should appear before ƒë·ªè if both mentioned"

# ============================================================================
# 2. Test Temporal Reasoning
# ============================================================================

@pytest.mark.asyncio
async def test_temporal_reasoning():
    """
    Test: Ph√¢n bi·ªát s·ª± ki·ªán theo th·ªùi gian
    
    Timeline:
    - Session 1 (May 2023): "T√¥i ƒëi ƒê√† L·∫°t"
    - Session 2 (June 2023): "T√¥i ƒëi Nha Trang"
    - Query: "Th√°ng 6 nƒÉm 2023 t√¥i ƒëi ƒë√¢u?"
    
    Expected: "Nha Trang"
    """
    client = MemoryAPIClient(API_BASE_URL)
    
    # Session 1: May
    await client.add_memory(
        messages=[
            {"role": "user", "content": "T√¥i v·ª´a ƒëi ƒê√† L·∫°t v·ªÅ"}
        ],
        user_id="test_user_temporal",
        run_id="temporal_may_2023"
    )
    
    # Session 2: June
    await client.add_memory(
        messages=[
            {"role": "user", "content": "T√¥i v·ª´a ƒëi Nha Trang v·ªÅ"}
        ],
        user_id="test_user_temporal",
        run_id="temporal_june_2023"
    )
    
    # Query v·ªõi temporal context
    result = await client.search(
        query="Th√°ng 6 nƒÉm 2023 t√¥i ƒëi ƒë√¢u?",
        user_id="test_user_temporal"
    )
    
    response_text = str(result).lower()
    assert "nha trang" in response_text
    assert "ƒë√† l·∫°t" not in response_text, "Should not return earlier trip"

# ============================================================================
# 3. Test Needle In Haystack
# ============================================================================

@pytest.mark.asyncio
async def test_needle_in_haystack():
    """
    Test: T√¨m th√¥ng tin c·ª• th·ªÉ trong conversation d√†i
    
    Setup:
    - 100 turns v·ªÅ nhi·ªÅu topics
    - Turn 50: "M√£ b√≠ m·∫≠t l√† XJ-9274"
    - Query: "M√£ b√≠ m·∫≠t l√† g√¨?"
    
    Expected: "XJ-9274"
    """
    client = MemoryAPIClient(API_BASE_URL)
    
    # T·∫°o haystack
    messages = []
    for i in range(100):
        if i == 49:  # Turn 50 (0-indexed)
            messages.append({
                "role": "user",
                "content": "√Ä ƒë√∫ng r·ªìi, m√£ b√≠ m·∫≠t l√† XJ-9274 nh√©"
            })
        else:
            messages.append({
                "role": "user",
                "content": f"ƒê√¢y l√† c√¢u s·ªë {i+1} n√≥i v·ªÅ ch·ªß ƒë·ªÅ ng·∫´u nhi√™n"
            })
    
    await client.add_memory(
        messages=messages,
        user_id="test_user_niah",
        run_id="niah_test"
    )
    
    # Query
    result = await client.search(
        query="M√£ b√≠ m·∫≠t l√† g√¨?",
        user_id="test_user_niah"
    )
    
    response_text = str(result)
    assert "XJ-9274" in response_text

# ============================================================================
# 4. Test Negation Handling
# ============================================================================

@pytest.mark.asyncio
async def test_negation_handling():
    """
    Test: Hi·ªÉu c√¢u ph·ªß ƒë·ªãnh
    
    Setup:
    - "T√¥i KH√îNG th√≠ch c√† ph√™"
    - Query: "T√¥i c√≥ th√≠ch c√† ph√™ kh√¥ng?"
    
    Expected: "kh√¥ng" / "no" / "kh√¥ng th√≠ch"
    """
    client = MemoryAPIClient(API_BASE_URL)
    
    await client.add_memory(
        messages=[
            {"role": "user", "content": "T√¥i KH√îNG th√≠ch c√† ph√™"}
        ],
        user_id="test_user_negation",
        run_id="negation_test"
    )
    
    result = await client.search(
        query="T√¥i c√≥ th√≠ch c√† ph√™ kh√¥ng?",
        user_id="test_user_negation"
    )
    
    response_text = str(result).lower()
    assert "kh√¥ng" in response_text or "no" in response_text

# ============================================================================
# 5. Test Cross-Session Memory
# ============================================================================

@pytest.mark.asyncio
async def test_cross_session_memory():
    """
    Test: Memory persist qua nhi·ªÅu sessions
    
    Setup:
    - Session 1: "T√¥i t√™n l√† C∆∞·ªùng"
    - Session 2: (10 turns v·ªÅ topic kh√°c)
    - Session 3: Query "T√™n t√¥i l√† g√¨?"
    
    Expected: "C∆∞·ªùng"
    """
    client = MemoryAPIClient(API_BASE_URL)
    user_id = "test_user_cross_session"
    
    # Session 1
    await client.add_memory(
        messages=[{"role": "user", "content": "T√¥i t√™n l√† C∆∞·ªùng"}],
        user_id=user_id,
        run_id="session_1"
    )
    
    # Session 2 (noise)
    await client.add_memory(
        messages=[
            {"role": "user", "content": f"Topic kh√°c turn {i}"}
            for i in range(10)
        ],
        user_id=user_id,
        run_id="session_2"
    )
    
    # Session 3: Query
    result = await client.search(
        query="T√™n t√¥i l√† g√¨?",
        user_id=user_id
    )
    
    response_text = str(result)
    assert "C∆∞·ªùng" in response_text or "cuong" in response_text.lower()

# ============================================================================
# 6. Test Latency Under Load
# ============================================================================

@pytest.mark.asyncio
async def test_latency_under_load():
    """
    Test: P95 latency v·ªõi concurrent requests
    
    Target: P95 < 5s
    """
    client = MemoryAPIClient(API_BASE_URL)
    
    # Prepare test data
    user_id = "test_user_latency"
    await client.add_memory(
        messages=[{"role": "user", "content": f"Info {i}"} for i in range(50)],
        user_id=user_id,
        run_id="latency_test"
    )
    
    # Concurrent queries
    latencies = []
    
    async def query_once():
        result = await client.search(
            query="Test query",
            user_id=user_id
        )
        return result.get('_latency', 0)
    
    # Run 100 concurrent requests
    tasks = [query_once() for _ in range(100)]
    latencies = await asyncio.gather(*tasks)
    
    # Calculate P95
    import numpy as np
    p95 = np.percentile(latencies, 95)
    
    print(f"P95 Latency: {p95:.2f}s")
    assert p95 < 5.0, f"P95 latency {p95:.2f}s exceeds 5s threshold"

# ============================================================================
# 7. Test LLM Judge Consistency
# ============================================================================

@pytest.mark.asyncio
async def test_llm_judge_consistency():
    """
    Test: LLM Judge c√≥ ch·∫•m ƒëi·ªÉm consistent kh√¥ng?
    
    Ch·∫°y c√πng 1 test case 10 l·∫ßn ‚Üí agreement rate > 90%
    """
    judge = LLMJudge(api_key=OPENAI_API_KEY)
    
    question = "What is the capital of France?"
    expected = "Paris"
    
    test_cases = [
        ("Paris", True),  # Exact match
        ("The capital is Paris", True),  # Contains correct answer
        ("It's Paris", True),  # Informal
        ("London", False),  # Wrong
        ("I don't know", False)  # No answer
    ]
    
    for ai_response, expected_correct in test_cases:
        # Run 10 times
        results = []
        for _ in range(10):
            eval_result = await judge.evaluate(
                question=question,
                expected_answer=expected,
                ai_response=ai_response
            )
            results.append(eval_result['is_correct'])
        
        # Check consistency
        agreement_rate = sum(results) / len(results)
        
        if expected_correct:
            assert agreement_rate > 0.9, f"Judge inconsistent for correct answer: {ai_response}"
        else:
            assert agreement_rate < 0.1, f"Judge inconsistent for wrong answer: {ai_response}"

```

### üé≠ 6.2. Integration Test v·ªõi LOCOMO Mini

```python
"""
Mini LOCOMO test v·ªõi 1 conversation nh·ªè
tests/test_integration_mini_locomo.py
"""

MINI_LOCOMO_DATA = {
    "sample_id": "test-001",
    "conversation": {
        "speaker_a": "Alice",
        "speaker_b": "Bob",
        "session_1": [
            {"speaker": "Alice", "text": "T·ªõ l√† Alice, t·ªõ l√†m AI Engineer", "dia_id": "d001"},
            {"speaker": "Bob", "text": "Ch√†o Alice! C·∫≠u l√†m v·ªÅ m·∫£ng g√¨?", "dia_id": "d002"},
            {"speaker": "Alice", "text": "T·ªõ l√†m LLM v√† RAG", "dia_id": "d003"}
        ],
        "session_1_date_time": "2023-05-01T10:00:00Z",
        "session_2": [
            {"speaker": "Alice", "text": "Tu·∫ßn tr∆∞·ªõc t·ªõ ƒëi ƒê√† L·∫°t", "dia_id": "d004"},
            {"speaker": "Bob", "text": "Th·∫ø √†? Vui kh√¥ng?", "dia_id": "d005"},
            {"speaker": "Alice", "text": "R·∫•t vui! T·ªõ th√≠ch th·ªùi ti·∫øt ·ªü ƒë√≥", "dia_id": "d006"}
        ],
        "session_2_date_time": "2023-05-08T14:00:00Z"
    },
    "qa": [
        {
            "question": "Alice l√†m ngh·ªÅ g√¨?",
            "answer": "AI Engineer",
            "category": 1,  # single-hop
            "evidence": ["d001"]
        },
        {
            "question": "Alice l√†m v·ªÅ m·∫£ng g√¨ c·ª• th·ªÉ?",
            "answer": "LLM v√† RAG",
            "category": 1,
            "evidence": ["d003"]
        },
        {
            "question": "Alice ƒëi ƒë√¢u v√†o tu·∫ßn tr∆∞·ªõc session 2?",
            "answer": "ƒê√† L·∫°t",
            "category": 2,  # temporal
            "evidence": ["d004"]
        }
    ]
}

@pytest.mark.asyncio
async def test_mini_locomo_integration():
    """Ch·∫°y full pipeline v·ªõi 1 conversation nh·ªè"""
    
    # 1. Setup
    client = MemoryAPIClient(API_BASE_URL)
    judge = LLMJudge(api_key=OPENAI_API_KEY)
    
    # 2. Parse data (gi·ªëng LOCOMODataLoader)
    conv = MINI_LOCOMO_DATA
    user_id = f"{conv['conversation']['speaker_a']}_{conv['conversation']['speaker_b']}"
    
    # 3. Ingest sessions
    for session_num in [1, 2]:
        session_key = f"session_{session_num}"
        turns = conv['conversation'][session_key]
        
        messages = []
        for turn in turns:
            role = "user" if turn['speaker'] == "Alice" else "assistant"
            messages.append({"role": role, "content": turn['text']})
        
        await client.add_memory(
            messages=messages,
            user_id=user_id,
            run_id=f"{conv['sample_id']}_session_{session_num}"
        )
    
    # 4. Run questions
    results = []
    for qa in conv['qa']:
        # Search
        search_result = await client.search(
            query=qa['question'],
            user_id=user_id
        )
        
        # TODO: Generate answer (simplified - just use first memory)
        memories = search_result.get('results', [])
        ai_response = memories[0]['content'] if memories else "Kh√¥ng bi·∫øt"
        
        # Evaluate
        evaluation = await judge.evaluate(
            question=qa['question'],
            expected_answer=qa['answer'],
            ai_response=ai_response
        )
        
        results.append(evaluation['is_correct'])
    
    # 5. Assert
    accuracy = sum(results) / len(results) * 100
    print(f"Mini LOCOMO Accuracy: {accuracy:.1f}%")
    
    assert accuracy >= 66.0, f"Accuracy {accuracy:.1f}% below Mem0 baseline (66.9%)"
```

---

## 7. CHECKLIST TRI·ªÇN KHAI

### ‚úÖ Phase 1: Setup C∆° B·∫£n (1-2 ng√†y)

- [ ] Clone LOCOMO dataset t·ª´ [snap-research/locomo](https://github.com/snap-research/locomo)
- [ ] C√†i ƒë·∫∑t dependencies (`requirements.txt`)
- [ ] Setup OpenAI API key cho LLM Judge
- [ ] Test API endpoints (`/memories`, `/search`) v·ªõi curl
- [ ] Ch·∫°y unit test c∆° b·∫£n (conflict, negation)

### ‚úÖ Phase 2: Adapter Layer (1 ng√†y)

- [ ] Implement `MemoryAPIClient` class
- [ ] Test connection v·ªõi API c·ªßa b·∫°n
- [ ] ƒêi·ªÅu ch·ªânh format request/response match v·ªõi API spec
- [ ] Handle error cases (timeout, 4xx, 5xx)

### ‚úÖ Phase 3: Evaluation Pipeline (2-3 ng√†y)

- [ ] Implement `LOCOMODataLoader`
- [ ] Implement `LLMJudge` v·ªõi GPT-4o
- [ ] Implement `MemoryBenchmarkRunner`
- [ ] Test v·ªõi Mini LOCOMO (1 conversation)
- [ ] Ch·∫°y full LOCOMO (10 conversations)

### ‚úÖ Phase 4: Analysis & Optimization (1-2 ng√†y)

- [ ] Generate metrics report (accuracy, latency)
- [ ] So s√°nh v·ªõi baselines (Mem0, OpenAI, Zep)
- [ ] Identify failure cases (xem question n√†o sai)
- [ ] Optimize system d·ª±a tr√™n findings

---

## 8. K·∫æT LU·∫¨N V√Ä KHUY·∫æN NGH·ªä

### üéØ Key Takeaways

1. **MECE Framework:** 4 tr·ª• c·ªôt ƒë·ªôc l·∫≠p - Retrieval, Generation, Performance, Robustness
2. **Benchmark Chu·∫©n:** LOCOMO l√† gold standard cho conversational memory (ƒë∆∞·ª£c Mem0, Zep, Backboard s·ª≠ d·ª•ng)
3. **LLM-as-Judge:** GPT-4o v·ªõi temperature=0.1 l√† ph∆∞∆°ng ph√°p evaluation ƒë∆∞·ª£c ch·∫•p nh·∫≠n r·ªông r√£i
4. **Baselines:** Mem0 (66.9%), Zep (~75%), Backboard (90%) - Target c·ªßa b·∫°n n√™n aim ‚â•65% ƒë·ªÉ competitive

### üí° Best Practices

1. **Isolated Testing:** T·∫°o assistant/user ri√™ng cho m·ªói conversation ƒë·ªÉ tr√°nh memory leak
2. **Timestamp Metadata:** Lu√¥n attach timestamp v√†o memories ƒë·ªÉ support temporal reasoning
3. **Generous Grading:** LLM Judge n√™n lenient - ch·∫•m CORRECT n·∫øu answer ‚Äútouches same topic‚Äù
4. **P95 Latency:** Target <5s, optimal <2s cho production
5. **Incremental Testing:** Ch·∫°y mini-test tr∆∞·ªõc khi ch·∫°y full benchmark (ti·∫øt ki·ªám cost)

### üöÄ Next Steps

Sau khi c√≥ k·∫øt qu·∫£ benchmark:

1. **So s√°nh v·ªõi SOTA:** ƒê·∫∑t k·∫øt qu·∫£ c·ªßa b·∫°n v√†o b·∫£ng leaderboard
2. **Failure Analysis:** Review t·ª´ng c√¢u sai ‚Üí t√¨m pattern ‚Üí fix
3. **Ablation Study:** Test t·ª´ng component ri√™ng (vector search, graph, reranking‚Ä¶)
4. **Publish Results:** N·∫øu ƒë·∫°t SOTA ‚Üí vi·∫øt blog post/paper

---

## PH·ª§ L·ª§C

### A. API Response Format (C·∫ßn ƒëi·ªÅu ch·ªânh theo API c·ªßa b·∫°n)

```json
{
  "results": [
    {
      "content": "Text c·ªßa memory",
      "score": 0.95,
      "metadata": {
        "timestamp": "2023-05-08T13:56:00Z",
        "run_id": "session_1"
      }
    }
  ],
  "total": 5
}
```

### B. Useful Links

- [LOCOMO Dataset](https://github.com/snap-research/locomo)
- [Mem0 Paper](https://arxiv.org/html/2504.19413v1)
- [Backboard Benchmark Implementation](https://github.com/Backboard-io/Backboard-Locomo-Benchmark)
- [Ragas Documentation](https://docs.ragas.io/)
- [DeepEval Documentation](https://docs.confident-ai.com/)

### C. Cost Estimation

**Full LOCOMO Benchmark:**

- Conversations: 10
- Questions: ~250
- LLM Judge calls: ~250 √ó GPT-4o = ~$5-10
- Total time: 30-60 minutes (v·ªõi concurrent processing)

---

**T√°c gi·∫£:** AI Assistant  
**Ng√†y:** 2026-01-05  
**Phi√™n b·∫£n:** 1.0

---

B·∫°n c√≥ mu·ªën t√¥i:

1. ‚úçÔ∏è Vi·∫øt th√™m code cho ph·∫ßn generate answer t·ª´ memories (integrate v·ªõi LLM)?
2. üìä T·∫°o visualization script ƒë·ªÉ plot accuracy heatmap?
3. üîß Customize cho format API c·ª• th·ªÉ c·ªßa b·∫°n (c·∫ßn xem response example)?
4. üß™ Vi·∫øt th√™m advanced test cases (multi-agent, RAG comparison)?