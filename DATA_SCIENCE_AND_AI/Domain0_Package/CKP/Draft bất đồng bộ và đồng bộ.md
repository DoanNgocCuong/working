# MECE Analysis: LLM Fallback Logic Cases

## T·ªïng quan

- **Entry Point**: `predict()` method
- **Main Timeout**: 1.5s
- **Fallback Timeout**: 4.0s (FALLBACK_TIMEOUT)
- **Fallback Model**: gpt-4o-mini

---

## Level 1: Initial Conditions

### Case 1.1: first_message v√† start_message ƒë∆∞·ª£c set

- **Condition**: `kwargs.get("first_message") is not None AND kwargs.get("start_message") not in [None, ""]`
- **Logic**:
  - Return `INTENT_FALLBACK` ngay l·∫≠p t·ª©c
  - Kh√¥ng g·ªçi LLM
- **Result**: `INTENT_FALLBACK`

### Case 1.2: Normal flow (kh√¥ng c√≥ first_message/start_message)

- **Condition**: Kh√¥ng th·ªèa Case 1.1
- **Logic**: Ti·∫øp t·ª•c v·ªõi main task
- **Next**: Level 2

---

## Level 2: Main Task Execution (< 1.5s)

### Case 2.1: Main task th√†nh c√¥ng trong 1.5s

- **Condition**: `await asyncio.wait_for(main_task, timeout=1.5)` th√†nh c√¥ng
- **Logic**:
  - Return k·∫øt qu·∫£ t·ª´ main task
  - Kh√¥ng k√≠ch ho·∫°t fallback
- **Result**: Main LLM response
- **Trace**: C√≥ trace trong Langfuse (main LLM)

### Case 2.2: Main task timeout sau 1.5s

- **Condition**: `asyncio.TimeoutError` trong 1.5s
- **Alert**: Send HIGH alert v·ªÅ LLM timeout
- **Next**: Level 3 (Fallback Decision)

### Case 2.3: Main task fail ngay (exception kh√¥ng ph·∫£i timeout)

- **Condition**: Exception kh√°c `asyncio.TimeoutError` trong 1.5s
- **Next**: Level 3 (Fallback Decision)

---

## Level 3: Fallback Decision

### Case 3.1: C√≥ th·ªÉ fallback (can_fallback = True)

- **Condition**: `fallback_llm is not None OR provider_name == "openai"`
- **Sub-cases**: Level 4 (_run_fallback_parallel)

### Case 3.2: Kh√¥ng th·ªÉ fallback (can_fallback = False)

- **Condition**: `fallback_llm is None AND provider_name != "openai"`
- **Logic**:
  - ƒê·ª£i main task ho√†n th√†nh v·ªõi timeout 4s
  - N·∫øu th√†nh c√¥ng ‚Üí return main result
  - N·∫øu fail/timeout ‚Üí return `INTENT_FALLBACK`
- **Result**: Main result ho·∫∑c `INTENT_FALLBACK`
- **Trace**: C√≥ trace main LLM (n·∫øu th√†nh c√¥ng)

---

## Level 4: Parallel Execution (_run_fallback_parallel)

**Context**: C·∫£ main_task v√† fallback_task ƒëang ch·∫°y song song
**Timeout t·ªïng**: 4s (FALLBACK_TIMEOUT)

### Case 4.1: C·∫£ 2 tasks timeout sau 4s

- **Condition**: `asyncio.wait_for(asyncio.wait(...), timeout=4.0)` timeout
- **Logic**:
  - Cancel c·∫£ 2 tasks
  - Send CRITICAL alert v·ªÅ c·∫£ 2 timeout
- **Result**: `INTENT_FALLBACK`
- **Trace**: C·∫£ 2 ƒë·ªÅu c√≥ trace (nh∆∞ng timeout)

### Case 4.2: C√≥ √≠t nh·∫•t 1 task ho√†n th√†nh trong 4s

- **Condition**: C√≥ task trong `done` set
- **Next**: Level 5 (Task Completion Handling)

---

## Level 5: First Completed Task Handling

### Case 5.1: Task ƒë·∫ßu ti√™n ho√†n th√†nh TH√ÄNH C√îNG

- **Condition**: `await completed_task` th√†nh c√¥ng
- **Sub-cases**: Level 6 (Which task completed first)

### Case 5.2: Task ƒë·∫ßu ti√™n ho√†n th√†nh NH∆ØNG FAIL

- **Condition**: `await completed_task` raise Exception
- **Logic**: ƒê·ª£i task c√≤n l·∫°i
- **Next**: Level 7 (Remaining Task Handling)

---

## Level 6: Which Task Completed First (Success Case)

### Case 6.1: Main task v·ªÅ tr∆∞·ªõc (th√†nh c√¥ng)

- **Condition**: `completed_task == main_task AND res = await completed_task th√†nh c√¥ng`
- **Logic**:
  - Return main result
  - Cancel fallback_task
- **Result**: Main LLM response
- **Trace**:
  - Main LLM: Complete trace
  - Fallback LLM: Cancelled (c√≥ trace nh∆∞ng cancelled)

### Case 6.2: Fallback task v·ªÅ tr∆∞·ªõc (th√†nh c√¥ng)

- **Condition**: `completed_task == fallback_task AND res = await completed_task th√†nh c√¥ng`
- **Logic**:
  - Return fallback result (d√πng lu√¥n)
  - Check main task status:
    - **N·∫øu main ƒë√£ done (fail/success)**: Ch·ªâ log, kh√¥ng ƒë·ª£i
    - **N·∫øu main ch∆∞a done (ƒëang ch·∫°y)**: ƒê·ª£i main v·ªõi timeout 4s ƒë·ªÉ trace
      - N·∫øu main timeout sau 4s ‚Üí cancel main
      - N·∫øu main th√†nh c√¥ng trong 4s ‚Üí gi·ªØ trace
      - N·∫øu main fail trong 4s ‚Üí log nh∆∞ng ƒë√£ c√≥ result
- **Result**: Fallback LLM response
- **Trace**:
  - Fallback LLM: Complete trace
  - Main LLM: Trace (n·∫øu ƒëang ch·∫°y th√¨ ƒë·ª£i ƒë·ªÉ trace, n·∫øu ƒë√£ fail/success th√¨ ch·ªâ log)

---

## Level 7: Remaining Task Handling (First Task Failed)

### Case 7.1: Remaining task l√† MAIN v√† th√†nh c√¥ng

- **Condition**:
  - First completed task failed
  - `remaining_task == main_task`
  - `await asyncio.wait_for(remaining_task, timeout=4.0)` th√†nh c√¥ng
- **Logic**:
  - Return main result
  - Cancel fallback_task
- **Result**: Main LLM response
- **Trace**:
  - Main LLM: Complete trace
  - Fallback LLM: Failed + Cancelled

### Case 7.2: Remaining task l√† FALLBACK v√† th√†nh c√¥ng

- **Condition**:
  - First completed task failed (main task ƒë√£ fail tr∆∞·ªõc ƒë√≥)
  - `remaining_task == fallback_task`
  - `await asyncio.wait_for(remaining_task, timeout=4.0)` th√†nh c√¥ng
- **Logic**:
  - Return fallback result
  - Check main task status:
    - **N·∫øu main ƒë√£ done (fail)**: Kh√¥ng ƒë·ª£i v√¨ main ƒë√£ fail r·ªìi, kh√¥ng c·∫ßn trace th√™m
    - **N·∫øu main ch∆∞a done (ƒëang ch·∫°y)**: ƒê·ª£i main v·ªõi timeout 4s ƒë·ªÉ trace (t∆∞∆°ng t·ª± Case 6.2)
- **Result**: Fallback LLM response
- **Trace**:
  - Fallback LLM: Complete trace
  - Main LLM: Trace (n·∫øu ƒëang ch·∫°y th√¨ ƒë·ª£i ƒë·ªÉ trace, n·∫øu ƒë√£ fail th√¨ kh√¥ng ƒë·ª£i)

### Case 7.3: Remaining task c≈©ng FAIL

- **Condition**: `await asyncio.wait_for(remaining_task, timeout=4.0)` raise Exception
- **Logic**:
  - Cancel c·∫£ 2 tasks
  - Send CRITICAL alert v·ªÅ c·∫£ 2 failed
- **Result**: `INTENT_FALLBACK`
- **Trace**: C·∫£ 2 ƒë·ªÅu c√≥ trace (nh∆∞ng failed)

---

## Level 8: Top-level Exception Handling

### Case 8.1: asyncio.CancelledError (t·ª´ b√™n ngo√†i)

- **Condition**: Task b·ªã cancel b·ªüi caller (v√≠ d·ª•: phoneme classifier ƒë√£ return)
- **Logic**: Return `INTENT_FALLBACK`
- **Result**: `INTENT_FALLBACK`
- **Note**: ƒê√¢y l√† behavior mong mu·ªën, kh√¥ng ph·∫£i error

### Case 8.2: Exception kh√°c (unexpected)

- **Condition**: Exception kh√¥ng ƒë∆∞·ª£c catch ·ªü c√°c level tr√™n
- **Logic**:
  - Log error
  - Return `INTENT_FALLBACK`
- **Result**: `INTENT_FALLBACK`

---

## Summary Table

| Case ID       | Condition                                | Result                  | Trace Status                             | Alerts    |
| ------------- | ---------------------------------------- | ----------------------- | ---------------------------------------- | --------- |
| 1.1           | first_message/start_message set          | INTENT_FALLBACK         | None                                     | None      |
| 2.1           | Main success < 1.5s                      | Main response           | Main only                                | None      |
| 2.2           | Main timeout 1.5s ‚Üí can_fallback=False  | Main or INTENT_FALLBACK | Main (if success)                        | None      |
| 2.3           | Main fail early ‚Üí can_fallback=False    | Main or INTENT_FALLBACK | Main (if success)                        | None      |
| 3.2 (timeout) | Main timeout + no fallback               | Main or INTENT_FALLBACK | Main (if success)                        | None      |
| 3.2 (fail)    | Main fail + no fallback                  | Main or INTENT_FALLBACK | Main (if success)                        | None      |
| 4.1           | Both timeout 4s                          | INTENT_FALLBACK         | Both (timeout)                           | CRITICAL  |
| 6.1           | Main first (success)                     | Main response           | Both (main complete, fallback cancelled) | None      |
| 6.2           | Fallback first (success)                 | Fallback response       | Both (fallback complete, main trace)     | None      |
| 7.1           | First failed, main remaining success     | Main response           | Both (main complete, fallback failed)    | None      |
| 7.2           | First failed, fallback remaining success | Fallback response       | Both (fallback complete, main trace)     | None      |
| 7.3           | Both failed                              | INTENT_FALLBACK         | Both (failed)                            | CRITICAL  |
| 8.1           | CancelledError                           | INTENT_FALLBACK         | None                                     | None      |
| 8.2           | Unexpected exception                     | INTENT_FALLBACK         | None                                     | Error log |

---

## Key Design Decisions

1. **Trace Preservation & Smart Wait Logic**: Khi fallback v·ªÅ tr∆∞·ªõc, ch·ªâ ƒë·ª£i main task n·∫øu main ch∆∞a done (ƒëang ch·∫°y).
   - N·∫øu main ƒë√£ success: Kh√¥ng ƒë·ª£i v√¨ main ƒë√£ ch·∫°y xong v√† trace xong r·ªìi
   - N·∫øu main ƒë√£ fail: Kh√¥ng ƒë·ª£i v√¨ ƒë√£ fail r·ªìi, kh√¥ng c·∫ßn trace th√™m
   - N·∫øu main ch∆∞a done (ƒëang ch·∫°y): ƒê·ª£i v·ªõi timeout 4s ƒë·ªÉ trace (c√≥ th·ªÉ th√†nh c√¥ng ho·∫∑c timeout/fail)
2. **Resource Management**: Cancel tasks kh√¥ng c·∫ßn thi·∫øt ƒë·ªÉ tr√°nh t·ªën resource
3. **Timeout Strategy**:
   - Main: 1.5s initial timeout
   - Parallel: 4s total timeout cho c·∫£ 2 tasks
   - Main trace wait: 4s khi fallback v·ªÅ tr∆∞·ªõc (ch·ªâ n·∫øu main ch∆∞a done)
4. **Alert Strategy**:
   - HIGH: Main timeout (normal fallback trigger)
   - CRITICAL: Both timeout ho·∫∑c both failed
5. **No Metadata Logging**: Fallback LLM kh√¥ng log metadata th·ªß c√¥ng (ch·ªâ trace timing)


---




Gi·∫£i th√≠ch l·∫°i:

## V·∫•n ƒë·ªÅ hi·ªán t·∫°i (block lu·ªìng)

Flow hi·ªán t·∫°i khi fallback v·ªÅ tr∆∞·ªõc:

```
1. Fallback ho√†n th√†nh ‚Üí c√≥ result ‚úÖ
2. ƒê·ª£i main task (t·ªëi ƒëa 4s) ‚è≥ ‚Üê BLOCK ·ªü ƒë√¢y
3. Sau khi ƒë·ª£i xong ‚Üí return result ‚ùå
```

V·∫•n ƒë·ªÅ: D√π ƒë√£ c√≥ result t·ª´ fallback, user v·∫´n ph·∫£i ch·ªù th√™m 4s.

## Gi·∫£i ph√°p (kh√¥ng block)

Flow m·ªõi khi fallback v·ªÅ tr∆∞·ªõc:

```
1. Fallback ho√†n th√†nh ‚Üí c√≥ result ‚úÖ
2. Return result ngay l·∫≠p t·ª©c ‚úÖ ‚Üê Kh√¥ng ƒë·ª£i
3. ƒê·ª£i main task ·ªü background (ƒë·ªÉ trace) üîÑ ‚Üê Ch·∫°y ng·∫ßm, kh√¥ng block
```

L·ª£i √≠ch: User nh·∫≠n result ngay; main v·∫´n ƒë∆∞·ª£c trace ·ªü background.

## So s√°nh code

Tr∆∞·ªõc (block):

```python
res = await completed_task  # L·∫•y result t·ª´ fallback

if completed_task == fallback_task:
    await self._handle_fallback_completed_first(...)  # ‚è≥ ƒê·ª£i 4s ·ªü ƒë√¢y
  
return res  # Ch·ªâ return sau khi ƒë·ª£i xong
```

Sau (kh√¥ng block):

```python
res = await completed_task  # L·∫•y result t·ª´ fallback

if completed_task == fallback_task:
    # T·∫°o task ch·∫°y ·ªü background, kh√¥ng ƒë·ª£i
    asyncio.create_task(self._wait_for_main_task_with_timeout(...))
  
return res  # ‚úÖ Return ngay l·∫≠p t·ª©c
```

## V√≠ d·ª• th·ªùi gian

Tr∆∞·ªõc:

- t=0s: Fallback v·ªÅ, c√≥ result
- t=0-4s: ƒê·ª£i main
- t=4s: Return result ‚Üí User nh·∫≠n sau 4s

Sau:

- t=0s: Fallback v·ªÅ, c√≥ result
- t=0s: Return result ngay ‚Üí User nh·∫≠n ngay
- t=0-4s: Main trace ch·∫°y ·ªü background (kh√¥ng block)

B·∫°n mu·ªën t√¥i implement gi·∫£i ph√°p n√†y kh√¥ng?

---



 git commit -m "[Small Update: Trace main_llms khi main_llms b·ªã timeout 1.5s]

>> ---
>>
>> V·∫•n ƒë·ªÅ l√†: Khi main timeout 1.5s ch·∫°y song song fallback llms l√† openAI, nh∆∞ng log langfuse ch·ªâ hi·ªÉn th·ªã log c·ªßa fallback llms
>>
>> ---
>>
>> Update:
>>
>> 1. Khi main timeout 1.5s th√¨ b·∫≠t song song con llms fallback.
>>
>> - N·∫øu main v·ªÅ tr∆∞·ªõc, l·∫•y main v√† hu·ª∑ fallback
>> - N·∫øu fallback v·ªÅ tr∆∞·ªõc, v·∫´n ƒë·ª£i main ch·∫°y ti·∫øp ƒë·ªÉ ra k·∫øt qu·∫£ v·ªõi timeout 4s
>>   N·∫øu main ƒë√£ success ‚Üí ƒë√£ trace xong, kh√¥ng c·∫ßn ƒë·ª£i
>>   N·∫øu main ƒë√£ fail ‚Üí ƒë√£ fail, kh√¥ng c·∫ßn ƒë·ª£i
>>   N·∫øu main ƒëang ch·∫°y ‚Üí ƒë·ª£i v·ªõi timeout 4s ƒë·ªÉ trace
>>
>> 2. Timeout t·ªïng c·ªßa 2 con n√†y l√† 4s
>> 3. T·∫Øt capture_in, capture_out c·ªßa c·∫£ 2 con n√†y
>>
>> ---
>>
>> ƒê√£ ƒë√°p ·ª©ng ƒë·∫ßy ƒë·ªß c√°c y√™u c·∫ßu. Logic ƒë·ª£i main ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u: ch·ªâ ƒë·ª£i n·∫øu main ƒëang ch·∫°y (ch∆∞a done), kh√¥ng ƒë·ª£i n·∫øu main ƒë√£ done.
>>
>> ---
>>
>> ƒê·ª£i main ho√†n th√†nh ƒë·ªÉ trace (t·ªëi ƒëa 4s) c√≥ block lu·ªìng ko?
>>




ƒêang tri·ªÉn khai gi·∫£i ph√°p kh√¥ng block lu·ªìng:

[6 tools called]

ƒêang ki·ªÉm tra c√°c ch·ªó kh√°c g·ªçi `_handle_fallback_completed_first`:

[2 tools called]

ƒê√£ ho√†n t·∫•t. T√≥m t·∫Øt thay ƒë·ªïi:

## ƒê√£ implement gi·∫£i ph√°p kh√¥ng block lu·ªìng

### Thay ƒë·ªïi ch√≠nh:

1. **`_handle_fallback_completed_first`**:

   - ƒê·ªïi t·ª´ `async def` ‚Üí `def` (kh√¥ng c·∫ßn await)
   - T·∫°o background task v·ªõi `asyncio.create_task()` ƒë·ªÉ trace main
   - Return ngay, kh√¥ng block
2. **`_run_fallback_parallel`**:

   - B·ªè `await` tr∆∞·ªõc `_handle_fallback_completed_first()`
   - Return result ngay sau khi fallback v·ªÅ tr∆∞·ªõc

### K·∫øt qu·∫£:

**Tr∆∞·ªõc:**

```python
res = await fallback_task  # C√≥ result
await _handle_fallback_completed_first(...)  # ‚è≥ ƒê·ª£i 4s
return res  # Return sau 4s
```

**Sau:**

```python
res = await fallback_task  # C√≥ result
_handle_fallback_completed_first(...)  # ‚úÖ T·∫°o background task, kh√¥ng ƒë·ª£i
return res  # ‚úÖ Return ngay l·∫≠p t·ª©c
```

### L·ª£i √≠ch:

- User nh·∫≠n result ngay (kh√¥ng ch·ªù th√™m 4s)
- Main v·∫´n ƒë∆∞·ª£c trace ·ªü background
- Kh√¥ng block lu·ªìng ch√≠nh
- Performance t·ªët h∆°n

Kh√¥ng c√≥ linter errors. Code ƒë√£ s·∫µn s√†ng.


---



ƒêang ki·ªÉm tra code hi·ªán t·∫°i ƒë·ªÉ xem nh·ªØng g√¨ ƒë√£ thay ƒë·ªïi:

[3 tools called]

Nh·ªØng thay ƒë·ªïi ƒë√£ th·ª±c hi·ªán:

## Nh·ªØng thay ƒë·ªïi ƒë√£ th·ª±c hi·ªán

### 1. **Method `_handle_fallback_completed_first`** (d√≤ng 472-490)

**Tr∆∞·ªõc:**

```python
async def _handle_fallback_completed_first(...):
    """X·ª≠ l√Ω khi fallback v·ªÅ tr∆∞·ªõc: ƒë·ª£i main task v·ªõi timeout ƒë·ªÉ trace"""
    logging.info(...)
    await self._wait_for_main_task_with_timeout(main_task, conversation_id, timeout)
    # ‚ö†Ô∏è Block ·ªü ƒë√¢y - ƒë·ª£i main xong m·ªõi return
```

**Sau:**

```python
def _handle_fallback_completed_first(...):  # ‚úÖ ƒê·ªïi t·ª´ async def ‚Üí def
    """X·ª≠ l√Ω khi fallback v·ªÅ tr∆∞·ªõc: ƒë·ª£i main task ·ªü background ƒë·ªÉ trace (kh√¥ng block response)"""
    logging.info(...)
  
    # ‚úÖ T·∫°o nested async function ƒë·ªÉ ch·∫°y ·ªü background
    async def _trace_main_in_background():
        try:
            await self._wait_for_main_task_with_timeout(main_task, conversation_id, timeout)
        except Exception as e:
            logging.debug(...)  # Ignore errors trong background task
  
    # ‚úÖ Fire-and-forget: T·∫°o task ch·∫°y ·ªü background, kh√¥ng await
    asyncio.create_task(_trace_main_in_background())
    # ‚úÖ Return ngay, kh√¥ng block
```

**Thay ƒë·ªïi:**

- `async def` ‚Üí `def` (kh√¥ng c·∫ßn await)
- T·∫°o nested function `_trace_main_in_background()`
- D√πng `asyncio.create_task()` ƒë·ªÉ ch·∫°y ·ªü background
- Kh√¥ng await ‚Üí kh√¥ng block

### 2. **Method `_run_fallback_parallel`** (d√≤ng 584-585)

**Tr∆∞·ªõc:**

```python
if completed_task == fallback_task:
    await self._handle_fallback_completed_first(...)  # ‚ö†Ô∏è ƒê·ª£i ·ªü ƒë√¢y
return res  # Ch·ªâ return sau khi ƒë·ª£i xong
```

**Sau:**

```python
if completed_task == fallback_task:
    # ‚úÖ Kh√¥ng await - g·ªçi function b√¨nh th∆∞·ªùng (kh√¥ng async)
    self._handle_fallback_completed_first(main_task, conversation_id, timeout)
    # ‚úÖ Return ngay l·∫≠p t·ª©c, kh√¥ng ƒë·ª£i background task
return res  # ‚úÖ Return ngay
```

**Thay ƒë·ªïi:**

- B·ªè `await` tr∆∞·ªõc `_handle_fallback_completed_first()`
- Th√™m comment gi·∫£i th√≠ch: "return ngay, trace main ·ªü background (kh√¥ng block)"

## So s√°nh flow

### **Tr∆∞·ªõc (Block):**

```
1. Fallback ho√†n th√†nh ‚Üí res = "fallback_result" ‚úÖ
2. G·ªçi _handle_fallback_completed_first() 
3. ƒê·ª£i _wait_for_main_task_with_timeout() (t·ªëi ƒëa 4s) ‚è≥
4. Return res ‚ùå (sau 4s)
```

### **Sau (Non-block):**

```
1. Fallback ho√†n th√†nh ‚Üí res = "fallback_result" ‚úÖ
2. G·ªçi _handle_fallback_completed_first()
   - T·∫°o background task v·ªõi asyncio.create_task()
   - Return ngay (kh√¥ng ƒë·ª£i) ‚úÖ
3. Return res ‚úÖ (ngay l·∫≠p t·ª©c)
4. Background task ch·∫°y ng·∫ßm ƒë·ªÉ trace main üîÑ (kh√¥ng block response)
```

## T√°c ƒë·ªông

- Performance: User nh·∫≠n result ngay, kh√¥ng ch·ªù th√™m 4s
- Functionality: Main v·∫´n ƒë∆∞·ª£c trace ·ªü background
- Code: Gi·ªØ logic trace, ch·ªâ ƒë·ªïi c√°ch ch·∫°y (background thay v√¨ blocking)

ƒê√¢y l√† c√°c thay ƒë·ªïi ch√≠nh ƒë√£ th·ª±c hi·ªán.


---



```python
"""
L·ªõp BaseLLM - Qu·∫£n l√Ω t∆∞∆°ng t√°c v·ªõi c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM)

M√¥ t·∫£:
    L·ªõp n√†y cung c·∫•p giao di·ªán th·ªëng nh·∫•t ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi c√°c nh√† cung c·∫•p LLM kh√°c nhau,
    bao g·ªìm OpenAI v√† Google Gemini. N√≥ x·ª≠ l√Ω vi·ªác g·ª≠i tin nh·∫Øn, nh·∫≠n ph·∫£n h·ªìi,
    v√† qu·∫£n l√Ω c√°c l·ªói timeout v√† retry.

T√≠nh nƒÉng ch√≠nh:
    - H·ªó tr·ª£ nhi·ªÅu nh√† cung c·∫•p LLM (OpenAI, Gemini)
    - X·ª≠ l√Ω timeout v√† retry t·ª± ƒë·ªông
    - Chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng tin nh·∫Øn cho t·ª´ng nh√† cung c·∫•p
    - Logging chi ti·∫øt cho vi·ªác debug
    - X·ª≠ l√Ω l·ªói v√† fallback

C√°c bi·∫øn m√¥i tr∆∞·ªùng:
    INTENT_FALLBACK: Gi√° tr·ªã fallback khi kh√¥ng th·ªÉ ph√¢n lo·∫°i intent
    TIMEOUT: Th·ªùi gian timeout cho m·ªói request (m·∫∑c ƒë·ªãnh: 5 gi√¢y)
    MAX_RETRIES: S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa (m·∫∑c ƒë·ªãnh: 3 l·∫ßn)

V√≠ d·ª• s·ª≠ d·ª•ng:
    # Kh·ªüi t·∫°o v·ªõi OpenAI
    openai_llm = BaseLLM(
        provider_setting={"api_key": "your_key", "base_url": "https://api.openai.com/v1"},
        provider_name="openai"
    )
  
    # Kh·ªüi t·∫°o v·ªõi Gemini
    gemini_llm = BaseLLM(
        provider_setting={"api_key": "your_gemini_key"},
        provider_name="gemini"
    )
  
    # G·ª≠i tin nh·∫Øn
    response = await llm.predict(
        messages=[{"role": "user", "content": "Xin ch√†o"}],
        params={"model": "gpt-4", "temperature": 0.7}
    )
"""

from typing import List, Optional
import traceback
import logging
import asyncio
import time
import os
import json
import re

# This client is already initialized in app/common/langfuse/__init__.py at module level
# which is more efficient than creating a new client each time
# from app.common.langfuse import langfuse_client

# Now import observe - it will use the client from __init__.py
from langfuse import observe, Langfuse
langfuse_client = Langfuse()

from ..providers.llm_providers import OpenAIProvider, GeminiProvider, GroqProvider
from ..utils.helpers import MessageConverter, JsonParser

# Import alert system
try:
    # Try absolute import first
    from app.common.alerts import AlertType, AlertLevel
    from app.common.alerts.helpers.send_alert_safe import send_alert_safe
    ALERT_ENABLED = True
except ImportError:
    # If failed, try relative import ho·∫∑c disable
    try:
        import sys
        from pathlib import Path
        # Add root to path
        root_path = Path(__file__).parent.parent.parent.parent.parent.parent.parent
        if str(root_path) not in sys.path:
            sys.path.insert(0, str(root_path))
        from app.common.alerts import AlertType, AlertLevel
        from app.common.alerts.helpers.send_alert_safe import send_alert_safe
        ALERT_ENABLED = True
    except ImportError as e:
        logging.warning(f"[BaseLLM] Alert system not available: {e}")
        ALERT_ENABLED = False
        # Dummy functions ƒë·ªÉ tr√°nh error
        def send_alert_safe(*args, **kwargs):
            pass
        AlertType = type('AlertType', (), {'LLM_TIMEOUT': 'llm_timeout', 'LLM_BOTH_FAILED': 'llm_both_failed', 'LLM_RATE_LIMIT': 'llm_rate_limit', 'LLM_PROVIDER_DOWN': 'llm_provider_down'})
        AlertLevel = type('AlertLevel', (), {'HIGH': 'high', 'CRITICAL': 'critical'})

# Constants
INTENT_FALLBACK = os.getenv("INTENT_FALLBACK")
TIMEOUT = 5
MAX_RETRIES = 3
FALLBACK_MODEL = "gpt-4o-mini"
FALLBACK_TIMEOUT = 4.0  # Timeout cho fallback model (gi√¢y)
FALLBACK_PARAMS = {
  "max_tokens": 1024,
  "temperature": 0.0,
  "top_p": 1,
  "model": "gpt-4o-mini",
  "stream": False
}

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)

class BaseLLM:
    """
    L·ªõp c∆° s·ªü ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM)
  
    L·ªõp n√†y cung c·∫•p giao di·ªán th·ªëng nh·∫•t ƒë·ªÉ l√†m vi·ªác v·ªõi c√°c nh√† cung c·∫•p LLM kh√°c nhau,
    x·ª≠ l√Ω vi·ªác chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng tin nh·∫Øn, qu·∫£n l√Ω timeout, retry v√† logging.
    """

    def __init__(self, provider_setting: dict, provider_name: str = None, **kwargs):
        """
        Kh·ªüi t·∫°o ƒë·ªëi t∆∞·ª£ng BaseLLM
    
        Args:
            provider_setting (dict): C·∫•u h√¨nh k·∫øt n·ªëi ƒë·∫øn nh√† cung c·∫•p LLM
                - api_key (str): API key ƒë·ªÉ x√°c th·ª±c
                - base_url (str): URL endpoint c·ªßa API (cho OpenAI)
            provider_name (str, optional): T√™n nh√† cung c·∫•p ('openai', 'gemini', etc.)
            **kwargs: C√°c tham s·ªë b·ªï sung
        
        V√≠ d·ª•:
            # C·∫•u h√¨nh OpenAI
            openai_llm = BaseLLM(
                provider_setting={
                    "api_key": "sk-...",
                    "base_url": "https://api.openai.com/v1"
                },
                provider_name="openai"
            )
        
            # C·∫•u h√¨nh Gemini
            gemini_llm = BaseLLM(
                provider_setting={"api_key": "AIza..."},
                provider_name="gemini"
            )
        """
        self.provider_setting = provider_setting
        self.provider_name = provider_name
    
        logging.info(f"[BaseLLM.__init__] Initializing BaseLLM with provider_name={provider_name}, provider_setting_keys={list(provider_setting.keys()) if isinstance(provider_setting, dict) else 'not_dict'}")
    
        # Initialize providers - only initialize the one we need (lazy initialization)
        self.openai_provider = None
        self.gemini_provider = None
        self.groq_provider = None
    
        # Initialize the provider based on provider_name
        if provider_name == 'openai':
            self.openai_provider = OpenAIProvider(provider_setting)
            logging.info(f"[BaseLLM.__init__] Successfully initialized OpenAIProvider")
        elif provider_name == 'gemini':
            self.gemini_provider = GeminiProvider(provider_setting)
            logging.info(f"[BaseLLM.__init__] Successfully initialized GeminiProvider")
        elif provider_name == 'groq':
            self.groq_provider = GroqProvider(provider_setting)
            logging.info(f"[BaseLLM.__init__] Successfully initialized GroqProvider")
        else:
            logging.warning(f"[BaseLLM.__init__] Unknown provider_name={provider_name}, no provider initialized")
        # Note: We don't initialize all providers to avoid validation errors
        # when provider_setting doesn't match the provider
    
        # Initialize utilities
        self.message_converter = MessageConverter()
        self.json_parser = JsonParser()

    async def get_response(self, messages: List, conversation_id: str = None, **params):
        """
        G·ª≠i tin nh·∫Øn ƒë·∫øn LLM v√† nh·∫≠n ph·∫£n h·ªìi v·ªõi c∆° ch·∫ø retry v√† timeout
        """
        logging.info(f"[BaseLLM]============= {str(conversation_id)} -messages: {json.dumps(messages, indent=4, ensure_ascii=False)}")
        start_time = time.time()
    
        for attempt in range(MAX_RETRIES):
            try:
                if self.provider_name == 'gemini':
                    return await self.gemini_provider.get_response(
                        messages, conversation_id, start_time, **params
                    )
                elif self.provider_name == 'groq':
                    return await self.groq_provider.get_response(
                        messages, conversation_id, start_time, **params
                    )
                elif self.provider_name == 'openai':
                    return await self.openai_provider.get_response(
                        messages, conversation_id, start_time, **params
                    )
                else:
                    raise ValueError(f"Provider name {self.provider_name} not supported")
                
            except asyncio.TimeoutError:
                logging.info(f"[BaseLLM] {str(conversation_id)} - Timeout occurred on attempt {attempt + 1}/{MAX_RETRIES}")
                if attempt == MAX_RETRIES - 1:
                    logging.info(f"[BaseLLM] {str(conversation_id)} - All {MAX_RETRIES} attempts failed due to timeout")
                await asyncio.sleep(0.1)
        
            except Exception as e:
                # Check for rate limit errors (HTTP 429)
                error_str = str(e).lower()
                error_str_full = str(e)  # Keep original case for status code extraction
                status_code = None
            
                # Try to extract status code from OpenAI/Groq errors
                # Method 1: Check if error has status_code attribute
                if hasattr(e, 'status_code'):
                    status_code = e.status_code
                # Method 2: Check response object
                elif hasattr(e, 'response') and hasattr(e.response, 'status_code'):
                    status_code = e.response.status_code
                # Method 3: Extract from error message (e.g., "Error code: 401")
                elif 'error code:' in error_str_full.lower():
                    match = re.search(r'error code:\s*(\d+)', error_str_full, re.IGNORECASE)
                    if match:
                        status_code = int(match.group(1))
                # Method 4: Check error string patterns
                elif '429' in error_str or 'rate limit' in error_str:
                    status_code = 429
                elif '401' in error_str or 'invalid_api_key' in error_str or 'authentication' in error_str:
                    status_code = 401
                elif '403' in error_str or 'forbidden' in error_str:
                    status_code = 403
                elif '500' in error_str or '502' in error_str or '503' in error_str or '504' in error_str:
                    # Server errors
                    if '500' in error_str:
                        status_code = 500
                    elif '502' in error_str:
                        status_code = 502
                    elif '503' in error_str:
                        status_code = 503
                    elif '504' in error_str:
                        status_code = 504
            
                model_name = params.get("model", "unknown")
                provider_name = self.provider_name or "unknown"
            
                # Send alert for rate limit
                if status_code == 429 or 'rate limit' in error_str:
                    logging.warning(f"[BaseLLM] {conversation_id} - Rate limit detected: {e}")
                
                    if ALERT_ENABLED:
                        send_alert_safe(
                            alert_type=AlertType.LLM_RATE_LIMIT,
                            level=AlertLevel.HIGH,
                            message="LLM rate limit detected. Switching to fallback.",
                            context={
                                "conversation_id": conversation_id,
                                "provider": provider_name,
                                "model": model_name,
                                "attempt": attempt + 1,
                                "max_retries": MAX_RETRIES,
                                "error": str(e)[:500]
                            },
                            component="base_llm"
                        )
                
                    # Re-raise ƒë·ªÉ caller c√≥ th·ªÉ handle (switch to fallback)
                    raise
            
                # Send alert for invalid API key (specific case)
                if status_code == 401 and ('invalid_api_key' in error_str or 'api key' in error_str):
                    logging.warning(f"[BaseLLM] {conversation_id} - Invalid API key detected: {e}")
                
                    if ALERT_ENABLED:
                        send_alert_safe(
                            alert_type=AlertType.LLM_INVALID_API_KEY,
                            level=AlertLevel.CRITICAL,
                            message="LLM invalid API key detected.",
                            context={
                                "conversation_id": conversation_id,
                                "provider": provider_name,
                                "model": model_name,
                                "error": str(e)[:500]
                            },
                            component="base_llm"
                        )
                
                    # Re-raise ƒë·ªÉ caller c√≥ th·ªÉ handle (switch to fallback)
                    raise
            
                # Send alert for authentication errors (401, 403)
                if status_code in [401, 403] or 'authentication' in error_str or 'forbidden' in error_str:
                    logging.warning(f"[BaseLLM] {conversation_id} - Authentication/Authorization error detected: {e}")
                
                    if ALERT_ENABLED:
                        send_alert_safe(
                            alert_type=AlertType.LLM_PROVIDER_DOWN,
                            level=AlertLevel.HIGH,
                            message="LLM authentication/authorization error. Switching to fallback.",
                            context={
                                "conversation_id": conversation_id,
                                "provider": provider_name,
                                "model": model_name,
                                "status_code": status_code or "unknown",
                                "error_type": "authentication_error",
                                "error": str(e)[:500]
                            },
                            component="base_llm"
                        )
                
                    # Re-raise ƒë·ªÉ caller c√≥ th·ªÉ handle (switch to fallback)
                    raise
            
                # Check for token limit exceeded
                if 'token' in error_str and ('limit' in error_str or 'exceeded' in error_str or 'too many' in error_str):
                    logging.warning(f"[BaseLLM] {conversation_id} - Token limit exceeded: {e}")
                
                    if ALERT_ENABLED:
                        send_alert_safe(
                            alert_type=AlertType.LLM_TOKEN_LIMIT_EXCEEDED,
                            level=AlertLevel.MEDIUM,
                            message="LLM token limit exceeded.",
                            context={
                                "conversation_id": conversation_id,
                                "provider": provider_name,
                                "model": model_name,
                                "error": str(e)[:500]
                            },
                            component="base_llm"
                        )
                
                    # Re-raise ƒë·ªÉ caller c√≥ th·ªÉ handle
                    raise
            
                # Check for context overflow
                if 'context' in error_str and ('overflow' in error_str or 'too long' in error_str or 'exceeded' in error_str):
                    logging.warning(f"[BaseLLM] {conversation_id} - Context overflow detected: {e}")
                
                    if ALERT_ENABLED:
                        send_alert_safe(
                            alert_type=AlertType.LLM_CONTEXT_OVERFLOW,
                            level=AlertLevel.MEDIUM,
                            message="LLM context window overflow.",
                            context={
                                "conversation_id": conversation_id,
                                "provider": provider_name,
                                "model": model_name,
                                "error": str(e)[:500]
                            },
                            component="base_llm"
                        )
                
                    # Re-raise ƒë·ªÉ caller c√≥ th·ªÉ handle
                    raise
            
                # Send alert for server errors (5xx)
                if status_code in [500, 502, 503, 504]:
                    logging.warning(f"[BaseLLM] {conversation_id} - LLM provider server error detected: {e}")
                
                    if ALERT_ENABLED:
                        send_alert_safe(
                            alert_type=AlertType.LLM_PROVIDER_DOWN,
                            level=AlertLevel.HIGH,
                            message=f"LLM provider server error ({status_code}). Switching to fallback.",
                            context={
                                "conversation_id": conversation_id,
                                "provider": provider_name,
                                "model": model_name,
                                "status_code": status_code,
                                "error_type": "server_error",
                                "error": str(e)[:500]
                            },
                            component="base_llm"
                        )
                
                    # Re-raise ƒë·ªÉ caller c√≥ th·ªÉ handle (switch to fallback)
                    raise
            
                # Re-raise other errors
                raise

    # ==================== Helper Methods for Fallback Logic ====================
  
    async def _cancel_task_safe(self, task: asyncio.Task, task_name: str = "task") -> None:
        """Cancel task m·ªôt c√°ch an to√†n, b·ªè qua exceptions"""
        if not task.done():
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                logging.debug(f"[BaseLLM] {task_name} cancelled successfully")
            except Exception:
                pass  # Ignore other errors when cancelling
  
    async def _cancel_both_tasks(self, main_task: asyncio.Task, fallback_task: asyncio.Task) -> None:
        """Cancel c·∫£ 2 tasks m·ªôt c√°ch an to√†n"""
        await asyncio.gather(
            self._cancel_task_safe(main_task, "main_task"),
            self._cancel_task_safe(fallback_task, "fallback_task"),
            return_exceptions=True
        )
  
    def _send_both_timeout_alert(self, conversation_id: str, provider_name: str, model_name: str, timeout: float) -> None:
        """Send alert khi c·∫£ 2 LLM ƒë·ªÅu timeout"""
        if ALERT_ENABLED:
            send_alert_safe(
                alert_type=AlertType.LLM_TIMEOUT,
                level=AlertLevel.CRITICAL,
                message=f"Both main and fallback LLM timeout after {timeout}s. Using INTENT_FALLBACK.",
                context={
                    "conversation_id": conversation_id,
                    "provider": provider_name,
                    "model": model_name,
                    "fallback_model": FALLBACK_MODEL,
                    "timeout": f"{timeout}s"
                },
                component="base_llm"
            )
  
    def _send_both_failed_alert(self, conversation_id: str, provider_name: str, model_name: str, error: str) -> None:
        """Send alert khi c·∫£ 2 LLM ƒë·ªÅu fail"""
        if ALERT_ENABLED:
            send_alert_safe(
                alert_type=AlertType.LLM_BOTH_FAILED,
                level=AlertLevel.CRITICAL,
                message="Both main and fallback LLM failed. Using INTENT_FALLBACK.",
                context={
                    "conversation_id": conversation_id,
                    "provider": provider_name,
                    "model": model_name,
                    "fallback_model": FALLBACK_MODEL,
                    "error": error[:500]
                },
                component="base_llm"
            )
  
    async def _wait_for_main_task_with_timeout(
        self, 
        main_task: asyncio.Task, 
        conversation_id: str, 
        timeout: float = FALLBACK_TIMEOUT
    ) -> None:
        """
        ƒê·ª£i main task ho√†n th√†nh v·ªõi timeout, cancel n·∫øu timeout.
        D√πng khi fallback ƒë√£ v·ªÅ tr∆∞·ªõc nh∆∞ng mu·ªën trace main task.
        Ch·ªâ ƒë·ª£i n·∫øu main task ch∆∞a done (ƒëang ch·∫°y). N·∫øu ƒë√£ fail th√¨ kh√¥ng ƒë·ª£i.
        """
        # N·∫øu main task ƒë√£ done (fail ho·∫∑c success), kh√¥ng c·∫ßn ƒë·ª£i
        if main_task.done():
            if main_task.exception() is not None:
                # Main ƒë√£ fail, ch·ªâ log
                logging.warning(f"[BaseLLM] {conversation_id} - Main task already failed: {str(main_task.exception())}, but fallback result already available")
            else:
                # Main ƒë√£ success (nh∆∞ng fallback v·ªÅ tr∆∞·ªõc), ch·ªâ log
                logging.info(f"[BaseLLM] {conversation_id} - Main task already completed")
            return
    
        # Main task ƒëang ch·∫°y, ƒë·ª£i v·ªõi timeout ƒë·ªÉ trace
        try:
            await asyncio.wait_for(main_task, timeout=timeout)
            logging.info(f"[BaseLLM] {conversation_id} - Main task completed (traced in Langfuse)")
        except asyncio.TimeoutError:
            logging.warning(f"[BaseLLM] {conversation_id} - Main task timeout after {timeout}s, cancelling to avoid resource waste")
            await self._cancel_task_safe(main_task, "main_task")
        except Exception as e:
            logging.warning(f"[BaseLLM] {conversation_id} - Main task failed: {str(e)}, but fallback result already available")
            # Main task failed nh∆∞ng ƒë√£ ƒë∆∞·ª£c trace trong Langfuse
  
    async def _handle_main_completed_first(
        self,
        main_task: asyncio.Task,
        fallback_task: asyncio.Task,
        conversation_id: str
    ) -> None:
        """X·ª≠ l√Ω khi main task v·ªÅ tr∆∞·ªõc: cancel fallback task"""
        logging.info(f"[BaseLLM] {conversation_id} - Main task completed first, cancelling fallback task")
        await self._cancel_task_safe(fallback_task, "fallback_task")
  
    def _handle_fallback_completed_first(
        self,
        main_task: asyncio.Task,
        conversation_id: str,
        timeout: float = FALLBACK_TIMEOUT
    ) -> None:
        """X·ª≠ l√Ω khi fallback v·ªÅ tr∆∞·ªõc: ƒë·ª£i main task ·ªü background ƒë·ªÉ trace (kh√¥ng block response)"""
        logging.info(f"[BaseLLM] {conversation_id} - Fallback task completed first, using fallback result. Waiting for main task to complete in background (for tracing)")
    
        # ƒê·ª£i main ·ªü background ƒë·ªÉ trace, kh√¥ng block response
        async def _trace_main_in_background():
            try:
                await self._wait_for_main_task_with_timeout(main_task, conversation_id, timeout)
            except Exception as e:
                # Ignore errors trong background task (ƒë√£ c√≥ error handling trong _wait_for_main_task_with_timeout)
                logging.debug(f"[BaseLLM] {conversation_id} - Background main trace task error (ignored): {str(e)}")
    
        # Fire-and-forget: Ch·∫°y ·ªü background, kh√¥ng await
        asyncio.create_task(_trace_main_in_background())
  
    async def _handle_both_tasks_timeout(
        self,
        main_task: asyncio.Task,
        fallback_task: asyncio.Task,
        conversation_id: str,
        provider_name: str,
        model_name: str,
        timeout: float = FALLBACK_TIMEOUT
    ) -> str:
        """X·ª≠ l√Ω khi c·∫£ 2 tasks ƒë·ªÅu timeout: cancel c·∫£ 2, send alert, return INTENT_FALLBACK"""
        logging.warning(f"[BaseLLM] {conversation_id} - Both main and fallback tasks timeout after {timeout}s, cancelling both")
        await self._cancel_both_tasks(main_task, fallback_task)
        self._send_both_timeout_alert(conversation_id, provider_name, model_name, timeout)
        return INTENT_FALLBACK if INTENT_FALLBACK is not None else "fallback"
  
    async def _handle_first_task_failed(
        self,
        main_task: asyncio.Task,
        fallback_task: asyncio.Task,
        completed_task: asyncio.Task,
        conversation_id: str,
        provider_name: str,
        model_name: str,
        first_error: Exception,
        timeout: float = FALLBACK_TIMEOUT
    ) -> str:
        """X·ª≠ l√Ω khi task ƒë·∫ßu ti√™n fail: ƒë·ª£i task c√≤n l·∫°i"""
        logging.warning(f"[BaseLLM] {conversation_id} - First completed task failed: {str(first_error)}")
    
        # X√°c ƒë·ªãnh task c√≤n l·∫°i
        remaining_task = fallback_task if completed_task == main_task else main_task
    
        try:
            res = await asyncio.wait_for(remaining_task, timeout=timeout)
            logging.info(f"[BaseLLM] {conversation_id} - Remaining task succeeded")
        
            # N·∫øu remaining task l√† main v√† th√†nh c√¥ng, h·ªßy fallback
            if remaining_task == main_task:
                await self._cancel_task_safe(fallback_task, "fallback_task")
            # N·∫øu remaining task l√† fallback v√† th√†nh c√¥ng, ƒë·ª£i main v·ªõi timeout (n·∫øu main ch∆∞a done)
            else:
                # Ch·ªâ ƒë·ª£i main n·∫øu main ch∆∞a done (ƒëang ch·∫°y), kh√¥ng ƒë·ª£i n·∫øu main ƒë√£ fail
                await self._wait_for_main_task_with_timeout(main_task, conversation_id, timeout)
        
            return res
        except Exception as e2:
            # C·∫£ 2 ƒë·ªÅu fail
            logging.warning(f"[BaseLLM] {conversation_id} - Both tasks failed. First: {str(first_error)}, Remaining: {str(e2)}")
            await self._cancel_both_tasks(main_task, fallback_task)
            self._send_both_failed_alert(conversation_id, provider_name, model_name, traceback.format_exc())
            return INTENT_FALLBACK if INTENT_FALLBACK is not None else "fallback"
  
    async def _run_fallback_parallel(
        self,
        main_task: asyncio.Task,
        fallback_task: asyncio.Task,
        conversation_id: str,
        provider_name: str,
        model_name: str,
        timeout: float = FALLBACK_TIMEOUT
    ) -> str:
        """
        Ch·∫°y main v√† fallback song song, x·ª≠ l√Ω race condition.
    
        Returns:
            str: Result t·ª´ task v·ªÅ tr∆∞·ªõc, ho·∫∑c INTENT_FALLBACK n·∫øu c·∫£ 2 ƒë·ªÅu fail/timeout
        """
        # ƒê·ª£i m·ªôt trong 2 task ho√†n th√†nh tr∆∞·ªõc
        try:
            done, pending = await asyncio.wait_for(
                asyncio.wait(
                    {main_task, fallback_task},
                    return_when=asyncio.FIRST_COMPLETED
                ),
                timeout=timeout
            )
        except asyncio.TimeoutError:
            # C·∫£ 2 ƒë·ªÅu timeout
            return await self._handle_both_tasks_timeout(
                main_task, fallback_task, conversation_id, provider_name, model_name, timeout
            )
    
        # L·∫•y k·∫øt qu·∫£ t·ª´ task ho√†n th√†nh ƒë·∫ßu ti√™n
        completed_task = next(iter(done))
    
        try:
            res = await completed_task
        
            # X·ª≠ l√Ω theo task n√†o v·ªÅ tr∆∞·ªõc
            if completed_task == main_task:
                await self._handle_main_completed_first(main_task, fallback_task, conversation_id)
            else:
                # Fallback v·ªÅ tr∆∞·ªõc: return ngay, trace main ·ªü background (kh√¥ng block)
                self._handle_fallback_completed_first(main_task, conversation_id, timeout)
        
            return res
        except Exception as e:
            # Task ƒë·∫ßu ti√™n fail, ƒë·ª£i task c√≤n l·∫°i
            return await self._handle_first_task_failed(
                main_task, fallback_task, completed_task, conversation_id,
                provider_name, model_name, e, timeout
            )
  
    def _prepare_fallback_task(
        self,
        fallback_llm,
        provider_name: str,
        messages: list,
        conversation_id: str
    ) -> asyncio.Task:
        """Chu·∫©n b·ªã fallback task v√† return task"""
        fallback_params = dict(FALLBACK_PARAMS)
        fallback_params["model"] = FALLBACK_MODEL
        fallback_llm_instance = fallback_llm if fallback_llm is not None else self
        fallback_provider_name = getattr(
            fallback_llm_instance, 
            'provider_name', 
            'openai' if provider_name == 'openai' else 'unknown'
        )
    
        fallback_task = asyncio.create_task(
            self._trace_fallback_llm_call(
                fallback_llm_instance, messages, conversation_id, 
                fallback_params, fallback_provider_name
            )
        )
    
        return fallback_task

    # ==================== Trace Methods ====================
  
    @observe(name="robot-v2.llm.main", capture_input=False, capture_output=True)
    async def _trace_main_llm_call(self, messages, conversation_id, params):
        """Trace main LLM call separately in Langfuse"""
        try:
            return await self.get_response(messages, conversation_id, **params)
        except Exception as e:
            # Error s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông capture b·ªüi @observe
            raise

    @observe(name="robot-v2.llm.fallback", capture_input=False, capture_output=True)
    async def _trace_fallback_llm_call(self, fallback_llm_instance, messages, conversation_id, fallback_params, fallback_provider_name="unknown"):
        """Trace fallback LLM call separately in Langfuse"""
        try:
            return await fallback_llm_instance.get_response(messages, conversation_id, **fallback_params)
        except Exception as e:
            # Error s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông capture b·ªüi @observe
            raise

    # @observe(name="robot-v2.llm.predict", capture_input=True, capture_output=True)
    async def predict(self, messages, params: dict, **kwargs):
        """
        D·ª± ƒëo√°n ph·∫£n h·ªìi t·ª´ LLM v·ªõi x·ª≠ l√Ω fallback v√† logging
    
        ƒê∆∞·ª£c trace v·ªõi # @observe ƒë·ªÉ monitor LLM calls trong Langfuse.
        capture_input=True, capture_output=True ƒë·ªÉ gi·∫£m overhead (ch·ªâ trace timing v√† metadata).
    
        Args:
            messages: Danh s√°ch tin nh·∫Øn ƒë·ªÉ g·ª≠i ƒë·∫øn LLM
            params (dict): Tham s·ªë c·∫•u h√¨nh cho LLM (t·ª´ generation_params)
                - model (str): T√™n m√¥ h√¨nh (vd: "openai/gpt-oss-20b", "gpt-4o-mini")
                - temperature (float): ƒê·ªô ng·∫´u nhi√™n
                - max_tokens (int): S·ªë token t·ªëi ƒëa
                - top_p (float): Top-p sampling
                - stream (bool): C√≥ streaming hay kh√¥ng
            **kwargs: C√°c tham s·ªë b·ªï sung
                - conversation_id (str): ID cu·ªôc h·ªôi tho·∫°i
                - first_message (str): Tin nh·∫Øn ƒë·∫ßu ti√™n (n·∫øu c√≥)
                - start_message (str): Tin nh·∫Øn kh·ªüi ƒë·∫ßu
                - fallback_llm (BaseLLM, optional): OpenAI provider instance ƒë·ªÉ fallback sang gpt-4o-mini
            
        Returns:
            str: Ph·∫£n h·ªìi t·ª´ LLM ho·∫∑c gi√° tr·ªã fallback

        H√†nh vi timeout & fallback:
            - G·ªçi model ch√≠nh, ch·ªù t·ªëi ƒëa 1.5s.
            - N·∫øu h·∫øt h·∫°n/l·ªói: log c·∫£nh b√°o, k√≠ch ho·∫°t song song model ph·ª• gpt-4o-mini qua OpenAI provider.
            - N·∫øu c√≥ fallback_llm trong kwargs: d√πng fallback_llm (OpenAI provider) ƒë·ªÉ g·ªçi gpt-4o-mini.
            - N·∫øu kh√¥ng c√≥ fallback_llm v√† provider hi·ªán t·∫°i l√† openai: d√πng self ƒë·ªÉ g·ªçi gpt-4o-mini.
            - N·∫øu kh√¥ng c√≥ fallback_llm v√† provider kh√¥ng ph·∫£i openai: kh√¥ng fallback, tr·∫£ v·ªÅ default.
            - ƒêua gi·ªØa main vs fallback: k·∫øt qu·∫£ n√†o v·ªÅ tr∆∞·ªõc ƒë∆∞·ª£c d√πng, h·ªßy task c√≤n l·∫°i.
            - N·∫øu c·∫£ hai ƒë·ªÅu l·ªói/timeout: tr·∫£ v·ªÅ fallback chu·∫©n h√≥a.
        """
        try:
            model_name = params.get("model", "unknown")
            provider_name = self.provider_name or "unknown"
            conversation_id = kwargs.get('conversation_id', 'unknown')
            logging.info(f"[BaseLLM] {conversation_id} - Start predict | Provider: {provider_name} | Model: {model_name}")
        
            if kwargs.get("first_message") is not None and kwargs.get("start_message") not in [None, ""]:
                res = INTENT_FALLBACK if INTENT_FALLBACK is not None else "fallback"
            else:
                # G·ªçi model ch√≠nh v·ªõi timeout 1.5s
                # ‚úÖ D√πng _trace_main_llm_call thay v√¨ get_response tr·ª±c ti·∫øp
                main_task = asyncio.create_task(
                    self._trace_main_llm_call(messages, conversation_id, params)
                )
            
                # X√°c ƒë·ªãnh fallback_llm: ∆∞u ti√™n fallback_llm t·ª´ kwargs, n·∫øu kh√¥ng c√≥ th√¨ check provider hi·ªán t·∫°i
                fallback_llm = kwargs.get("fallback_llm")
                can_fallback = fallback_llm is not None or provider_name == "openai"
            
                # Log fallback_llm status ƒë·ªÉ debug
                if fallback_llm is not None:
                    logging.info(f"[BaseLLM] {conversation_id} - fallback_llm available | provider={getattr(fallback_llm, 'provider_name', 'unknown')}")
                else:
                    logging.info(f"[BaseLLM] {conversation_id} - fallback_llm=None | can_fallback={can_fallback} | current_provider={provider_name}")
            
                try:
                    # Ch·ªù model ch√≠nh t·ªëi ƒëa 1.5s
                    res = await asyncio.wait_for(main_task, timeout=1.5)
                    logging.info(f"[BaseLLM] {conversation_id} - Main model succeeded within timeout")
                except asyncio.TimeoutError:
                    # Model ch√≠nh ch·∫≠m >1.5s: k√≠ch ho·∫°t fallback song song
                    # Send alert v·ªÅ LLM timeout
                    if ALERT_ENABLED:
                        send_alert_safe(
                            alert_type=AlertType.LLM_TIMEOUT,
                            level=AlertLevel.HIGH,
                            message="LLM main model timeout after 1.5s. Switching to fallback.",
                            context={
                                "conversation_id": conversation_id,
                                "provider": provider_name,
                                "model": model_name,
                                "timeout": "1.5s"
                            },
                            component="base_llm"
                        )
                
                    if can_fallback:
                        logging.warning(f"[BaseLLM] {conversation_id} - Main model timeout after 1.5s, starting fallback in parallel")
                    
                        # Chu·∫©n b·ªã v√† ch·∫°y fallback task
                        fallback_task = self._prepare_fallback_task(
                            fallback_llm, provider_name, messages, conversation_id
                        )
                    
                        # Ch·∫°y c·∫£ 2 song song v√† x·ª≠ l√Ω race condition
                        res = await self._run_fallback_parallel(
                            main_task, fallback_task, conversation_id, 
                            provider_name, model_name, FALLBACK_TIMEOUT
                        )
                    else:
                        # Kh√¥ng th·ªÉ fallback (kh√¥ng c√≥ fallback_llm v√† provider kh√¥ng ph·∫£i openai)
                        logging.warning(f"[BaseLLM] {conversation_id} - Main model timeout, but cannot fallback (provider={provider_name}, fallback_llm=None)")
                        # ƒê·ª£i main task ho√†n th√†nh ƒë·ªÉ trace
                        try:
                            res = await asyncio.wait_for(main_task, timeout=FALLBACK_TIMEOUT)
                            logging.info(f"[BaseLLM] {conversation_id} - Main task eventually completed")
                        except Exception:
                            res = INTENT_FALLBACK if INTENT_FALLBACK is not None else "fallback"
                except Exception as e:
                    # N·∫øu main_task fail ngay (kh√¥ng ph·∫£i timeout): c≈©ng ch·∫°y fallback n·∫øu c√≥ th·ªÉ
                    if can_fallback:
                        logging.warning(f"[BaseLLM] {conversation_id} - Main model failed early, fallback to {FALLBACK_MODEL}: {traceback.format_exc()}")
                    
                        # Chu·∫©n b·ªã v√† ch·∫°y fallback task
                        fallback_task = self._prepare_fallback_task(
                            fallback_llm, provider_name, messages, conversation_id
                        )
                    
                        # Ch·∫°y c·∫£ 2 song song v√† x·ª≠ l√Ω race condition
                        res = await self._run_fallback_parallel(
                            main_task, fallback_task, conversation_id,
                            provider_name, model_name, FALLBACK_TIMEOUT
                        )
                    else:
                        # Kh√¥ng th·ªÉ fallback, ƒë·ª£i main ho√†n th√†nh ƒë·ªÉ trace
                        try:
                            res = await asyncio.wait_for(main_task, timeout=FALLBACK_TIMEOUT)
                        except Exception:
                            res = INTENT_FALLBACK if INTENT_FALLBACK is not None else "fallback"
            
                if isinstance(res, str):
                    res = res.rstrip()
                
            logging.info(f"[BaseLLM] {conversation_id} - Predict: {res} | Provider: {provider_name} | Model: {model_name}")
            return res
        except asyncio.CancelledError:
            # CancelledError l√† b√¨nh th∆∞·ªùng khi task b·ªã cancel do phoneme ƒë√£ tr·∫£ v·ªÅ k·∫øt qu·∫£ h·ª£p l·ªá tr∆∞·ªõc
            # (khi ch·∫°y song song phoneme v√† LLM, n·∫øu phoneme c√≥ k·∫øt qu·∫£ th√¨ cancel LLM task)
            # Kh√¥ng c·∫ßn log nh∆∞ error v√¨ ƒë√¢y l√† behavior mong mu·ªën
            logging.debug(f"[BaseLLM] Task cancelled for {kwargs.get('conversation_id')} (phoneme classifier returned result first, LLM task cancelled)")
            res = INTENT_FALLBACK if INTENT_FALLBACK is not None else "fallback"
            return res
        except Exception as e:
            logging.error(f"[ERROR][BaseLLM] Request failed {kwargs.get('conversation_id')}: {traceback.format_exc()}")
            res = INTENT_FALLBACK if INTENT_FALLBACK is not None else "fallback"
            return res

    def parsing_json(self, data: str) -> dict:
        """
        Ph√¢n t√≠ch chu·ªói JSON v·ªõi x·ª≠ l√Ω c√°c ƒë·ªãnh d·∫°ng ƒë·∫∑c bi·ªát
        """
        try:
            return self.json_parser.parse(data)
        except (json.JSONDecodeError, ValueError, TypeError) as e:
            # Send alert for malformed response
            if ALERT_ENABLED:
                send_alert_safe(
                    alert_type=AlertType.LLM_MALFORMED_RESPONSE,
                    level=AlertLevel.MEDIUM,
                    message="LLM returned malformed response.",
                    context={
                        "provider": self.provider_name or "unknown",
                        "error": str(e)[:500],
                        "data_preview": data[:200] if data else "empty"
                    },
                    component="base_llm"
                )
            # Re-raise ƒë·ªÉ caller c√≥ th·ªÉ handle
            raise

```